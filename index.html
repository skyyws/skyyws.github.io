<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>skyyws的藏宝阁</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="skyyws的藏宝阁">
<meta property="og:url" content="https://skyyws.github.io/index.html">
<meta property="og:site_name" content="skyyws的藏宝阁">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="汪胜">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="skyyws的藏宝阁" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">skyyws的藏宝阁</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://skyyws.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-Impala-3-4-SQL查询之ScanRange流程归纳（六）" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/29/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8BScanRange%E6%B5%81%E7%A8%8B%E5%BD%92%E7%BA%B3%EF%BC%88%E5%85%AD%EF%BC%89/" class="article-date">
  <time class="dt-published" datetime="2021-04-29T12:27:49.000Z" itemprop="datePublished">2021-04-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/29/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8BScanRange%E6%B5%81%E7%A8%8B%E5%BD%92%E7%BA%B3%EF%BC%88%E5%85%AD%EF%BC%89/">Impala 3.4 SQL查询之ScanRange流程归纳（六）</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>我们在前面几篇文章，从代码处理层面，详细分析了Impala的ScanRange相关知识，包括FE端的处理、parquet文件的处理、IO thread的处理等，涉及到的内容比较多。本文笔者将前几篇文章的内容做了一个汇总，整体看一下Impala的整个ScanRange的处理流程。需要注意的是，我们当前的分析都是基于parquet格式、remote HDFS的场景。我们将整个处理过程汇总到了一张流程图上，如下所示：<br><img src="https://img-blog.csdnimg.cn/20210429202016959.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70#pic_center" alt="1"><br>接下来，我们就根据流程图，来看下整个的ScanRange处理流程。</p>
<h4 id="Coordinator处理"><a href="#Coordinator处理" class="headerlink" title="Coordinator处理"></a>Coordinator处理</h4><p>首先是左上方的紫色方框，表示的是Coordinator接收客户端发来的SQL请求，然后通过JNI传到FE端进行解析，最终生成分布式的执行信息，发到各个Executor上进行处理。这块的处理其实就是thrfit结构体在BE/FE之间的传输，我们在<a target="_blank" rel="noopener" href="https://blog.csdn.net/skyyws/article/details/115751129">Impala 3.4 SQL查询之ScanRange详解（三）</a>一文中，已经详细描述过了，这里不再赘述。</p>
<h4 id="Disk-Queue与IO-thread构造"><a href="#Disk-Queue与IO-thread构造" class="headerlink" title="Disk Queue与IO thread构造"></a>Disk Queue与IO thread构造</h4><p>Executor有一个类专门用来管理本地磁盘或者远端文件系统上的IO相关的操作，叫DiskIoMgr。DiskIoMgr初始化的时候，会构造一个disk_queues_集合，集合中的每个成员都是代表一个本地disk对应的队列，或者是一种远端文件系统，例如remote HDFS/S3等。同时，对每个队列都会绑定指定数量的线程来处理后续的数据扫描，这些线程就是IO thead。不同的队列，可以通过参数配置IO thread数量，以remote HDFS为例，对应的参数就是num_remote_hdfs_io_threads，默认是8个。我们在<a target="_blank" rel="noopener" href="https://blog.csdn.net/skyyws/article/details/115350188">Impala HDFS_SCAN_NODE之IO threads模型</a>文章中，有详细介绍过这部分的处理流程。对应流程图中的，就是右上角的第一个蓝色方框。</p>
<h4 id="RequestContext处理"><a href="#RequestContext处理" class="headerlink" title="RequestContext处理"></a>RequestContext处理</h4><p>当查询计划信息发到executor之后，executor会根据相关信息构造RequestContext对象，然后放到request_contexts_队列当中。一个RequestContext对象，可以简单理解为对一个表的扫描请求的封装。这个executor上所有每个表的扫描请求，都在这个request_contexts_队列中等待处理。<br>每个RequestContext都会包含一个PerDiskState集合，我们可以根据当前这个RequestContext的disk queue类型，获取到指定的PerDiskState对象，这个PerDiskState就包含了每个disk queue的状态，比如unstarted_scan_ranges_、in_flight_ranges_等。<br>这些包含关系就对应了流程图中的黄色方框，关于RequestContext和PerDiskState的介绍，在<a target="_blank" rel="noopener" href="https://blog.csdn.net/skyyws/article/details/115350188">Impala HDFS_SCAN_NODE之IO threads模型</a>和<a target="_blank" rel="noopener" href="https://blog.csdn.net/skyyws/article/details/115751129">Impala 3.4 SQL查询之ScanRange详解（三）</a>中都有详细说明。</p>
<h4 id="Footer-ScanRange构造"><a href="#Footer-ScanRange构造" class="headerlink" title="Footer ScanRange构造"></a>Footer ScanRange构造</h4><p>Executor获取到coordinator发过来的执行计划信息之后，会构造footer ScanRange对象，然后加入到unstarted_scan_ranges队列中，然后准备启动scanner线程进行后续的处理。这个过程就对应流程图中左边的绿色方框。关于footer ScanRange的构造，我们在<a target="_blank" rel="noopener" href="https://blog.csdn.net/skyyws/article/details/115770717">Impala 3.4 SQL查询之ScanRange详解（四）</a>一文中，有详细介绍。</p>
<h4 id="IO-thread处理"><a href="#IO-thread处理" class="headerlink" title="IO thread处理"></a>IO thread处理</h4><p>IO线程启动之后，首先会先从request_contexts_队列中获取队首的RequestContext对象，然后获取对应的PerDiskState对象，接着从PerDiskState对象的unstarted_scan_ranges_队列中获取一个ScanRange成员，获取完成之后，IO thread会将该RequestContext对象又放回到request_contexts_队列的尾部。<br>接着，IO thread会将刚刚获取到的ScanRange对象加入到ready_to_start_ranges_队列中。然后，再从in_flight_ranges_队列的首部获取一个ScanRange对象，这个才是IO thread真正要处理的ScanRange。<br>从in_flight_ranges_队列获取到ScanRange之后，IO thread就会进行实际的scan操作，操作完成之后，会更新ScanRange的相关信息。然后再判断该ScanRange是否处理完成，如果没有处理完成，则加入到in_flight_ranges_队列尾部；如果已经处理完成，则直接返回。这表示IO thread完成了本次处理，结束之后，继续上述步骤，处理其他的ScanRange，直至结束。<br>上述的这些流程，我们在<a target="_blank" rel="noopener" href="https://blog.csdn.net/skyyws/article/details/115770717">Impala 3.4 SQL查询之ScanRange详解（四）</a>一文中，有详细的描述。</p>
<h4 id="Scanner线程处理"><a href="#Scanner线程处理" class="headerlink" title="Scanner线程处理"></a>Scanner线程处理</h4><p>当Executor构造完footer ScanRange之后，就会启动scanner线程进行处理，主要就是流程图中的红色方框部分。Scanner线程首先会从ready_to_start_ranges_队列中获取头部的ScanRange进行判断，如果buffer_tag不是NO_BUFFER（以remote HDFS为例），那么会分配buffer，然后加入到in_flight_ranges_。<br>此时IO thread就可以获取in_flight_ranges_中的ScanRange（这里是footer ScanRange）进行处理。处理完成之后，scanner线程就会根据扫描的数据，解析parquet文件的元数据，进而构造data ScanRange。同样，分配buffer之后，会将这些data ScanRange加入到in_flight_ranges_队列。等待IO thread处理完这些data ScanRange，scanner线程再进行后续处理。关于scanner线程的处理，可以参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/skyyws/article/details/116237150">Impala 3.4 SQL查询之ScanRange详解（五）</a>，有详细的介绍。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>以上就是整个ScanRange的基本处理流程，这其中比较重要的就是理解IO thread和scanner线程各自负责的功能，当然我们省略了一些实现细节。由于我们这里讨论的是parquet格式，因此我们<a target="_blank" rel="noopener" href="https://blog.csdn.net/skyyws/article/details/116237150">Impala 3.4 SQL查询之ScanRange详解（五）</a>一文中，也详细介绍了Impala对parquet文件的处理，这个在流程图中并没有体现。对于其他的文件格式的处理，我们目前也没有展开。后续有机会，再跟大家一起学习。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://skyyws.github.io/2021/04/29/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8BScanRange%E6%B5%81%E7%A8%8B%E5%BD%92%E7%BA%B3%EF%BC%88%E5%85%AD%EF%BC%89/" data-id="cko2v5j8t0000oux5g2go7ngi" data-title="Impala 3.4 SQL查询之ScanRange流程归纳（六）" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Impala-SQL%E6%9F%A5%E8%AF%A2%E7%B3%BB%E5%88%97/" rel="tag">Impala SQL查询系列</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/olap/" rel="tag">olap</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Impala-3-4-SQL查询之ScanRange详解（五）" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/28/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8BScanRange%E8%AF%A6%E8%A7%A3%EF%BC%88%E4%BA%94%EF%BC%89/" class="article-date">
  <time class="dt-published" datetime="2021-04-28T08:57:09.000Z" itemprop="datePublished">2021-04-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/28/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8BScanRange%E8%AF%A6%E8%A7%A3%EF%BC%88%E4%BA%94%EF%BC%89/">Impala 3.4 SQL查询之ScanRange详解（五）</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>在上篇文章中，我们介绍了PerDiskState的unstarted_scan_ranges_这个队列的更新逻辑，主要就是成员的入队和出队。总结下来就是：HdfsScanNode会获取每个文件的footer ScanRange，然后入队；IO thread会通过RequestContext获取对应的PerDiskState，然后出队，并设置到next_scan_range_to_start_成员，同时入队到RequestContext的ready_to_start_ranges_队列。IO thead并不会直接从unstarted_scan_ranges_获取对象，进行scan操作，而是会从另外一个队列in_flight_ranges_中获取对象，返回并进行后续的操作。在本文中，我们同样会结合代码，一起学习下，in_flight_ranges_队列是如何更新的。</p>
<h4 id="ScanRange分配buffer"><a href="#ScanRange分配buffer" class="headerlink" title="ScanRange分配buffer"></a>ScanRange分配buffer</h4><p>首先，我们来看下ScanRange的buffer分配问题。在将ScanRange放到in_flight_ranges_队列之前，需要先给ScanRange分配buffer，只有当分配了buffer之后，IO thread才能进行实际的scan操作。Buffer分配的主要处理就是在AllocateBuffersForRange函数中。我们先来看下主要的处理逻辑：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; DiskIoMgr::AllocateBuffersForRange()</span><br><span class="line">  vector&lt;unique_ptr&lt;BufferDescriptor&gt;&gt; buffers;</span><br><span class="line">  for (int64_t buffer_size : ChooseBufferSizes(range-&gt;bytes_to_read(), max_bytes)) &#123;</span><br><span class="line">    BufferPool::BufferHandle handle;</span><br><span class="line">    status &#x3D; bp-&gt;AllocateBuffer(bp_client, buffer_size, &amp;handle);</span><br><span class="line">    if (!status.ok()) goto error;</span><br><span class="line">    buffers.emplace_back(new BufferDescriptor(range, bp_client, move(handle)));</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">&#x2F;&#x2F; DiskIoMgr::ChooseBufferSizes()</span><br><span class="line">&#x2F;&#x2F; 删除了部分代码，只保留了关键的部分</span><br><span class="line">vector&lt;int64_t&gt; DiskIoMgr::ChooseBufferSizes(int64_t scan_range_len, int64_t max_bytes) &#123;</span><br><span class="line">  while (bytes_allocated &lt; scan_range_len) &#123;</span><br><span class="line">    int64_t bytes_remaining &#x3D; scan_range_len - bytes_allocated;</span><br><span class="line">    int64_t next_buffer_size;</span><br><span class="line">    if (bytes_remaining &gt;&#x3D; max_buffer_size_) &#123;</span><br><span class="line">      next_buffer_size &#x3D; max_buffer_size_;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      next_buffer_size &#x3D;</span><br><span class="line">          max(min_buffer_size_, BitUtil::RoundUpToPowerOfTwo(bytes_remaining));</span><br><span class="line">    &#125;</span><br><span class="line">    if (next_buffer_size + bytes_allocated &gt; max_bytes) &#123;</span><br><span class="line">      if (bytes_allocated &gt; 0) break;</span><br><span class="line">      next_buffer_size &#x3D; BitUtil::RoundDownToPowerOfTwo(max_bytes);</span><br><span class="line">    &#125;</span><br><span class="line">    buffer_sizes.push_back(next_buffer_size);</span><br><span class="line">    bytes_allocated +&#x3D; next_buffer_size;</span><br><span class="line">  &#125;</span><br><span class="line">  return buffer_sizes;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里主要涉及到两个参数：bytes_to_read_，表示这个ScanRange需要read的字节数；max_bytes，是一个阈值，我们这里先不展开它的获取方式，后面再介绍。接着，在ChooseBufferSizes函数中，会根据这个两个参数，来循环构造buffer，所有的buffer都放到一个vector中。这里的max_buffer_size_对应的就是read_size参数，默认是8M；min_buffer_size_对应的是min_buffer_size参数，默认是8K。代码的主要逻辑就是：</p>
<ol>
<li>如果待分配字节数（初始就是range的bytes_to_read_）大于max_buffer_size_，则直接分配一个max_buffer_size_大小的buffer，加入到vector中；如果小于max_buffer_size_，则取待分配字节数和min_buffer_size_较大的，保证分配的buffer不会小于min_buffer_size_；</li>
<li>如果分配的buffer总大小超过了max_bytes限制，则结束此次分配，也就是说，给ScanRange一次分配的buffer数量，不一定能够保证所有的bytes_to_read_都足够读取，必须小于max_bytes；</li>
</ol>
<p>当获取了需要的buffer之后，我们根据这些buffer，构造BufferDescriptor，更新ScanRange的unused_iomgr_buffer_bytes_和unused_iomgr_buffers_成员。然后IO thread就会获取buffer，进行后续的scan操作。</p>
<h4 id="IO-thread处理ScanRange流程"><a href="#IO-thread处理ScanRange流程" class="headerlink" title="IO thread处理ScanRange流程"></a>IO thread处理ScanRange流程</h4><p>当IO thread获取到ScanRange的对象之后，就会进行实际的scan操作。整个ScanRange的处理流程如下所示：<br><img src="https://img-blog.csdnimg.cn/20210428163538276.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70#pic_center" alt="1"><br>这里有几点需要注意：</p>
<ol>
<li><p>需要先获取buffer，才能进行scan操作，如果没有可用的buffer，则直接返回，需要scanner线程分配buffer之后，才能继续；</p>
</li>
<li><p>如果本次操作完成之后，当前的ScanRange还没有读完，需要放回in_flight_range队列，等待再次处理；</p>
</li>
<li><p>保存数据的buffer，会更新到ScanRange的ready_buffers_成员，后续scanner线程会获取ready_buffers_中的buffer，进行处理；</p>
<h4 id="Impala处理parquet格式文件"><a href="#Impala处理parquet格式文件" class="headerlink" title="Impala处理parquet格式文件"></a>Impala处理parquet格式文件</h4><p>接着我们再来看下Impala对于parquet格式的文件是如何处理的。这个对于后面Impala处理ScanRange的介绍有一定的帮助。首先简单看下parquet的文件结构：<br><img src="https://img-blog.csdnimg.cn/2021042816370119.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70#pic_center" alt="2"><br>一个parquet文件主要包括三个部分：header和footer以及中间的数据区，数据区由多个RowGroup组成，每个RowGroup包含一批数据；每个RowGroup又分为多个ColumnChunk，每个ColumnChunk表示一个列的数据；ColumnChunk又包含多个DataPage，这是数据存储的最小单元。<br>为了读取parquet文件的数据，针对上述文件结构，Impala也设计了相应的类进行处理，如下所示：<br><img src="https://img-blog.csdnimg.cn/20210428163618795.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70#pic_center" alt="3"><br>结合上述的UML，我们将处理流程归纳为如下几点：</p>
</li>
<li><p>对于每一个split，executor都会构造一个HdfsParquetScanner（如果是其他的文件格式，则是其他的scanner对象）；</p>
</li>
<li><p>HdfsParquetScanner会根据SQL中涉及列，来构造ParquetColumnReader，或者是其子类BaseScalarColumnReader，每一个reader负责处理一个列的数据；</p>
</li>
<li><p>一个split，可能会包含多个RowGroup，Impala会根据RowGroup中的ColumnChunk信息，来初始化BaseScalarColumnReader中的ParquetColumnChunkReader对象，ParquetColumnChunkReader主要负责从data pages中读取数据、解压、数据buffer的拷贝等；</p>
</li>
<li><p>在初始化ParquetColumnChunkReader的时候，会一并初始化的它的一个成员ParquetPageReader，ParquetPageReader就是最终实际去读page headers和data pages。</p>
</li>
</ol>
<p>需要注意的是，上面的这些操作，都是在executor上，由scanner线程进行处理的，而真正的ScanRange的扫描操作，是由IO thread进行的。</p>
<h4 id="in-flight-ranges-的出队操作"><a href="#in-flight-ranges-的出队操作" class="headerlink" title="in_flight_ranges_的出队操作"></a>in_flight_ranges_的出队操作</h4><p>介绍了一些前置基础知识，接下来我们看下in_flight_ranges_队列的更新操作。其实在<a href="https://skyyws.github.io/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8BScanRange%E8%AF%A6%E8%A7%A3%EF%BC%88%E5%9B%9B%EF%BC%89">Impala 3.4 SQL查询之ScanRange详解（四）</a>一文中，已经有in_flight_ranges_的出现了，主要是在RequestContext::GetNextRequestRange函数中，先对unstarted_scan_ranges_进行了出队操作，然后再判断in_flight_ranges_是否为空，不为空的话直接弹出队头成员，否则直接返回空，相关函数如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">if (request_disk_state-&gt;in_flight_ranges()-&gt;empty()) &#123;</span><br><span class="line">  &#x2F;&#x2F; There are no inflight ranges, nothing to do.</span><br><span class="line">  request_disk_state-&gt;DecrementDiskThread(request_lock, this);</span><br><span class="line">  return nullptr;</span><br><span class="line">&#125;</span><br><span class="line">DCHECK_GT(request_disk_state-&gt;num_remaining_ranges(), 0);</span><br><span class="line">RequestRange* range &#x3D; request_disk_state-&gt;in_flight_ranges()-&gt;Dequeue();</span><br><span class="line">DCHECK(range !&#x3D; nullptr);</span><br></pre></td></tr></table></figure>
<p>因此，我们可以知道，IO thread实际每次是取in_flight_ranges_队列的队首成员返回进行处理的。出队操作比较简单，入队操作相对比较复杂。</p>
<h4 id="in-flight-ranges-的入队操作"><a href="#in-flight-ranges-的入队操作" class="headerlink" title="in_flight_ranges_的入队操作"></a>in_flight_ranges_的入队操作</h4><p>关于in_flight_ranges_的入队操作，涉及到的情况比较多，因此我们将相关的代码调用整理成了一张图，如下所示：<br><img src="https://img-blog.csdnimg.cn/20210428163723315.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70#pic_center" alt="4"><br>图中每个方框表示相应的函数或者函数调用栈，最下面的方框就是最终的in_flight_ranges_的入队。黄色方框表示的是，当满足该条件时，才会插入到in_flight_ranges_队列。下面我们就结合代码来看看不同场景下，in_flight_ranges_的入队操作。</p>
<h5 id="Footer-ScanRange的处理"><a href="#Footer-ScanRange的处理" class="headerlink" title="Footer ScanRange的处理"></a>Footer ScanRange的处理</h5><p>在<a href="https://skyyws.github.io/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8BScanRange%E8%AF%A6%E8%A7%A3%EF%BC%88%E5%9B%9B%EF%BC%89">Impala 3.4 SQL查询之ScanRange详解（四）</a>一文中，我们提到过：对于parquet格式的文件，会针对每个split（一个文件的一个block，会对应一个HdfsFileSplit），构造一个footer ScanRange，大小是100KB，并且保存着原始的split信息，主要是offset、len等。这些footer ScanRange会经由图中的第三条路径，通过scanner线程加入到in_flight_ranges_队列中。为了防止大家混淆，我们将这个流程单独拎出来，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ScannerThread(hdfs-scan-node.cc):403</span><br><span class="line">-StartNextScanRange(hdfs-scan-node-base.cc):692</span><br><span class="line">--AllocateBuffersForRange(disk-io-mgr.cc):399</span><br><span class="line">---AddUnusedBuffers(scan-range.cc):147</span><br><span class="line">----ScheduleScanRange(request-context.cc):797</span><br><span class="line">-----state.in_flight_ranges()-&gt;Enqueue(range)</span><br></pre></td></tr></table></figure>
<p>首先需要先对这些ScanRange分配buffer，然后再将这个ScanRange加入到in_flight_ranges_队列中。对照上面的ScanRange分配buffer的逻辑来看，scan_range_len参数对应初始的footer ScanRange大小，是100KB，而max_bytes参数的大小，来自于FE端的计算，表示处理一个ScanRange需要的最小内存，以HdfsScanNode为例，相关函数调用如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">doCreateExecRequest(Frontend.java):1600</span><br><span class="line">-getPlannedExecRequest(Frontend.java):1734</span><br><span class="line">--createExecRequest(Frontend.java):1420</span><br><span class="line">---computeResourceReqs(Planner.java):435</span><br><span class="line">----computeResourceProfile(PlanFragment.java):263</span><br><span class="line">-----computeRuntimeFilterResources(PlanFragment.java):327</span><br><span class="line">------computeNodeResourceProfile(HdfsScanNode.java):1609</span><br><span class="line">-------computeMinMemReservation(HdfsScanNode.java)</span><br></pre></td></tr></table></figure>
<p>最终，在computeMinMemReservation函数中，会计算出一个值，通过TBackendResourceProfile结构体的min_reservation成员保存，并传到BE端。一般情况下，这个值是大于100KB的，因此，对于footer ScanRange，处理之后会分配1个buffer，大小是128KB（通过函数BitUtil::RoundUpToPowerOfTwo()向上取到2的整数次幂），最后将footer ScanRange加到in_flight_ranges_队列。之后IO thread就可以通过in_flight_ranges_队列取到这些footer ScanRange，根据上面的ScanRange处理流程进行处理。也就是说，对于每一个split，都会先构造一个footer ScanRange，该footer ScanRange处理完成之后，才能继续进行后面的数据扫描处理。</p>
<h5 id="Data-ScanRange的处理"><a href="#Data-ScanRange的处理" class="headerlink" title="Data ScanRange的处理"></a>Data ScanRange的处理</h5><p>前面我们提到了对于每个split，Impala都会构造一个footer ScanRange。只有先解析出footer的信息，我们才能知道parquet文件的元数据信息，进而构造data ScanRange，扫描真正的数据。我们将data ScanRange的处理流程进行了梳理，如下所示：<br><img src="https://img-blog.csdnimg.cn/20210428163742459.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70#pic_center" alt="5"><br>整个处理流程同样是通过scanner线程进行处理的，主要分为如下几个部分：</p>
<ol>
<li>最左边红色的方框，就是scanner线程读取footer ScanRange的buffer中的元数据信息。通过ScanRange::GetNext函数，就可以获取ready_buffers_中的buffer成员，进行后续的解析操作。需要注意的是，此时footer ScanRange是已经被IO thead处理完成，如果没有处理完成的话，scanner线程会一直等待；</li>
<li>中间蓝色的方框，就是HdfsParquetScanner在获取到元数据之后，构造相应的column reader成员，这里主要就是根据SQL中涉及到的column进行构造，详细构造过程不展开；</li>
<li>左下角黄色的方框，就是计算每个ScanRange分配的最大字节数，也就是我们在上面提到的max_bytes。最终，在给ScanRange分配buffer的时候，分配的总字节数不会超过这个max_bytes。这个地方的计算与column reader包含的ScanRange的bytes_to_read_以及read_size和min_buffer_size参数有关系，核心实现逻辑在HdfsParquetScanner::DivideReservationBetweenColumnsHelper函数中，这块的计算也相对比较复杂，感兴趣的同学可以自行学习；</li>
<li>最后是右边的绿色方框，就是根据这些column reader构造对应的data ScanRange，然后分配buffer，并添加到in_flight_ranges_队列。此时IO thread就可以获取这些data ScanRange进行实际的scan操作了。</li>
</ol>
<p>整个data ScanRange的处理流程就in_flight_ranges_队列图的第四条路径，也就是最右边的那个绿色方框。需要注意的是，如果分配给ScanRange的buffer不能一次读完所有的字节数，那么当IO thread用完分配的buffer之后，scanner线程会重新分配buffer，等待后续IO thead再次处理。</p>
<h5 id="IO-thread的处理"><a href="#IO-thread的处理" class="headerlink" title="IO thread的处理"></a>IO thread的处理</h5><p>最左边的红色方框代表的路径表示：IO thread在处理完对应的ScanRange时，会更新相应的bytes_read、unused_iomgr_buffers_等成员。处理完成之后，会判断当前这个ScanRange是否处理完成，如果处理完成的话，则直接将num_remaining_ranges_成员减1，表示这个ScanRange已经处理完成。如果处理的结果是ReadOutcome::SUCCESS_NO_EOSR，则表示这个ScanRange还没有处理完成，会将这个ScanRange再次放回到in_flight_ranges_队列。这样其他的IO thread可以再次获取这个ScanRange进行处理。</p>
<h5 id="非ExternalBufferTag-NO-BUFFER"><a href="#非ExternalBufferTag-NO-BUFFER" class="headerlink" title="非ExternalBufferTag::NO_BUFFER"></a>非ExternalBufferTag::NO_BUFFER</h5><p>对于图中的第二条路径，主要是针对非remote HDFS的情况。在<a href="https://skyyws.github.io/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8BScanRange%E8%AF%A6%E8%A7%A3%EF%BC%88%E5%9B%9B%EF%BC%89">Impala 3.4 SQL查询之ScanRange详解（四）</a>中介绍BE端的ScanRange的时候，我们提到会根据FE端的文件信息来构造ScanRange，此时会构造一个buffer tag，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; HdfsScanNodeBase::Prepare()</span><br><span class="line">    int cache_options &#x3D; BufferOpts::NO_CACHING;</span><br><span class="line">    if (params.__isset.try_hdfs_cache &amp;&amp; params.try_hdfs_cache) &#123;</span><br><span class="line">      cache_options |&#x3D; BufferOpts::USE_HDFS_CACHE;</span><br><span class="line">    &#125;</span><br><span class="line">    if ((!expected_local || FLAGS_always_use_data_cache) &amp;&amp; !IsDataCacheDisabled()) &#123;</span><br><span class="line">      cache_options |&#x3D; BufferOpts::USE_DATA_CACHE;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>对于remote HDFS，这里最终cache_options的值就是4，即NO_CACHING|USE_DATA_CACHE。接着，在RequestContext::GetNextUnstartedRange函数中，会使用该tag进行判断，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; RequestContext::GetNextUnstartedRange()</span><br><span class="line">      ScanRange::ExternalBufferTag buffer_tag &#x3D; (*range)-&gt;external_buffer_tag();</span><br><span class="line">      if (buffer_tag &#x3D;&#x3D; ScanRange::ExternalBufferTag::NO_BUFFER) &#123;</span><br><span class="line">        &#x2F;&#x2F; We can&#39;t schedule this range until the client gives us buffers. The context</span><br><span class="line">        &#x2F;&#x2F; must be rescheduled regardless to ensure that &#39;next_scan_range_to_start&#39; is</span><br><span class="line">        &#x2F;&#x2F; refilled.</span><br><span class="line">        disk_states_[disk_id].ScheduleContext(lock, this, disk_id);</span><br><span class="line">        (*range)-&gt;SetBlockedOnBuffer();</span><br><span class="line">        *needs_buffers &#x3D; true;</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        ScheduleScanRange(lock, *range);</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure>
<p>只有当tag不是NO_BUFFER的时候，才会将ScanRange加入in_flight_ranges_队列。也就是说，对于remote HDFS的scan操作，不是直接将ScanRange加入到in_flight_ranges_队列，而是在其他的地方进行处理。由于笔者手头的测试环境都是remote HDFS，因此，对于这种情况，目前暂不展开说明。</p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>到这里，关于in_flight_ranges_队列的更新，我们就基本介绍完毕了，当然这不是全部的情况，目前还有一些其他的情况我们没有展示在这篇文章当中。由于篇幅原因，本文也省略了很多细节的地方。总结一下，在这篇文章当中，我们首先介绍了ScanRange分配buffer，也就是说对于每个ScanRange，都需要先通过scanner线程来分配buffer，之后才能通过IO thread进行实际的scan操作。接着，我们介绍了IO thread处理ScanRange流程和Impala处理parquet格式文件。最后我们看到了in_flight_ranges_队列是如何更新，最重要的部分就是footer ScanRange和data ScanRange的处理，这个Impala的IO模型比较关键的地方。本文所有的代码都是基于3.4.0分支，都是笔者个人结合调试结果，分析得出，如有错误，欢迎指正。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://skyyws.github.io/2021/04/28/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8BScanRange%E8%AF%A6%E8%A7%A3%EF%BC%88%E4%BA%94%EF%BC%89/" data-id="cko188g0c0000ffx5fbxxh3a5" data-title="Impala 3.4 SQL查询之ScanRange详解（五）" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Impala-SQL%E6%9F%A5%E8%AF%A2%E7%B3%BB%E5%88%97/" rel="tag">Impala SQL查询系列</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/olap/" rel="tag">olap</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-关于Impala的use-local-tz-for-unix-timestamp-conversions参数探究" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/16/%E5%85%B3%E4%BA%8EImpala%E7%9A%84use-local-tz-for-unix-timestamp-conversions%E5%8F%82%E6%95%B0%E6%8E%A2%E7%A9%B6/" class="article-date">
  <time class="dt-published" datetime="2021-04-16T12:24:37.000Z" itemprop="datePublished">2021-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/16/%E5%85%B3%E4%BA%8EImpala%E7%9A%84use-local-tz-for-unix-timestamp-conversions%E5%8F%82%E6%95%B0%E6%8E%A2%E7%A9%B6/">关于Impala的use_local_tz_for_unix_timestamp_conversions参数探究</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>使用过Impala的同学都知道，impala默认对于timestamp都是当成UTC来处理的，并不会做任何的时区转换。这也就是说，当你写入一个timestamp的数据时，impala就会把它当成是UTC的时间存起来，而不是本地时间。但是Impala同时又提供了use_local_tz_for_unix_timestamp_conversions和convert_legacy_hive_parquet_utc_timestamps这两个参数来处理timestamp的时区问题。convert_legacy_hive_parquet_utc_timestamps这个参数主要是用来处理hive写parquet文件，impala读取的问题，本文暂不展开，这里主要介绍下use_local_tz_for_unix_timestamp_conversions这个参数的作用。首先，我们来看下官方的解释：<br><em><strong>The –use_local_tz_for_unix_timestamp_conversions setting affects conversions from TIMESTAMP to BIGINT, or from BIGINT to TIMESTAMP. By default, Impala treats all TIMESTAMP values as UTC, to simplify analysis of time-series data from different geographic regions. When you enable the –use_local_tz_for_unix_timestamp_conversions setting, these operations treat the input values as if they are in the local time zone of the host doing the processing. See Impala Date and Time Functions for the list of functions affected by the –use_local_tz_for_unix_timestamp_conversions setting.</strong></em><br>简单来说，就是开启了这个参数之后（默认false，表示关闭），当SQL里面涉及到了timestamp-&gt;bigint/bigint-&gt;timestamp的转换操作时，impala会把timestamp当成是本地的时间来处理，而不是UTC时间。这个地方听起来似乎很简单，但是实际理解起来的时候非常容易出错，这里笔者将结合自己的实际测试结果来看一下use_local_tz_for_unix_timestamp_conversions这个参数究竟是如何起作用的。</p>
<h4 id="测试数据准备"><a href="#测试数据准备" class="headerlink" title="测试数据准备"></a>测试数据准备</h4><p>首先，我们将测试集群的impala的use_local_tz_for_unix_timestamp_conversions和convert_legacy_hive_parquet_utc_timestamps参数都配置为false，然后重启集群。<br>接着，我们使用如下SQL来创建测试表，然后插入数据：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> timestamp_test_parquet(id <span class="type">int</span>,ts <span class="type">timestamp</span>,sec <span class="type">bigint</span>) stored <span class="keyword">as</span> parquet;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> timestamp_test_parquet <span class="keyword">values</span>(<span class="number">1001</span>,UTC_TIMESTAMP(),<span class="built_in">cast</span>(UTC_TIMESTAMP() <span class="keyword">as</span> <span class="type">bigint</span>));</span><br></pre></td></tr></table></figure>
<p>这里，为了保证一致，我们特地插入了UTC的时间，然后查询表，可以看到如下的结果：<br>|id|ts|sec|<br>|–|–|–|<br>| 1001 | 2020-07-29 06:20:20.728530000 |1596003620|<br>我们执行date -d @1596003620 “+%Y-%m-%d %H:%M:%S”命令，可以得到：2020-07-29 06:20:20，现在这2个时间都是UTC的时间没问题。如果是本地时间的话，应该是2020-07-29 14:20:20（当前笔者所处的时区为东八区，即UTC+8，后面默认本地时区都是UTC+8）。</p>
<h4 id="参数开启前后对比"><a href="#参数开启前后对比" class="headerlink" title="参数开启前后对比"></a>参数开启前后对比</h4><p>为了验证参数里面提到的timestamp-&gt;bigint/bigint-&gt;timestamp这个转换操作，我们分别关闭/开启参数，重启集群保证参数生效，然后执行如下SQL：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> id,<span class="built_in">cast</span>(ts <span class="keyword">as</span> <span class="type">bigint</span>),<span class="built_in">cast</span>(sec <span class="keyword">as</span> <span class="type">timestamp</span>) <span class="keyword">from</span> timestamp_test_parquet;</span><br></pre></td></tr></table></figure>
<p>以下是两次SQL执行的结果，我们合并到了一起进行对比：<br>|参数值|id|cast(ts as bigint)|cast(sec as timestamp)|<br>|–|–|–|–|<br>|false|1001|1596003620|2020-07-29 06:20:20|<br>|true|1001|1595974820|2020-07-29 14:20:20|<br>通过上述结果，我们发现，参数开启前后，两次SQL的执行结果明显是不同的。接下来，我们来根据结果进行仔细分析。</p>
<h5 id="cast-ts-as-bigint"><a href="#cast-ts-as-bigint" class="headerlink" title="cast(ts as bigint)"></a>cast(ts as bigint)</h5><p>当我们将timestamp转换为bigint的时候，前后两次的结果分别是：1596003620和1595974820，我们通过date命令进行转换，结果如下：<br>|参数值|cast(ts as bigint)|date -d @ts_bigint “+%Y-%m-%d %H:%M:%S”|<br>|–|–|–|<br>|false|1596003620|2020-07-29 06:20:20|<br>|true|1595974820|2020-07-28 22:20:20|<br>我们发现参数开启后，得到的bigint值比开启前的少了8h！这个地方要如何理解呢？结合官方的解释：<strong>参数开启之后，会将timestamp当成是本地时区的时间</strong>。所以，当我们开启参数之后，2020-07-29 06:20:20这个时间就被当成了是UTC+8的时间（默认是UTC的时间），因此在cast成bigint的时候，会把它转换成UTC的时间，所以需要减掉8h，就变成了2020-07-28 22:20:20，也就是1595974820。</p>
<h5 id="cast-sec-as-timestamp"><a href="#cast-sec-as-timestamp" class="headerlink" title="cast(sec as timestamp)"></a>cast(sec as timestamp)</h5><p>当我们将bigint转换成timestamp的时候，前后2次的结果分别是：2020-07-29 06:20:20和2020-07-29 14:20:20，这里参数开启后，得到的值比开启前的多了8h！同样的道理，当参数开启之后，sec存储的是UTC的时间对应的bigint，即2020-07-29 06:20:20，所以转换成UTC+8之后，就变成了2020-07-29 14:20:20</p>
<h5 id="其他生效的情况"><a href="#其他生效的情况" class="headerlink" title="其他生效的情况"></a>其他生效的情况</h5><p>除了上面提到的cast情况，还有一些其他的情况也会生效，例如：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> unix_timestamp(ts),FROM_UNIXTIME(sec,<span class="string">&#x27;yyyy-MM-dd HH:mm:ss&#x27;</span>),TO_TIMESTAMP(sec) <span class="keyword">from</span> timestamp_test_parquet;</span><br></pre></td></tr></table></figure>
<p>结果如下：<br>|参数值|id|UNIX_TIMESTAMP(ts)|from_unixtime(sec, ‘yyyy-mm-dd hh:mm:ss’)|to_timestamp(sec)|<br>|–|–|–|–|–|<br>|false|1001|1596003620|2020-07-29 06:20:20|2020-07-29 06:20:20|<br>|true|1001|1595974820|2020-07-29 14:20:20|2020-07-29 14:20:20|<br>这个结果也与上面的cast情况保持一致：当timestmap-&gt;bigint时，是会减8h；当bigint-&gt;timestmap时，是会加8h；</p>
<h4 id="源码学习"><a href="#源码学习" class="headerlink" title="源码学习"></a>源码学习</h4><p>我们以to_timestamp这个函数为例，看下Impala是如何根据参数进行时区转换操作的，函数信息如下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">TO_TIMESTAMP(BIGINT unixtime), TO_TIMESTAMP(STRING date, STRING pattern)</span><br><span class="line">Purpose: Converts an integer <span class="keyword">or</span> <span class="built_in">string</span> representing a date/time value into the corresponding TIMESTAMP value.</span><br><span class="line">Return type: TIMESTAMP</span><br></pre></td></tr></table></figure>
<p>我们使用的是第一个，输入参数为bigint。首先在common/function-registry/impala_functions.py文件中找到这个udf对应的c++的函数，如下所示：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[&#x27;to_timestamp&#x27;], &#x27;TIMESTAMP&#x27;, [&#x27;BIGINT&#x27;],</span><br><span class="line"> &#x27;_ZN6impala18TimestampFunctions11ToTimestampEPN10impala_udf15FunctionContextERKNS1_9BigIntValE&#x27;],</span><br></pre></td></tr></table></figure>
<p>其中ToTimestamp就是该udf在BE端的实际函数名，然后我们按照代码调用顺序，就能找到如下相关的代码：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//相关代码为2.12.0版本</span></span><br><span class="line"><span class="comment">// timestamp-functions-ir.cc</span></span><br><span class="line"><span class="function">TimestampVal <span class="title">TimestampFunctions::ToTimestamp</span><span class="params">(FunctionContext* context,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> BigIntVal&amp; bigint_val)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (bigint_val.is_null) <span class="keyword">return</span> TimestampVal::null();</span><br><span class="line">  <span class="keyword">const</span> TimestampValue&amp; tv = TimestampValue::FromUnixTime(bigint_val.val);</span><br><span class="line">  TimestampVal tv_val;</span><br><span class="line">  tv.ToTimestampVal(&amp;tv_val);</span><br><span class="line">  <span class="keyword">return</span> tv_val;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// timestmap-value.h</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> TimestampValue <span class="title">FromUnixTime</span><span class="params">(<span class="keyword">time_t</span> unix_time)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> TimestampValue(UnixTimeToPtime(unix_time));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// timestmap-value.cc</span></span><br><span class="line"><span class="function">ptime <span class="title">TimestampValue::UnixTimeToPtime</span><span class="params">(<span class="keyword">time_t</span> unix_time)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (FLAGS_use_local_tz_for_unix_timestamp_conversions) &#123;</span><br><span class="line">    <span class="keyword">return</span> UnixTimeToLocalPtime(unix_time);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> UnixTimeToUtcPtime(unix_time);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>到这里就差不多可以看到了，参数开启或者关闭，Impala会调用不同的处理函数，来决定是否将输入的bigint转换为本地时间，有兴趣的读者可以自己阅读源码。3.x版本的代码整体改动比较大，可能与当前的有所不同，但是原理上应该类似。</p>
<h4 id="需要注意的地方"><a href="#需要注意的地方" class="headerlink" title="需要注意的地方"></a>需要注意的地方</h4><p>到这里，use_local_tz_for_unix_timestamp_conversions参数，我们就已经聊的差不多了，这里有以下几点需要注意：</p>
<ol>
<li>本文只探讨use_local_tz_for_unix_timestamp_conversions这个参数的作用，不涉及convert_legacy_hive_parquet_utc_timestamps这个参数，从始至终，convert_legacy_hive_parquet_utc_timestamps都是false；</li>
<li>笔者的测试环境为impala-2.12.0，如果是impala-3.x的版本，在执行sql之前需要设置：set timezone=’Asia/Shanghai’;，否则默认的时区是UTC，参数修改前后sql执行结果会一样。不同的时区请按照实际情况进行修改；</li>
<li>这里的测试表是parquet格式，text和kudu也同样适用，这点笔者已经验证过了。其他的格式，读者有兴趣的可以自行验证，应该也是都有效果的。<h4 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h4><a target="_blank" rel="noopener" href="https://docs.cloudera.com/documentation/enterprise/5-16-x/topics/impala_timestamp.html#timestamp">Impala cdh5.16 Timestamp Data Type</a><br><a target="_blank" rel="noopener" href="https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/impala_timestamp.html">Impala cdh6.3 Timestamp Data Type</a></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://skyyws.github.io/2021/04/16/%E5%85%B3%E4%BA%8EImpala%E7%9A%84use-local-tz-for-unix-timestamp-conversions%E5%8F%82%E6%95%B0%E6%8E%A2%E7%A9%B6/" data-id="cknkaavog0000b2x5ea5ahgbb" data-title="关于Impala的use_local_tz_for_unix_timestamp_conversions参数探究" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/olap/" rel="tag">olap</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/" rel="tag">经验总结</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Impala配置Ranger服务进行权限控制" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/16/Impala%E9%85%8D%E7%BD%AERanger%E6%9C%8D%E5%8A%A1%E8%BF%9B%E8%A1%8C%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6/" class="article-date">
  <time class="dt-published" datetime="2021-04-16T12:23:13.000Z" itemprop="datePublished">2021-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/16/Impala%E9%85%8D%E7%BD%AERanger%E6%9C%8D%E5%8A%A1%E8%BF%9B%E8%A1%8C%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6/">Impala配置Ranger服务进行权限控制</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Impala目前在新版本3.4中提供了Sentry和Ranger的2种权限管控，我们这里分别介绍一下最新的使用，这里跟2.12.0的版本有所不同。</p>
<h4 id="Sentry"><a href="#Sentry" class="headerlink" title="Sentry"></a>Sentry</h4><p>在impala-2.12.0版本中，我们可以通过如下配置来指定sentry文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-server_name&#x3D;hive-cluster1</span><br><span class="line">-authorization_policy_file&#x3D;&#x2F;user&#x2F;impala&#x2F;policy.cfg</span><br><span class="line">-authorization_policy_provider_class&#x3D;org.apache.sentry.provider.file.LocalGroupResourceAuthorizationProvider</span><br></pre></td></tr></table></figure>
<p>这样，即使没有Sentry服务，我们也可以进行权限的管理操作。但是在3.4版本中authorization_policy_file配置项被移除了，所以我们没办法直接配置hdfs上的Sentry文件作为权限管理了。因此需要通过配置sentry-site.xml来进行权限控制，关于最新版本的Sentry配置可以参考：<a target="_blank" rel="noopener" href="https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/impala_authorization.html">Impala权限管理
</a>。由于我们内部都是使用的Ranger服务，关于Sentry服务的配置，就不多做介绍。下面主要来讲一下Ranger服务的配置。</p>
<h4 id="Ranger"><a href="#Ranger" class="headerlink" title="Ranger"></a>Ranger</h4><p>社区相关JIRA：<a target="_blank" rel="noopener" href="https://issues.apache.org/jira/browse/IMPALA-7916">IMPALA-7916</a></p>
<p>Impala在新版本中提供了对Ranger的支持，我们可以通过直接配置Ranger的信息，来进行权限的管控。目前，我们可以直接使用测试的集群进行ranger的配置，具体信息参考：<a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/IMPALA/How+to+start+a+Ranger-enabled+Impala+minicluster">如何配置带Ranger的Impala集群
</a><br>Ranger服务起来之后，我们就可以通过hostname:6080来进行访问，登陆用户和密码默认是admin/admin，具体的Ranger配置位于：$RANGER_HOME/ews/webapp/WEB-INF/classes/conf/ranger-admin-site.xml。相关页面如下：<br><img src="https://img-blog.csdnimg.cn/20200814143830561.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70#pic_center" alt="Ranger主页面"><br><img src="https://img-blog.csdnimg.cn/20200814143845771.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70#pic_center" alt="test_impala policies"><br>我看可以看到，有一个名为test_impala的service，这个service里面配置了具体的各个用户以及对应的policy，这里主要就是admin用户。这里的test_impala对应的就是ranger-hive-security.xml中的如下配置项：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;ranger.plugin.hive.service.name&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;test_impala&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;description&gt;</span><br><span class="line">    Name of the Ranger service containing policies.</span><br><span class="line">  &lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
<p>配置完成之后，我们就可以使用文档里面的命令重启impala测试集群，然后进行测试：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;start-impala-cluster.py \</span><br><span class="line">--impalad_args&#x3D;&quot;--server-name&#x3D;server1 --ranger_service_type&#x3D;hive --ranger_app_id&#x3D;impala --authorization_provider&#x3D;ranger&quot; \</span><br><span class="line">--catalogd_args&#x3D;&quot;--server-name&#x3D;server1 --ranger_service_type&#x3D;hive --ranger_app_id&#x3D;impala --authorization_provider&#x3D;ranger&quot;</span><br></pre></td></tr></table></figure>
<p>关于上面各个参数的释义如下：</p>
<ul>
<li>server_name，该参数配置的值例如server1，从日志来看，会去加载ranger-hive-server1-audit.xml、ranger-hive-server1-security.xml和ranger-hive-server1-policymgr-ssl.xml这几个文件，其他作用暂时还没发现，在测试过程中，发现配置为其他值也可以正常加载权限；</li>
<li>ranger_service_type，表示ranger服务的类型，目前仅支持hive，impala会根据这个去加载ranger-hive-audit.xml、ranger-hive-security.xml和ranger-hive-policymgr-ssl.xml这几个文件；</li>
<li>ranger_app_id，该参数测试发现也并没有什么特别的作用，默认配置为impala，作为标识；</li>
<li>authorization_provider，不配置表示禁用权限控制功能，目前支持配置为ranger；</li>
</ul>
<p>我们使用$IMPALA_HOME/shell/build/impala-shell-4.0.0-SNAPSHOT/impala-shell -u hive命令连接到测试进行，然后创建database，会提示没有权限，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[localhost:21000] default&gt; create database ranger_test;</span><br><span class="line">Query: create database ranger_test</span><br><span class="line">ERROR: AuthorizationException: User &#39;hive&#39; does not have privileges to execute &#39;CREATE&#39; on: ranger_test</span><br></pre></td></tr></table></figure>
<p>这是由于测试的配置里面默认没有hive用户，因此我们需要在ranger里面新建一个hive的User，然后将hive加入all-database这个policy，如下所示：<br><img src="https://img-blog.csdnimg.cn/20200814144150287.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70#pic_center" alt="policy_new"><br>配置完成之后，我们再执行上述的SQL发现可以执行成功：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[localhost:21000] default&gt; create database ranger_test;</span><br><span class="line">Query: create database ranger_test</span><br><span class="line">+----------------------------+</span><br><span class="line">| summary                    |</span><br><span class="line">+----------------------------+</span><br><span class="line">| Database has been created. |</span><br><span class="line">+----------------------------+</span><br><span class="line">Fetched 1 row(s) in 0.38s</span><br></pre></td></tr></table></figure>
<p>需要注意的是，配置ranger的文件ranger-hive-security.xml和ranger-hive-audit.xml位于$IMPALA_HOME/fe/src/test/resources/，依赖组件的这些xml配置都是通过执行bin/create-test-configuration.sh脚本自动生成的。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://skyyws.github.io/2021/04/16/Impala%E9%85%8D%E7%BD%AERanger%E6%9C%8D%E5%8A%A1%E8%BF%9B%E8%A1%8C%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6/" data-id="cknka9hb600007rx571llh8xt" data-title="Impala配置Ranger服务进行权限控制" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/olap/" rel="tag">olap</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/" rel="tag">经验总结</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-记一次Apache-Kylin的慢查询排查及优化" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/16/%E8%AE%B0%E4%B8%80%E6%AC%A1Apache-Kylin%E7%9A%84%E6%85%A2%E6%9F%A5%E8%AF%A2%E6%8E%92%E6%9F%A5%E5%8F%8A%E4%BC%98%E5%8C%96/" class="article-date">
  <time class="dt-published" datetime="2021-04-16T12:22:01.000Z" itemprop="datePublished">2021-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/16/%E8%AE%B0%E4%B8%80%E6%AC%A1Apache-Kylin%E7%9A%84%E6%85%A2%E6%9F%A5%E8%AF%A2%E6%8E%92%E6%9F%A5%E5%8F%8A%E4%BC%98%E5%8C%96/">记一次Apache Kylin的慢查询排查及优化</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>目前业务在使用Kylin的时候反馈查询很慢，直接超时了（超时时间设置的为5min），在日志中获取了相应的SQL以及Cube之后发现：</p>
<ol>
<li>SQL扫描了不到2个月的数据，500多G大小，26亿的记录数；</li>
<li>SQL中涉及到了4个count distinct计算（bitmap）；</li>
</ol>
<p>当前集群环境如下：</p>
<ol>
<li>Kylin服务器2台，部署了2个节点，都是all；</li>
<li>HBase集群服务器4台，每台部署了2个regionserver；</li>
<li>Kylin版本为2.6.6；</li>
</ol>
<p>该cube设置的合并时间为7天/28天，Storage情况如下所示：</p>
<ol>
<li>有一个时间跨度为1个月的segment，大小270G，region数量为10；</li>
<li>有两个时间跨度为1周的segment，大小为90G，region数量为10；</li>
<li>若干个时间跨度为1天的segment，大小为13G，region数量为4个，记录数大概是7000W；</li>
</ol>
<p>初步猜测是因为扫描的数据太多了，而且涉及到多个count distinct的计算，所以比较耗时。而且在执行SQL的过程中，我们发现HBase集群的CPU也没有明显的飙升。首先怀疑是因为region数量设置的太少，导致并发度不够，因此CPU没有完全利用起来，当前设置的最大region数量是10。为了进行验证，我们在这个cube上配置最大region数量为50，然后refresh了其中一个1周的数据，refresh前后对比如下：<br>|操作|segment| region个数 |查询时间|<br>|–|–|–|–|<br>| 刷新前 | [20200801,20200808)|10 |37.11/35.69/36.33/36.25/35.84|<br>|刷新后| [20200801,20200808)|25|37.36/36.18/36.93/35.74/35.45|<br>我们可以发现，即使segment对应的HBase表的region数量增多了，对于查询也并没有提升。我们通过仔细查看上述SQL的相关日志发现如下所示的情况：<br><img src="https://img-blog.csdnimg.cn/20200821113337280.png#pic_center" alt="查询1周的数据"><br>由于我们测试时候只查询了1周的数据，也只有1个segment，所以我们在日志中发现，只有1个request发送到了1个regionserver上，结合Kylin的coprocessor相关代码（相关代码位于CubeHBaseEndpointRPC.java，有兴趣的可以自行阅读），我们发现：<strong>Kylin中的一个hbase表（即一个segment）对应一个coprocessor，查询过程中会先发送一个request到一个segionserver上，然后该HBase会把该表的数据都拉到这个regionserver上，使用coprocessor进行聚合运算</strong>。基于这个情况，我们目前判断应该是segment越多的情况下，扫描会越快。因此我们又进行了下述的测试，相关信息如下：<br>|segment区间|segment数量|region个数 |查询时间|<br>|–|–|–|–|<br>| [20200801,20200806)|1|25 |27.27/26.46/25.44/24.97/24.88|<br>| [20200816,20200821)|5|5*4|6.77/6.47/6.34/6.55/6.43|<br>我们可以看到，查询同样5天的数据，5个segment的情况，比1个segment的情况，查询性能好了4倍以上。这也证实了我们上面的结论。到这里，我们这个问题就已经排查的差不多了。<br>针对这个问题，我们进行了如下的调整：</p>
<ol>
<li>Clone该cube，然后新的cube去掉segment的合并配置，用新的cube跑完数据之后，disable掉老的cube，需要指的注意的是，两个cube涉及到的表都是一样的，查询的时候可能会路由到不同的cube；</li>
<li>由于当前还是只查询了1周的数据，未来的查询时间跨度可能更长，因此我们准备对HBase进行相应的扩容。</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://skyyws.github.io/2021/04/16/%E8%AE%B0%E4%B8%80%E6%AC%A1Apache-Kylin%E7%9A%84%E6%85%A2%E6%9F%A5%E8%AF%A2%E6%8E%92%E6%9F%A5%E5%8F%8A%E4%BC%98%E5%8C%96/" data-id="cknka7oac000057x50s348t5v" data-title="记一次Apache Kylin的慢查询排查及优化" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kylin/" rel="tag">kylin</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/olap/" rel="tag">olap</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/" rel="tag">问题排查</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-问题排查-Kylin开启G1垃圾回收算法导致进程无法启动" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/16/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Kylin%E5%BC%80%E5%90%AFG1%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%97%E6%B3%95%E5%AF%BC%E8%87%B4%E8%BF%9B%E7%A8%8B%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8/" class="article-date">
  <time class="dt-published" datetime="2021-04-16T12:20:31.000Z" itemprop="datePublished">2021-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/16/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Kylin%E5%BC%80%E5%90%AFG1%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%97%E6%B3%95%E5%AF%BC%E8%87%B4%E8%BF%9B%E7%A8%8B%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8/">问题排查--Kylin开启G1垃圾回收算法导致进程无法启动</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Kylin的JVM参数可以通过修改$conf/setenv.sh中的KYLIN_JVM_SETTINGS参数来进行配置，默认使用的GC算法是ParNew+CMS，我们可以通过jcmd pid VM.flags来查看进程的JVM参数，如下所示：<br><img src="https://img-blog.csdnimg.cn/20201111103853168.png#pic_center" alt="kylin_jvm_01"><br>最近由于查询和任务变多，Kylin节点在高峰期会出现由于GC导致服务停顿时间比较久的情况，因此准备将GC算法调整为G1，我们直接修改KYLIN_JVM_SETTINGS，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export KYLIN_JVM_SETTINGS&#x3D;&quot;-Xms128G -Xmx128G -XX:+UseG1GC -XX:+UnlockExperimentalVMOptions -XX:MaxGCPauseMillis&#x3D;90 -XX:G1NewSizePercent&#x3D;5 -XX:InitiatingHeapOccupancyPercent&#x3D;65 -XX:+ParallelRefProcEnabled -XX:ConcGCThreads&#x3D;4 -XX:ParallelGCThreads&#x3D;16 -XX:MaxTenuringThreshold&#x3D;1 -XX:G1MixedGCCountTarget&#x3D;64 -XX:G1OldCSetRegionThresholdPercent&#x3D;5 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:$KYLIN_HOME&#x2F;logs&#x2F;kylin.gc.$$ -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles&#x3D;10 -XX:GCLogFileSize&#x3D;64M&quot;</span><br></pre></td></tr></table></figure>
<p>但是在执行./kylin.sh start启动服务的时候，脚本正常执行完成，但是实际进程并没有起来。查看kylin.out日志发现如下的错误：<br><img src="https://img-blog.csdnimg.cn/20201111104301648.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70#pic_center" alt="kylin_jvm_02"><br>网上搜索该错误之后发现，该错误主要是由于配置了多种GC算法导致。但是我们在kylin端只配置了G1算法，所以应该是其他地方引入的。经过排查之后，我们发现在服务器上配置的$HBASE_CLIENT/conf/hbase-env.sh中存在如下的配置：<br><img src="https://img-blog.csdnimg.cn/20201111110101320.png#pic_center" alt="kylin_jvm_03"><br>我们将这个地方改成如下配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HBASE_OPTS&#x3D;&quot;-XX:+UseG1GC&quot;</span><br></pre></td></tr></table></figure>
<p>此时再启动Kylin服务，问题解决。我们可以通过查看新进程的jvm参数，如下所示：<br><img src="https://img-blog.csdnimg.cn/20201111110351522.png#pic_center" alt="kylin_jvm_04"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://skyyws.github.io/2021/04/16/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Kylin%E5%BC%80%E5%90%AFG1%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%97%E6%B3%95%E5%AF%BC%E8%87%B4%E8%BF%9B%E7%A8%8B%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8/" data-id="cknka5ptc00002ex52pan91ue" data-title="问题排查--Kylin开启G1垃圾回收算法导致进程无法启动" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kylin/" rel="tag">kylin</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/olap/" rel="tag">olap</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/" rel="tag">问题排查</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-问题排查-Impala查询Decimal数据为NULL，Hive查询正常" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/16/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Impala%E6%9F%A5%E8%AF%A2Decimal%E6%95%B0%E6%8D%AE%E4%B8%BANULL%EF%BC%8CHive%E6%9F%A5%E8%AF%A2%E6%AD%A3%E5%B8%B8/" class="article-date">
  <time class="dt-published" datetime="2021-04-16T12:18:35.000Z" itemprop="datePublished">2021-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/16/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Impala%E6%9F%A5%E8%AF%A2Decimal%E6%95%B0%E6%8D%AE%E4%B8%BANULL%EF%BC%8CHive%E6%9F%A5%E8%AF%A2%E6%AD%A3%E5%B8%B8/">问题排查--Impala查询Decimal数据为NULL，Hive查询正常</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>目前，线上反馈一个问题：同一张表，使用Hive查询正常，但是使用Impala查询，返回的数据中，部分字段值为NULL。<br>我们使用impala执行了invalidate metadata xxx，排查了元数据不一致的问题，同时查看源文件，也排除了数据本身的问题。最终，通过在Impalad的web页面上查看该SQL的profile发现，其中存在如下的Errors：<br><img src="https://img-blog.csdnimg.cn/20201112164132329.png#pic_center" alt="impala_decimal_01"><br>通过查询相关的错误，我们发现官方对于这块有相应地解释：<br><em><strong>For text-based formats (text, RCFile, and SequenceFile tables), you can issue an ALTER TABLE … REPLACE COLUMNS statement to change the precision and scale of an existing DECIMAL column. As long as the values in the column fit within the new precision and scale, they are returned correctly by a query. Any values that do not fit within the new precision and scale are returned as NULL, and Impala reports the conversion error. Leading zeros do not count against the precision value, but trailing zeros after the decimal point do.</strong></em><br>这段话主要的意思就是，如果通过alter table操作，修改了DECIMAL类型的precision和scale，如果实际值和新设置的DECIMAL不匹配的话，那么就会返回NULL。例如：如果数据是1.234，column定义是DECIMAL(4, 2)，那么就会返回NULL。<br>我们可以通过如下的SQL进行简单的测试验证：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">create table text_decimals (x string);</span><br><span class="line">insert into text_decimals values (&quot;1&quot;), (&quot;2&quot;), (&quot;99.99&quot;), (&quot;1.234&quot;), (&quot;000001&quot;), (&quot;1.000000000&quot;);</span><br><span class="line">alter table text_decimals replace columns (x decimal(4,2));</span><br><span class="line">select * from text_decimals;</span><br></pre></td></tr></table></figure>
<p>如果用Impala执行最后一行，那么返回的记录中，第四行和第六行都是NULL；如果用Hive执行，则第四行和第六行分别是1.23和1.00。感兴趣的同事可以自行测试一下。<br>目前Impala还没有提供参数项配置，可以像Hive一样返回一个四舍五入的近似值，因此我们需要保证在定义的时候，不会出现这种情况。但是在实际测试的过程中我们发现，对于1.234，如果column定义是DECIMAL(4, 5)，那么查询可以正常返回1.234，这意味着Impala是不允许精度丢失。值得注意的是，如果我们先执行<strong>set abort_on_error=1</strong>，再执行select查询，那么SQL会直接返回失败，而不是NULL，如下所示：<br><img src="https://img-blog.csdnimg.cn/20201112165241354.png#pic_center" alt="impala_decimal_02"><br>官方链接参考：<a target="_blank" rel="noopener" href="https://docs.cloudera.com/documentation/enterprise/5-16-x/topics/impala_decimal.html#decimal">https://docs.cloudera.com/documentation/enterprise/5-16-x/topics/impala_decimal.html#decimal</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://skyyws.github.io/2021/04/16/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Impala%E6%9F%A5%E8%AF%A2Decimal%E6%95%B0%E6%8D%AE%E4%B8%BANULL%EF%BC%8CHive%E6%9F%A5%E8%AF%A2%E6%AD%A3%E5%B8%B8/" data-id="cknka3jxw0000zax57sjf0g30" data-title="问题排查--Impala查询Decimal数据为NULL，Hive查询正常" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/olap/" rel="tag">olap</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/" rel="tag">问题排查</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Impala-2-12-0与3-4-0版本的compute-stats兼容问题" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/16/Impala-2-12-0%E4%B8%8E3-4-0%E7%89%88%E6%9C%AC%E7%9A%84compute-stats%E5%85%BC%E5%AE%B9%E9%97%AE%E9%A2%98/" class="article-date">
  <time class="dt-published" datetime="2021-04-16T12:17:25.000Z" itemprop="datePublished">2021-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/16/Impala-2-12-0%E4%B8%8E3-4-0%E7%89%88%E6%9C%AC%E7%9A%84compute-stats%E5%85%BC%E5%AE%B9%E9%97%AE%E9%A2%98/">Impala 2.12.0与3.4.0版本的compute stats兼容问题</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>对于Impala来说，compute [incremental] stats [partition_spec]是我们经常会使用到的语句，这个语句的功能就是对表，执行统计信息计算。Impala在进行SQL解析的时候，就可以利用这些统计信息进行更好地优化，生成更高效地执行计划。但是，最近我们在将集群从2.12.0升级到3.4.0版本的时候，遇到了一些compute stats相关的问题。<br>本文在第一章和第三章分别描述了问题以及重现的步骤，第二章是详细的代码探究。如果不感兴趣的话，可以直接略过。</p>
<h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>当我们在3.4.0版本，对表的某个具体分区执行compute incremental stats table_name [partition_spec]时，发现执行过程中，会出现TableLoadingException的异常，如下所示：<br><img src="https://img-blog.csdnimg.cn/20201223153428925.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="exception"><br>这个exception主要是由于列统计信息不符合约束导致的，这里就是由于numNulls_的约束检查失败导致的。我们可以在相关的类中找到如下代码，该方法在2.12.0中是不存在的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 3.4.0  ColumnStats.java</span><br><span class="line">public void validate(Type colType) &#123;</span><br><span class="line">  &#x2F;&#x2F; avgSize_ and avgSerializedSize_ must be set together.</span><br><span class="line">  Preconditions.checkState(avgSize_ &gt;&#x3D; 0 &#x3D;&#x3D; avgSerializedSize_ &gt;&#x3D; 0, this);</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; Values must be either valid or -1.</span><br><span class="line">  Preconditions.checkState(avgSize_ &#x3D;&#x3D; -1 || avgSize_ &gt;&#x3D; 0, this);</span><br><span class="line">  Preconditions.checkState(avgSerializedSize_ &#x3D;&#x3D; -1 || avgSerializedSize_ &gt;&#x3D; 0, this);</span><br><span class="line">  Preconditions.checkState(maxSize_ &#x3D;&#x3D; -1 || maxSize_ &gt;&#x3D; 0, this);</span><br><span class="line">  Preconditions.checkState(numDistinctValues_ &#x3D;&#x3D; -1 || numDistinctValues_ &gt;&#x3D; 0, this);</span><br><span class="line">  Preconditions.checkState(numNulls_ &#x3D;&#x3D; -1 || numNulls_ &gt;&#x3D; 0, this);</span><br><span class="line">  if (colType !&#x3D; null &amp;&amp; colType.isFixedLengthType()) &#123;</span><br><span class="line">    Preconditions.checkState(avgSize_ &#x3D;&#x3D; colType.getSlotSize(), this);</span><br><span class="line">    Preconditions.checkState(avgSerializedSize_ &#x3D;&#x3D; colType.getSlotSize(), this);</span><br><span class="line">    Preconditions.checkState(maxSize_ &#x3D;&#x3D; colType.getSlotSize(), this);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们观察Impala的页面发现，compute stats的两条相关SQL执行是成功的，如下所示：<br><img src="https://img-blog.csdnimg.cn/20201223153455744.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="web"><br>这就说明，新版本部署启动之后，第一次加载这个表是正常的，并且compute stats相关的两条SQL都能正常执行成功。因此，问题应该是出在了计算完成之后，更新到metastore中导致的，我们通过查看元数据库对应的表发现，确实numNulls_对应的值是-6（这些统计信息位于元数据库的TAB_COL_STATS表，其中numNulls_对应的列是NUM_NULLS）。</p>
<h4 id="深入研究"><a href="#深入研究" class="headerlink" title="深入研究"></a>深入研究</h4><h5 id="两个版本统计信息对比"><a href="#两个版本统计信息对比" class="headerlink" title="两个版本统计信息对比"></a>两个版本统计信息对比</h5><p>当我们对表执行了compute stats之后，我们可以通过show column stats table_name来查看表的列统计信息，如下所示：<br><img src="https://img-blog.csdnimg.cn/20201223153514771.png" alt="初始状态"><br>以上是我们表的初始列统计信息状态，当我们执行了compute stats table_name之后，2.12.0版本的结果如下所示：<br><img src="https://img-blog.csdnimg.cn/20201223153534447.png" alt="在这里插入图片描述"><br>而3.4.0版本的结果如下所示：<br><img src="https://img-blog.csdnimg.cn/20201223153553993.png" alt="3.4.0版本"><br>通过上面两幅图对比，我们可以发现，“#Nulls”这一列在两个版本中的值是不一样的。2.12.0版本对于这一列，即使执行了compute stats之后，仍然是-1（除去分区列），而3.4.0版本则是实际的数值，是大于等于0的。这里的“#Nulls”列对应的就是异常日志中的“numNulls”。<br>值得一提的是，对于每一个分区（Hdfs表对应的是THdfsPartition结构体，这个thrift结构体包括了单个分区详细信息，如果存在多层分区的话，那么该结构体包含的是到最里层的分区，例如day=20200101/type=xxx/id=xxx这种），其中每一个THdfsPartition都有一个has_incremental_stats变量，这个变量默认是false（即没有执行compute stats），当我们执行相关SQL的时候，情况分别如下所示：</p>
<ul>
<li>执行compute stats table_name，所有分区对应的has_incremental_stats参数变为false；</li>
<li>执行compute incremental stats table_name，所有分区对应的has_incremental_stats参数变为true；</li>
<li>执行compute incremental stats table_name partition(day=’2020-12-01’)，这个分区对应的has_incremental_stats参数变为true，其他分区的仍然为false。</li>
</ul>
<p>我们这里讨论的前提是：这个表是默认没有进行任何的compute stats操作的，上述情况对于2.12.0和3.4.0都是同样的情况。因此，当has_incremental_stats为true，就表示对应的某个分区包含了增量的历史统计信息，而初始状态，或者compute stats table_name是不算做增量统计信息计算的。我们后续的分析，都是基于增量的统计信息计算。</p>
<h5 id="初始状态分析"><a href="#初始状态分析" class="headerlink" title="初始状态分析"></a>初始状态分析</h5><p>为了研究，到底是哪里导致的这个“#Nulls”的值小于-1，我们接下来跟着代码一步一步看下去（这里以3.4.0的代码为例）。<br>首先，假设表为初始状态，当我们第一次访问，加载表的时候，此时表没有任何统计信息，加载操作由catalogd执行。Catalogd会对表信息、分区信息等进行初始化，主要就是从metastore中进行加载，然后转换成Impala对应的各种类和结构体。我们这里主要关注HDFS表的加载，下面我们简单看下相关的函数调用栈：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">HdfsTable.load()</span><br><span class="line">-HdfsTable.loadAllPartitions()</span><br><span class="line">--HdfsTable.createPartition() 分区表会循环调用这个函数</span><br><span class="line">---HdfsPartition.ctor() public</span><br><span class="line">----HdfsPartition.ctor() private</span><br><span class="line">-----HdfsPartition.extractAndCompressPartStats()</span><br><span class="line">------PartitionStatsUtil.partStatsBytesFromParameters()</span><br><span class="line">------HdfsPartition.setPartitionStatsBytes() 使用上面函数的返回结果作为输入参数</span><br></pre></td></tr></table></figure>
<p>可以看到，在加载表分区的信息时，会调用partStatsBytesFromParameters这个函数，我们将相关的代码粘贴出来：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">  &#x2F;&#x2F; PartitionStatsUtil.java</span><br><span class="line">  public static final String INCREMENTAL_STATS_NUM_CHUNKS &#x3D;</span><br><span class="line">    &quot;impala_intermediate_stats_num_chunks&quot;;</span><br><span class="line"> &#x2F;&#x2F; 省略后续代码</span><br><span class="line">&#x2F;**</span><br><span class="line"> * Reconstructs the intermediate stats from chunks and returns the corresponding</span><br><span class="line"> * byte array. The output byte array is deflate-compressed. Sets hasIncrStats to</span><br><span class="line"> * &#39;true&#39; if the partition stats contain intermediate col stats.</span><br><span class="line"> *&#x2F;</span><br><span class="line">public static byte[] partStatsBytesFromParameters(</span><br><span class="line">    Map&lt;String, String&gt; hmsParameters, Reference&lt;Boolean&gt; hasIncrStats) throws</span><br><span class="line">    ImpalaException &#123;</span><br><span class="line">  if (hmsParameters &#x3D;&#x3D; null) return null;</span><br><span class="line">  String numChunksStr &#x3D; hmsParameters.get(INCREMENTAL_STATS_NUM_CHUNKS);</span><br><span class="line">  if (numChunksStr &#x3D;&#x3D; null) return null;</span><br><span class="line">  int numChunks &#x3D; Integer.parseInt(numChunksStr);</span><br><span class="line">  if (numChunks &#x3D;&#x3D; 0) return null;</span><br><span class="line"> &#x2F;&#x2F; 省略后续代码</span><br></pre></td></tr></table></figure>
<p>我们通过上述函数代码可以看到：当分区的参数列表中（分区的参数列表，可以直接从metastore中加载），没有INCREMENTAL_STATS_NUM_CHUNKS参数时，整个函数会返回null。这里先提一下，当函数初始没有计算统计信息的时候，就不会有这个参数，后续我们还会再提到这个参数。紧接着就会使用上述函数的返回结果，来执行setPartitionStatsBytes这个函数。这里又涉及到了两个成员变量：partitionStats_和hasIncrementalStats_。这里我们暂且不详细探究其含义，我们只需要知道，在初始状态下，对于具体的某个分区而言，partitionStats_为null，而hasIncrementalStats_为false。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; Binary representation of the TPartitionStats for this partition. Populated</span><br><span class="line">&#x2F;&#x2F; when the partition is loaded and updated using setPartitionStatsBytes().</span><br><span class="line">private byte[] partitionStats_;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; True if partitionStats_ has intermediate_col_stats populated.</span><br><span class="line">private boolean hasIncrementalStats_ ;</span><br><span class="line">&#x2F;&#x2F; 省略后续代码</span><br><span class="line">public void setPartitionStatsBytes(byte[] partitionStats, boolean hasIncrStats) &#123;</span><br><span class="line">  if (hasIncrStats) Preconditions.checkNotNull(partitionStats);</span><br><span class="line">  partitionStats_ &#x3D; partitionStats;</span><br><span class="line">  hasIncrementalStats_ &#x3D; hasIncrStats;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于初始的状态而言，2.12.0和3.4.0版本，虽然在代码处理逻辑上有所不同，但是总体而言，这些主要的结构和成员变量都是相差不大的。</p>
<h5 id="增量计算后的状态变化"><a href="#增量计算后的状态变化" class="headerlink" title="增量计算后的状态变化"></a>增量计算后的状态变化</h5><p>上面我们已经了解一些，初始状态下的分区相关统计信息。现在来看一下，当我们执行了compute [incremental] stats [partition_spec]之后，状态会发生哪些变化，而这也跟我们最初的问题有关系。<br>当我们提交了SQL之后，Impala会自动提交两条子SQL来进行相应信息的获取，相关的SQL我们可以在第一章的第二幅图中看到，3.4.0和2.12.0版本的两个SQL略有不同。从截图中我们可以看到，这两条SQL的执行是没有问题，因此我们当前不关注这两条SQL的生成以及执行，着重于后续的统计信息更新部分。<br>我们假设当前表处于初始状态，此时提交执行compute incremental stats table_name partition(day=’2020-12-01’)这种SQL。我们来看一下状态是如何更新的：<br>首先，当SQL提交到Impalad的时候，会进行一系列的计算操作，主要就是执行上述的两个子查询。计算完成之后，会生成相应的变量来保存信息，然后将变量传到catalogd进程进行元数据的更新。这里我们先看下catalogd的相关处理流程，主要的api调用如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ExecDdl(catalog-server.cc)</span><br><span class="line">-ExecDdl(catalog.cc)</span><br><span class="line">--execDdl(JniCatalog.java)</span><br><span class="line">---CatalogOpExecutor.execDdlRequest</span><br><span class="line">----CatalogOpExecutor.alterTable</span><br></pre></td></tr></table></figure>
<p>可以看到，catalogd首先是在c++端通过JNI调用了Java的api，最终执行了一个alterTable的函数，来更新表的元数据信息（这里主要是统计信息），这里我们涉及到了一些thrift结构体信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">CatalogService.TDdlExecRequest</span><br><span class="line">-alter_table_params: JniCatalog.TAlterTableParams</span><br><span class="line">--update_stats_params: JniCatalog.TAlterTableUpdateStatsParams</span><br><span class="line">---partition_stats: map&lt;list&lt;string&gt;, CatalogObjects.TPartitionStats&gt;</span><br><span class="line">----CatalogObjects.TPartitionStats</span><br><span class="line">-----intermediate_col_stats: map&lt;string, TIntermediateColumnStats&gt;</span><br><span class="line">------CatalogObjects.TIntermediateColumnStats</span><br><span class="line">---column_stats: map&lt;string, CatalogObjects.TColumnStats&gt;</span><br><span class="line">----CatalogObjects.TColumnStats</span><br></pre></td></tr></table></figure>
<p>我们将一些相关的结构体包含关系列了出来。从上面的包含关系可以看到：本次计算涉及到的分区都会保存在partition_stats这个数组中，数组的每一个成员都是一个TPartitionStats结构体，代表一个分区的信息。这个结构体主要包括两个成员：TTableStats和一个map，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; Per-partition statistics</span><br><span class="line">struct TPartitionStats &#123;</span><br><span class="line">  &#x2F;&#x2F; so would interfere with the non-incremental stats path</span><br><span class="line">  1: required TTableStats stats</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; Intermediate state for incremental statistics, one entry per column name.</span><br><span class="line">  2: optional map&lt;string, TIntermediateColumnStats&gt; intermediate_col_stats</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>每个TPartitionStats都包含一个map，叫intermediate_col_stats。这个map的key表示列名，而value就是TIntermediateColumnStats，表的每一列都会对应一条KV记录。其中TIntermediateColumnStats的结构体如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; Intermediate state for the computation of per-column stats. Impala can aggregate these</span><br><span class="line">&#x2F;&#x2F; structures together to produce final stats for a column.</span><br><span class="line">struct TIntermediateColumnStats &#123;</span><br><span class="line">  &#x2F;&#x2F; One byte for each bucket of the NDV HLL computation</span><br><span class="line">  1: optional binary intermediate_ndv</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; If true, intermediate_ndv is RLE-compressed</span><br><span class="line">  2: optional bool is_ndv_encoded</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; Number of nulls seen so far (or -1 if nulls are not counted)</span><br><span class="line">  3: optional i64 num_nulls</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; The maximum width, in bytes, of the column</span><br><span class="line">  4: optional i32 max_width</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; The average width (in bytes) of the column</span><br><span class="line">  5: optional double avg_width</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; The number of rows counted, needed to compute NDVs from intermediate_ndv</span><br><span class="line">  6: optional i64 num_rows</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果执行了增量的统计信息计算，那么partition_stats这个变量就会包含当前正在进行计算的各个分区信息，而每个分区又会包含各自的intermediate_col_stats成员，其中有相应的列的统计信息。这里需要注意的是，3.4.0版本和2.12.0版本是不一样的：</p>
<ul>
<li>在两个版本中，初始状态下，列的num_nulls都是-1；</li>
<li>在3.4.0版本，如果执行了统计信息计算，num_nulls是一个大于等于0的值；</li>
<li>在2.12.0版本，如果执行了统计信息计算，num_nulls仍然是-1；</li>
</ul>
<p>但是，如果我们执行的是compute stats，而不是增量的话，那么每个分区的intermediate_col_stats是空的（注意，partition_stats不为空，其包含的stats也不为空，只是intermediate_col_stats这个变量为空）。这块的处理主要是在BE端进行的，只有当执行增量统计信息计算的时候，才会将分区的列统计信息存入intermediate_col_stats中，相关代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; catalog-op-executor.cc ExecComputeStats()</span><br><span class="line">&#x2F;&#x2F; 其中FinalizePartitionedColumnStats方法就是用来构造intermediate_col_stats的相关信息</span><br><span class="line">&#x2F;&#x2F; col_stats_schema and col_stats_data will be empty if there was no column stats query.</span><br><span class="line">if (!col_stats_schema.columns.empty()) &#123;</span><br><span class="line">  if (compute_stats_params.is_incremental) &#123;</span><br><span class="line">    RuntimeProfile::Counter* incremental_finalize_timer &#x3D;</span><br><span class="line">        ADD_TIMER(profile_, &quot;FinalizeIncrementalStatsTimer&quot;);</span><br><span class="line">    SCOPED_TIMER(incremental_finalize_timer);</span><br><span class="line">    FinalizePartitionedColumnStats(col_stats_schema,</span><br><span class="line">        compute_stats_params.existing_part_stats,</span><br><span class="line">        compute_stats_params.expected_partitions,</span><br><span class="line">        col_stats_data, compute_stats_params.num_partition_cols, &amp;update_stats_params);</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    SetColumnStats(col_stats_schema, col_stats_data, &amp;update_stats_params);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里我们可以看到，只有执行增量统计信息计算的时候（is_incremental为true），FinalizePartitionedColumnStats函数才会被调用。我们接着上面的catalogd处理流程继续往下看：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">CatalogOpExecutor.alterTable</span><br><span class="line">-CatalogOpExecutor.alterTableUpdateStats</span><br><span class="line">--CatalogOpExecutor.alterTableUpdateStatsInner</span><br><span class="line">---CatalogOpExecutor.updatePartitionStats</span><br><span class="line">----PartitionStatsUtil.partStatsToPartition</span><br><span class="line">-----HdfsPartition.setPartitionStatsBytes</span><br><span class="line">---CatalogOpExecutor.bulkAlterPartitions</span><br><span class="line">----HdfsPartition.toHmsPartition</span><br><span class="line">-----PartitionStatsUtil.partStatsToParams</span><br></pre></td></tr></table></figure>
<p>统计信息计算的结果，最终通过catalogd对表的分区进行了元数据更新，上述updatePartitionStats函数调用后两步，刚好与我们第一节中，提到的HDFS表加载形成了呼应，我们提到的INCREMENTAL_STATS_NUM_CHUNKS参数也会在partStatsToParams函数中进行设置。之后如果表再重新加载元数据的话，partStatsBytesFromParameters就不会返回空了。<br>除此之外，我们之前提到的partitionStats_和hasIncrementalStats_，最终在这里也进行了设置。我们将本节中涉及到的partition_stats数组，通过循环处理，将数组中的成员TPartitionStats进行压缩，最终保存到了HdfsPartition的partitionStats_成员变量中；hasIncrementalStats_保存是一个布尔值：TPartitionStats中的intermediate_col_stats成员是否为空，就是我们是否对该分区执行了增量统计信息计算（上面的分析已经提到过，只有执行增量统计信息计算的时候，intermediate_col_stats才不会为空）。这里我们对几种情况进行归纳：<br>| 状态 | INCREMENTAL_STATS_NUM_CHUNKS | partition_stats | intermediate_col_stats | partitionStats_ | hasIncrementalStats_ |<br>| — | — | — | — | — | — |<br>| 初始状态 | 不包括 | 空 | 空 | 空 | false |<br>| compute stats | 包括 | 不为空 | 空 | 不为空 |false  |<br>| 增量compute stats | 包括 | 不为空 |不为空  | 不为空 | true |<br>这里有几个地方，我们需要注意一下：</p>
<ul>
<li>partition_stats包含了本次操作涉及到的分区信息集合，而partitionStats_和hasIncrementalStats_是针对单个分区的信息；</li>
<li>partitionStats_是由partition_stats中的单个成员，也就是TPartitionStats，经过处理之后得到的；</li>
<li>intermediate_col_stats是TPartitionStats的一个成员，所以它是否为空，不会影响TPartitionStats，继而也不会影响partitionStats_；</li>
</ul>
<p>总结一下，当我们执行compute incremental stats [partition_spec]的时候，会在Impalad的BE端根据SQL解析和计算的结果，构造一个TDdlExecRequest变量，并且传到catalogd端，catalogd通过JNI调用Java的api对表的元数据信息进行更新。之后如果再加载表的元数据时，就能获取到这些已经计算的增量统计信息。</p>
<h5 id="错误产生分析"><a href="#错误产生分析" class="headerlink" title="错误产生分析"></a>错误产生分析</h5><p>上一节提到，当我们执行了compute incremental stats [partition_spec]的时候，表就会包含一些增量的统计信息，例如partitionStats_。因为我们最开始是在2.12.0版本每天执行了增量的统计信息计算，当我们升级到3.4.0版本之后，HDFS表被加载起来之后，就会包含相关的历史增量统计信息。此时，当我们在3.4.0版本再次执行增量统计信息计算的时候，就会出现了第一章中的问题。接下来就结合代码来看一下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">executeAndWait(impala-beeswax-server.cc)</span><br><span class="line">-Execute(impala-server.cc)</span><br><span class="line">--ExecuteInternal(impala-server.cc)</span><br><span class="line">---Exec(client-request-stats.cc)</span><br><span class="line">----ExecDdlRequest(client-request-stats.cc)</span><br><span class="line">-----ExecAsync(child-query.cc)</span><br><span class="line">------ExecChildQueries(child-query.cc)</span><br><span class="line">-------ExecAndFetch(child-query.cc)</span><br><span class="line">-Wait(client-request-state.cc)</span><br><span class="line">--WaitInternal(client-request-state.cc)</span><br><span class="line">---UpdateTableAndColumnStats(client-request-state.cc)</span><br><span class="line">----ExecComputeStats(catalog-op-executor.cc)</span><br><span class="line">-----SetTableStats(catalog-op-executor.cc)</span><br><span class="line">-----FinalizePartitionedColumnStats(incr-stats-util.cc)</span><br><span class="line">------Update(incr-stats-util.cc)</span><br></pre></td></tr></table></figure>
<p>上述的代码调用都是属于BE模块的，这里主要分为两个分支流程：1）Execute函数，主要就是对两个子查询就行计算，并且保存相应地结果，这里我们不展开；2）Wait函数，这个后续的相关操作就是对统计信息的结构体进行更新。从上一节的代码中我们可以看到，在ExecComputeStats函数中，对FinalizePartitionedColumnStats进行了调用，其中涉及到了existing_part_stats这个成员变量。我们来看下相关的结构体：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Frontend.TExecRequest</span><br><span class="line">-catalog_op_request: Frontend.TCatalogOpRequest</span><br><span class="line">--ddl_params: CatalogService.TDdlExecRequest</span><br><span class="line">---compute_stats_params: JniCatalog.TComputeStatsParams</span><br><span class="line">----existing_part_stats: list&lt;CatalogObjects.TPartitionStats&gt;</span><br><span class="line">-----CatalogObjects.TPartitionStats</span><br><span class="line">------intermediate_col_stats: map&lt;string, TIntermediateColumnStats&gt;</span><br><span class="line">-------CatalogObjects.TIntermediateColumnStats</span><br></pre></td></tr></table></figure>
<p>从这里，我们就可以很明显的看出来，这个existing_part_stats变量，与我们上一节中提及到的partition_stats成员，其实是一样的内容。我们这里来看一下partition_stats是如何转换为existing_part_stats的：<br>首先，通过上一节的分析我们可以知道，如果某个分区进行了增量的统计信息计算，那么该分区包含的partitionStats_就不为空，并且hasIncrementalStats_为true（这两个成员变量都位于HdfsPartition类中）。<br>其次，Impala在进行SQL解析的时候，compute [incremental] stats [partiiton_spec]最终都会被解析为一个ComputeStatsStmt类，而这个类中就有一个变量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; The list of valid partition statistics that can be used in an incremental computation</span><br><span class="line">&#x2F;&#x2F; without themselves being recomputed. Populated in analyze().</span><br><span class="line">private final List&lt;TPartitionStats&gt; validPartStats_ &#x3D; new ArrayList&lt;&gt;();</span><br></pre></td></tr></table></figure>
<p>validPartStats_这个变量就是在解析的过程中，根据表的元数据信息（这里就是每个分区的partitionStats_），将partition_stats解析出来，并保存下来，相关的解析流程位于ComputeStatsStmt.analyze()。最后，再构造TComputeStatsParams，通过thrift传到BE端，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; ComputeStatsStmt.toThrift()</span><br><span class="line">public TComputeStatsParams toThrift() &#123;</span><br><span class="line">  TComputeStatsParams params &#x3D; new TComputeStatsParams();</span><br><span class="line">  params.setTable_name(new TTableName(table_.getDb().getName(), table_.getName()));</span><br><span class="line">  params.setTbl_stats_query(tableStatsQueryStr_);</span><br><span class="line">  if (columnStatsQueryStr_ !&#x3D; null) &#123;</span><br><span class="line">    params.setCol_stats_query(columnStatsQueryStr_);</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    params.setCol_stats_queryIsSet(false);</span><br><span class="line">  &#125;</span><br><span class="line">  params.setIs_incremental(isIncremental_);</span><br><span class="line">  params.setExisting_part_stats(validPartStats_);</span><br><span class="line">  params.setExpect_all_partitions(expectAllPartitions_);</span><br><span class="line">  &#x2F;&#x2F; 省略后续代码</span><br></pre></td></tr></table></figure>
<p>最终我们就在BE端获取到了existing_part_stats。也就是说，只有分区执行过增量的统计信息计算，existing_part_stats才不为空。最终在FinalizePartitionedColumnStats函数中，对existing_part_stats进行循环处理，调用了Update函数。我们来看下最后的Update函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; Updates all aggregate statistics with a new set of measurements.</span><br><span class="line">void Update(const string&amp; ndv, int64_t num_new_rows, double new_avg_width,</span><br><span class="line">    int32_t max_new_width, int64_t num_new_nulls) &#123;</span><br><span class="line">  DCHECK_EQ(intermediate_ndv.size(), ndv.size()) &lt;&lt; &quot;Incompatible intermediate NDVs&quot;;</span><br><span class="line">  DCHECK_GE(num_new_rows, 0);</span><br><span class="line">  DCHECK_GE(max_new_width, 0);</span><br><span class="line">  DCHECK_GE(new_avg_width, 0);</span><br><span class="line">  DCHECK_GE(num_new_nulls, 0);</span><br><span class="line">  for (int j &#x3D; 0; j &lt; ndv.size(); ++j) &#123;</span><br><span class="line">    intermediate_ndv[j] &#x3D; ::max(intermediate_ndv[j], ndv[j]);</span><br><span class="line">  &#125;</span><br><span class="line">  num_nulls +&#x3D; num_new_nulls;</span><br><span class="line">  max_width &#x3D; ::max(max_width, max_new_width);</span><br><span class="line">  avg_width +&#x3D; (new_avg_width * num_new_rows);</span><br><span class="line">  num_rows +&#x3D; num_new_rows;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们可以很明显的看到，这个函数里面都是对统计信息的更新，而其中就有num_nulls的处理。到这里，这个问题产生的原因基本就已经明了：我们通过2.12.0版本执行了compute incremental stats [partition_spec]，这些分区对应的列统计信息中，Nulls都是-1。当我们在3.4.0版本再次执行compute incremental stats [partition_spec]，会对之前的增量分区统计信息进行汇总（对于Nulls，是多个-1相加，最终结果小于-1），并写入到metastore中。当catalogd再次触发表的元数据加载时，由于Nulls的约束检查失败，导致了表的加载失败。<br>需要注意的是，当我们使用debug模式进行编译、调试的话，那么当执行到DCHECK_GE(num_new_nulls, 0)这一行代码的时候，Impalad会直接挂掉，只有使用release进行编译，才会发生最上面提到的异常。</p>
<h4 id="复现步骤"><a href="#复现步骤" class="headerlink" title="复现步骤"></a>复现步骤</h4><p>这里我们使用一个测试表进行测试，在2.12.0版本执行如下SQL：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE stats_test (id INT, name STRING)</span><br><span class="line">PARTITIONED BY (day STRING)</span><br><span class="line">STORED AS PARQUET;</span><br><span class="line">insert into stats_test partition(day&#x3D;&#39;2020-01-01&#39;) values(1,&#39;Jack&#39;);</span><br><span class="line">insert into stats_test partition(day&#x3D;&#39;2020-01-02&#39;) values(1,&#39;Jack&#39;);</span><br><span class="line">compute incremental stats stats_test;</span><br></pre></td></tr></table></figure>
<p>启动3.4.0版本之后，再执行如下的SQL：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">compute incremental stats stats_test partition(day&#x3D;&#39;2020-01-01&#39;);</span><br></pre></td></tr></table></figure>
<p>要触发这个错误，需要保证除当前待计算的分区之外，还有其他分区有增量的历史统计信息（如果我们在2.12.0中只对2020-12-02分区进行增量统计信息计算，3.4.0执行同样的SQL仍然会重现这个错误）。<br>目前的解决方法有两种：</p>
<ul>
<li>对于已经计算过统计信息的表，执行<strong>drop stats table_name</strong>，去掉已有的统计信息，然后再重新计算；</li>
<li>将社区<a target="_blank" rel="noopener" href="https://issues.apache.org/jira/browse/IMPALA-9699">IMPALA-9699</a>这个patch backport到较低的版本上来。<h4 id="后续补充"><a href="#后续补充" class="headerlink" title="后续补充"></a>后续补充</h4>后续我们发现，社区也已经有了类似的JIRA：<a target="_blank" rel="noopener" href="https://issues.apache.org/jira/browse/IMPALA-10230">IMPALA-10230</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://skyyws.github.io/2021/04/16/Impala-2-12-0%E4%B8%8E3-4-0%E7%89%88%E6%9C%AC%E7%9A%84compute-stats%E5%85%BC%E5%AE%B9%E9%97%AE%E9%A2%98/" data-id="cknka1qkk0000wsx5hwgl5a9k" data-title="Impala 2.12.0与3.4.0版本的compute stats兼容问题" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/olap/" rel="tag">olap</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/" rel="tag">问题排查</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Impala-cast-timestamp导致相同SQL查询不一致问题排查" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/16/Impala-cast-timestamp%E5%AF%BC%E8%87%B4%E7%9B%B8%E5%90%8CSQL%E6%9F%A5%E8%AF%A2%E4%B8%8D%E4%B8%80%E8%87%B4%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/" class="article-date">
  <time class="dt-published" datetime="2021-04-16T12:14:42.000Z" itemprop="datePublished">2021-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/16/Impala-cast-timestamp%E5%AF%BC%E8%87%B4%E7%9B%B8%E5%90%8CSQL%E6%9F%A5%E8%AF%A2%E4%B8%8D%E4%B8%80%E8%87%B4%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/">Impala cast timestamp导致相同SQL查询不一致问题排查</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>最近，线上业务在使用Impala进行查询的时候，遇到这种问题：同一个SQL执行，有时候提示AnalysisException，有时候执行正常，错误信息如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">org.apache.impala.common.AnalysisException: xxxxxxxxx org.apache.impala.analysis.SelectStmt$SelectAnalyzer.verifyAggregation(SelectStmt.java:832)</span><br><span class="line">	at org.apache.impala.analysis.SelectStmt$SelectAnalyzer.analyze(SelectStmt.java:233)</span><br><span class="line">	at org.apache.impala.analysis.SelectStmt$SelectAnalyzer.access$100(SelectStmt.java:199)</span><br><span class="line">	at org.apache.impala.analysis.SelectStmt.analyze(SelectStmt.java:192)</span><br><span class="line">	at org.apache.impala.analysis.AnalysisContext.analyze(AnalysisContext.java:518)</span><br><span class="line">	at org.apache.impala.analysis.AnalysisContext.analyzeAndAuthorize(AnalysisContext.java:426)</span><br></pre></td></tr></table></figure>
<p>我们在测试环境构造了测试表和SQL，如下所示，目前在2.12.0和3.4.0版本都碰到了同样的问题，但是4.0的开发环境目前没有出现过：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">create table test_table(dt STRING) partitioned by(day STRING) STORED AS PARQUET;</span><br><span class="line"></span><br><span class="line">SELECT</span><br><span class="line">	(CASE WHEN (DAYS_ADD(CAST(CAST(TO_DATE(TO_TIMESTAMP(&#96;t1&#96;.&#96;dt&#96;, &#39;yyyy-MM-dd&#39;)) AS TIMESTAMP) AS TIMESTAMP), 7) </span><br><span class="line">		&gt; CAST(&#39;2021-01-26&#39; AS TIMESTAMP))</span><br><span class="line">		THEN 0 ELSE 1 END) &#96;d1&#96;</span><br><span class="line">FROM</span><br><span class="line"> (SELECT dt FROM test_table</span><br><span class="line">  WHERE day&#x3D;to_date(days_sub(now(),1))</span><br><span class="line">  GROUP BY dt) &#96;t1&#96;</span><br><span class="line">GROUP BY (CASE WHEN (DAYS_ADD(CAST(CAST(TO_DATE(TO_TIMESTAMP(&#96;t1&#96;.&#96;dt&#96;, &#39;yyyy-MM-dd&#39;)) AS TIMESTAMP) AS TIMESTAMP), 7) </span><br><span class="line">	&gt; CAST(&#39;2021-01-26&#39; AS TIMESTAMP))</span><br><span class="line">	THEN 0 ELSE 1 END)</span><br><span class="line">LIMIT 20;</span><br></pre></td></tr></table></figure>
<p>如果我们设置enable_expr_rewrites为false，则SQL可以成功执行。Impala默认设置了enable_expr_rewrites为true，所以在解析完成之后，会对SQL进行重写，然后再次解析，接下来我们从错误出发，倒着来看问题产生的原因。</p>
<h4 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h4><p>首先，问题出现的地方是在SelectStmt.SelectAnalyzer.verifyAggregation函数中，当我们对SelectStmt进行了rewrite之后，再次analyze，会进行verify，相关代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">for (int i &#x3D; 0; i &lt; selectList_.getItems().size(); ++i) &#123;</span><br><span class="line">  if (!resultExprs_.get(i).isBound(multiAggInfo_.getResultTupleId())) &#123;</span><br><span class="line">    SelectListItem selectListItem &#x3D; selectList_.getItems().get(i);</span><br><span class="line">    throw new AnalysisException(</span><br><span class="line">        &quot;select list expression not produced by aggregation output &quot;</span><br><span class="line">        + &quot;(missing from GROUP BY clause?): &quot;</span><br><span class="line">        + selectListItem.toSql());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里主要就是通过对resultExprs_中的各个expr进行bound检查。经过多次测试和对比，我们发现，两种情况下的resultExprs_变量内容不同，导致了这种结果的差异。在SQL执行失败的情况下，resultExprs_的内容如下所示：<br><img src="https://img-blog.csdnimg.cn/20210208111534440.png#pic_center" alt="1"><br>而当SQL成功执行的情况下，resultExprs_的内容如下所示：<br><img src="https://img-blog.csdnimg.cn/20210208111604820.png#pic_center" alt="2">我们通过比较这两张截图可以看到，resultExprs_包含的expr在不同情况下，一个是CaseExpr，一个则是SlotRef，这两个成员对应的是SQL中的case when子句。正是这个CaseExpr造成了bound的检查失败。这个bound检查就是通过递归，不断对这个CaseExpr以及其children进行检查，我们将这个CaseExpr及其children的树状关系画出来了，如下所示：<br><img src="https://img-blog.csdnimg.cn/20210208111627874.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70#pic_center" alt="tree">最终在左下角的SlotRef中，bound检查失败。需要注意的是，这是我们经过ExprRewrite之后的再次执行SelectStmt.analyze()。重写之前的SelectStmt.analyze()是没有问题的，如下所示：<br><img src="https://img-blog.csdnimg.cn/20210208111652750.png#pic_center" alt="3">需要注意的是，由于这里还没有经过重写，因此截图里面显示的仍然是CAST(‘2021-01-26’ AS TIMESTAMP)，与图一和图二中的不一样。现在，我们的关注点就在于：为什么重写之后，这个resultExprs_包含的这个expr，有时候会是CaseExpr，有时候是SlotRef。而这正是SQL执行有时候成功，有时候失败的关键。<br>为了弄清楚这个问题，我们需要关注下resultExprs_这个变量是如何来的。我们查看这个变量的定义：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; QueryStmt.java</span><br><span class="line">  &#x2F;&#x2F; For a select statment:</span><br><span class="line">  &#x2F;&#x2F; original list of exprs in select clause (star-expanded, ordinals and</span><br><span class="line">  &#x2F;&#x2F; aliases substituted, agg output substituted)</span><br><span class="line">  &#x2F;&#x2F; For a union statement:</span><br><span class="line">  &#x2F;&#x2F; list of slotrefs into the tuple materialized by the union.</span><br><span class="line">  protected List&lt;Expr&gt; resultExprs_ &#x3D; new ArrayList&lt;&gt;();</span><br></pre></td></tr></table></figure>
<p>首先，会在SelectStmt.SelectAnalyzer.analyzeSelectClause()方法中，将SelectList的成员对应的expr依次加入到resultExprs_中，这里SQL解析的SelectList，第二个成员对应的expr是CaseExpr，这里是没有问题的。紧接着，会在SelectStmt.SelectAnalyzer.buildResultExprs()方法中进行substitute操作，相关调用栈如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">SelectStmt.analyze()</span><br><span class="line">-SelectStmt.SelectAnalyzer.analyze()</span><br><span class="line">--SelectStmt.SelectAnalyzer.buildResultExprs()</span><br><span class="line">---Expr.substituteList()</span><br><span class="line">----Expr.trySubstituteList()</span><br><span class="line">-----Expr.trySubstitute()</span><br><span class="line">------Expr.substituteImpl()</span><br><span class="line">-------ExprSubstitutionMap.get()</span><br><span class="line">--------Expr.equals()</span><br><span class="line">---------Expr.matches()</span><br><span class="line">----------TimestampLiteral.localEquals()</span><br></pre></td></tr></table></figure>
<p>由于这里涉及到的调用路径比较长，我这里简单的总结下：当进行substitute操作的时候，会从一个ExprSubstitutionMap中进行匹配，如果匹配上了，则使用ExprSubstitutionMap中的expr来替代原先的expr，相关代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; ExprSubstitutionMap.java</span><br><span class="line">  public Expr get(Expr lhsExpr) &#123;</span><br><span class="line">    for (int i &#x3D; 0; i &lt; lhs_.size(); ++i) &#123;</span><br><span class="line">      if (lhsExpr.equals(lhs_.get(i))) return rhs_.get(i);</span><br><span class="line">    &#125;</span><br><span class="line">    return null;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>关于ExprSubstitutionMap这里我们不用展开说明。可以看到，当lhs_中能匹配到时，则返回rhs_中对应的成员。这里我们就是用CaseExpr进行匹配。所以，当匹配到了，就会将resultExprs_中的CaseExpr替换为SlotRef（来自rhs_），此时SQL就能执行成功；如果匹配不到，则保持原先的CaseExpr不变，此时SQL执行报错。<br>所以问题就在于这个equals方法，代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; Expr.java</span><br><span class="line">  public final boolean equals(Object obj) &#123;</span><br><span class="line">    return obj instanceof Expr &amp;&amp; matches((Expr) obj, SlotRef.SLOTREF_EQ_CMP);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>在Expr.matches()方法中，就是对expr的各个children进行比较，我们发现，有时候TIMESTAMP ‘2021-01-26’这个TimestmapLiteral的比较失败（由CAST(‘2021-01-26’ AS TIMESTAMP)重写得到），导致SQL执行失败；有时候，能够比较成功，则SQL能执行成功。这里的TimestmapLiteral就对应树状图中的黄色节点部分。<br>我们发现ExprSubstitutionMap中的CaseExpr的TimestmapLiteral内容总是如下所示：<br><img src="https://img-blog.csdnimg.cn/20210208111724985.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70#pic_center" alt="4">而resultExpr_中的CaseExpr包含的TimestmapLiteral，有时候与上述一样，有时候则不一样，如下所示：<br><img src="https://img-blog.csdnimg.cn/20210208111739392.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70#pic_center" alt="5">我们可以看到，后四位是明显不一样的，正是因为这个不一样，导致ExprSubstitutionMap匹配为空，进而影响CaseExpr没有替换为SlotRef，最终影响了SQL的执行。所以现在的问题就是要搞清楚，为什么这个TimestmapLiteral包含的16位字节数组，多次执行的结果不一致。<br>目前的问题，主要就是对CAST(‘2021-01-26’ AS TIMESTAMP)的处理导致的，在进行重写的时候，这个表达式会通过FoldConstantsRule这个规则进行重写，这其中会调用到BE端的代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">FoldConstantsRule.apply()</span><br><span class="line">-LiteralExpr.createBounded()</span><br><span class="line">--FeSupport.EvalExprWithoutRowBounded()</span><br><span class="line">---FeSupport.EvalExprsWithoutRowBounded()</span><br><span class="line">----FeSupport.NativeEvalExprsWithoutRow()</span><br><span class="line">------Java_org_apache_impala_service_FeSupport_NativeEvalExprsWithoutRow fe-support.cc</span><br></pre></td></tr></table></figure>
<p>最终在BE端，通过这个函数进行了计算和转换，得到对应的TColumnValue，然后在FE端转换成相应的TimestmapLiteral，在BE端的主要转换流程如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TExpr-&gt;ScalarExpr-&gt;ScalarExprEvaluator-&gt;TimestampVal-&gt;TimestampValue-&gt;TColumnValue-&gt;TResultRow</span><br></pre></td></tr></table></figure>
<p>最终将构造好的TResultRow序列化传到FE端。在Java_org_apache_impala_service_FeSupport_NativeEvalExprsWithoutRow方法中，我们通过GDB打印TColumnValue包含的binary_val（最终会使用这个来构造TimestmapLiteral），发现SQL执行失败的情况下，最后几个字节确实会有问题：<br><img src="https://img-blog.csdnimg.cn/20210208111814366.png#pic_center" alt="6"><br>而SQL执行成功的时候，最后几个节点是这样的：<br><img src="https://img-blog.csdnimg.cn/20210208111831850.png#pic_center" alt="7"><br>这与我们在java的ide进行远程调试的时候，看到的TimestmapLiteral包含的字节数组的最后几位也是一致的，这就说明我们在BE端构造TColumnValue的时候就已经有问题了。<br>我们继续往上追溯发现，TColumnValue的binary_val构造代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; fe-support.cc SetTColumnValue()</span><br><span class="line">    case TYPE_TIMESTAMP: &#123;</span><br><span class="line">      const uint8_t* uint8_val &#x3D; reinterpret_cast&lt;const uint8_t*&gt;(value);</span><br><span class="line">      col_val-&gt;binary_val.assign(uint8_val, uint8_val + type.GetSlotSize());</span><br><span class="line">      col_val-&gt;__isset.binary_val &#x3D; true;</span><br><span class="line">      RawValue::PrintValue(value, type, -1, &amp;col_val-&gt;string_val);</span><br><span class="line">      col_val-&gt;__isset.string_val &#x3D; true;</span><br><span class="line">      break;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>这里的value是一个void*，在当前情况下，对应的是TimestampValue类型的指针；对于TImestmap类型，type.GetSlotSize()会返回16。TimestampValue的构造代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; ScalarExprEvaluator.cc GetValue()</span><br><span class="line">    case TYPE_TIMESTAMP: &#123;</span><br><span class="line">      impala_udf::TimestampVal v &#x3D; expr.GetTimestampVal(this, row);</span><br><span class="line">      if (v.is_null) return nullptr;</span><br><span class="line">      result_.timestamp_val &#x3D; TimestampValue::FromTimestampVal(v);</span><br><span class="line">      return &amp;result_.timestamp_val;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>我们通过GDB来打印result_.timestamp_val的最后4个字节内容，如下所示：<br><img src="https://img-blog.csdnimg.cn/2021020811185329.png#pic_center" alt="8"><br>从这里可以看到，在构造完TimestampValue之后，最后几个节点对应的值就已经有问题了。我们继续查看TimestampVal发现最后几个字节都是0，也就是说TimestampVal构造没有问题，但是在构造result_.timestamp_val的时候，出现了问题：<br><img src="https://img-blog.csdnimg.cn/20210208112126488.png#pic_center" alt="10"><br>result_.timestamp_val是一个TimestampValue的便利，包含2个成员变量：4个字节的date_和8个字节的time_，由于对齐机制，一共16个字节。通过调试我们发现：对于TimestampValue变量，0～7字节存储的是time_，对于“2021-01-26 00:00:00”而言，一直为0，所以0～7字节的值一直是0；8～11字节存储的是date_，对应截图中的：105、-122、37、0；最后的8<del>15是填充字节，而正是这四个字节的不同，导致了整个TimestampValue的不同。<br>经过调试发现，对于代码：result_.timestamp_val = TimestampValue::FromTimestampVal(v)，timestamp_val变量在被赋值之前，就已经有内容了，由于最后8</del>15这四个填充字节的不同，导致了返回到FE端的字节数组不同。<br>这个result_属于ScalarExprEvaluator，是一个ExprValue类型的变量，初始化流程如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Java_org_apache_impala_service_FeSupport_NativeEvalExprsWithoutRow fe-support.cc</span><br><span class="line">-Create() scalar-expr-evaluator.cc</span><br><span class="line">-ctor() scalar-expr-evaluator.cc</span><br><span class="line">-ctor() expr-value.h</span><br></pre></td></tr></table></figure>
<p>对于timestamp_val的初始化直接使用了timestamp_val ()，目前我怀疑是因为初始化该SQL的时候，timestamp_val分配的内存没有置0。为验证这个猜想，我们在ExprValue的构造函数中显示对timestamp_val的内存进行清空，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">ExprValue()</span><br><span class="line">  : bool_val(false),</span><br><span class="line">    tinyint_val(0),</span><br><span class="line">    smallint_val(0),</span><br><span class="line">    int_val(0),</span><br><span class="line">    bigint_val(0),</span><br><span class="line">    float_val(0.0),</span><br><span class="line">    double_val(0.0),</span><br><span class="line">    string_val(NULL, 0),</span><br><span class="line">    timestamp_val(),</span><br><span class="line">    decimal4_val(),</span><br><span class="line">    decimal8_val(),</span><br><span class="line">    decimal16_val(),</span><br><span class="line">    collection_val(),</span><br><span class="line">    date_val(0) &#123;</span><br><span class="line">  memset(&amp;timestamp_val, 0, sizeof(timestamp_val));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>重新编译之后，再测试，多次执行SQL没有再出现同样的问题了。</p>
<h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><p>目前，针对这种情况，由于社区的4.x开发版本，我们无法复现该问题，并且我们也没有看到相关的patch，因此怀疑是4.0依赖的编译器之类的，会保证在new的时候，直接对分配的内存空间置0，所以不会出现该问题。我们已经将问题反馈到社区，等待社区的相关回复：<a target="_blank" rel="noopener" href="https://issues.apache.org/jira/browse/IMPALA-10461">IMPALA-10461</a><br>针对3.4.0版本的问题，我们目前的解决方案有两种：</p>
<ol>
<li>上面其实已经提到了，就是在ExprValue的构造函数中，显示地对Timestamp置0：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">memset(&amp;timestamp_val, 0, sizeof(timestamp_val));</span><br></pre></td></tr></table></figure></li>
<li>由于是最后的4个padding字节导致的问题，因此我们可以对FE端的TimestampLiteral.localEquals方法进行调整，只比较前12个字节：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> public boolean localEquals(Expr that) &#123;</span><br><span class="line">  return super.localEquals(that) &amp;&amp;</span><br><span class="line">      &#x2F;&#x2F; Arrays.equals(value_, ((TimestampLiteral) that).value_);</span><br><span class="line">      Arrays.equals(Arrays.copyOfRange(value_, 0, 12),</span><br><span class="line">          Arrays.copyOfRange(((TimestampLiteral) that).value_, 0, 12));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://skyyws.github.io/2021/04/16/Impala-cast-timestamp%E5%AF%BC%E8%87%B4%E7%9B%B8%E5%90%8CSQL%E6%9F%A5%E8%AF%A2%E4%B8%8D%E4%B8%80%E8%87%B4%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/" data-id="cknk9zwi20000tcx579kt0v0c" data-title="Impala cast timestamp导致相同SQL查询不一致问题排查" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/olap/" rel="tag">olap</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/" rel="tag">问题排查</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Impala-3-4-SQL查询之ScanRange详解（四）" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8BScanRange%E8%AF%A6%E8%A7%A3%EF%BC%88%E5%9B%9B%EF%BC%89/" class="article-date">
  <time class="dt-published" datetime="2021-04-16T12:11:20.000Z" itemprop="datePublished">2021-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8BScanRange%E8%AF%A6%E8%A7%A3%EF%BC%88%E5%9B%9B%EF%BC%89/">Impala 3.4 SQL查询之ScanRange详解（四）</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>在上篇文章中，我们主要介绍了ScanRange的构造，以及在FE和BE端的一些处理流程。同时，我们还介绍了IO thead处理模型中一个比较重要的对象RequestContext::PerDiskState，以及部分成员变量的含义，在本篇文章中，我们将介绍其中一个比较重要的成员：unstarted_scan_ranges_。</p>
<h4 id="关于BE端的ScanRange"><a href="#关于BE端的ScanRange" class="headerlink" title="关于BE端的ScanRange"></a>关于BE端的ScanRange</h4><p>在上篇文章中，我们提到，在FE端的ScanRange信息，主要通过TScanRange传到BE端，然后构造为TPlanFragmentInstanceCtx中的TScanRangeParams，传到各个executor进行实际的扫描操作，那么当各个executor接收到请求之后，就会根据这些信息，构造相应的ScanRange类。ScanRange是继承RequestRange这个类的，另外WriteRange也是继承了RequestRange对象的。从名字就可以看出，WriteRange主要是针对写入的情况，这里我们不展开介绍，主要看下ScanRange对象。首先，RequestRange主要包含了file、offset、len这些基本信息。而ScanRange则增加了一些额外的信息，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class ScanRange : public RequestRange &#123;</span><br><span class="line">    struct SubRange &#123;</span><br><span class="line">    int64_t offset;</span><br><span class="line">    int64_t length;</span><br><span class="line">  &#125;;</span><br><span class="line">  </span><br><span class="line">  DiskIoMgr* io_mgr_ &#x3D; nullptr;</span><br><span class="line">  RequestContext* reader_ &#x3D; nullptr;</span><br><span class="line">  bool read_in_flight_ &#x3D; false;</span><br><span class="line">  int64_t bytes_read_ &#x3D; 0;</span><br><span class="line">  std::vector&lt;SubRange&gt; sub_ranges_;</span><br><span class="line">  ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>关于这些成员变量的含义，我们这里先不一一介绍了，后面在相应的场景下，我们再一一展开说明。<br>当我们将TPlanFragmentInstanceCtx的信息传到对应的executor的时候，对应的executor节点就会构造相应的HdfsScanNode，然后在HdfsScanNodeBase::Prepare函数中，会循环遍历每个TScanRangeParams，然后初始化下面的这个map成员：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; hdfs-scan-node-base.h</span><br><span class="line">&#x2F;&#x2F;&#x2F; This is a pair for partition ID and filename</span><br><span class="line">typedef pair&lt;int64_t, std::string&gt; PartitionFileKey;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;&#x2F; partition_id, File path &#x3D;&gt; file descriptor (which includes the file&#39;s splits)</span><br><span class="line">typedef std::unordered_map&lt;PartitionFileKey, HdfsFileDesc*, pair_hash&gt; FileDescMap;</span><br><span class="line">FileDescMap file_descs_;</span><br><span class="line"></span><br><span class="line">struct HdfsFileDesc &#123;</span><br><span class="line">  hdfsFS fs;</span><br><span class="line">  std::string filename;</span><br><span class="line">  int64_t file_length;</span><br><span class="line">  int64_t mtime</span><br><span class="line">  THdfsCompression::type file_compression;</span><br><span class="line">  bool is_erasure_coded;</span><br><span class="line">  std::vector&lt;io::ScanRange*&gt; splits;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>file_descs_是一个map，用分区id和文件名来作为map的key，value是一个HdfsFileDesc对象。当循环遍历TScanRangeParams对象的时候，Impala会用其中包含的THdfsFileSplit对象的信息，来构造一个HdfsFileDesc对象，填充其中的fs、filename等信息，关键代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">for (const TScanRangeParams&amp; params: *scan_range_params_) &#123;</span><br><span class="line">  const THdfsFileSplit&amp; split &#x3D; params.scan_range.hdfs_file_split;</span><br><span class="line">  partition_ids_.insert(split.partition_id);</span><br><span class="line">  HdfsPartitionDescriptor* partition_desc &#x3D;</span><br><span class="line">      hdfs_table_-&gt;GetPartition(split.partition_id);</span><br><span class="line"></span><br><span class="line">  filesystem::path file_path(partition_desc-&gt;location());</span><br><span class="line">  file_path.append(split.relative_path, filesystem::path::codecvt());</span><br><span class="line">  const string&amp; native_file_path &#x3D; file_path.native();</span><br><span class="line"></span><br><span class="line">  auto file_desc_map_key &#x3D; make_pair(partition_desc-&gt;id(), native_file_path);</span><br><span class="line">  HdfsFileDesc* file_desc &#x3D; NULL;</span><br><span class="line">  FileDescMap::iterator file_desc_it &#x3D; file_descs_.find(file_desc_map_key);</span><br><span class="line">  if (file_desc_it &#x3D;&#x3D; file_descs_.end()) &#123;</span><br><span class="line">    &#x2F;&#x2F; Add new file_desc to file_descs_ and per_type_files_</span><br><span class="line">    file_descs_[file_desc_map_key] &#x3D; file_desc;</span><br><span class="line">    &#x2F;&#x2F; 省略其余代码</span><br><span class="line">    file_desc &#x3D; runtime_state_-&gt;obj_pool()-&gt;Add(new HdfsFileDesc(native_file_path));</span><br><span class="line">    per_type_files_[partition_desc-&gt;file_format()].push_back(file_desc);</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    &#x2F;&#x2F; File already processed</span><br><span class="line">    file_desc &#x3D; file_desc_it-&gt;second;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  file_desc-&gt;splits.push_back(</span><br><span class="line">      AllocateScanRange(file_desc-&gt;fs, file_desc-&gt;filename.c_str(), split.length,</span><br><span class="line">          split.offset, split.partition_id, params.volume_id, expected_local,</span><br><span class="line">          file_desc-&gt;is_erasure_coded, file_desc-&gt;mtime, BufferOpts(cache_options)));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们删除了部分代码，只保留了关键的部分。可以看到，当file_descs_中，不存在指定key时，我们构造新的key和value，加入到map中。这里关注下对于splits这个vector的处理。对于分区的某个指定文件，在map中会有一条记录，如果这个文件对应多个TScanRangeParams，那么这个map的value对应的splits则会有多个成员，但是这条key-value记录只有一条。我们前面说过了，一个ScanRange在HDFS_SCAN_NODE代表一个block，所以如果文件跨越了多个block，那么就会分成多个ScanRange，此时map的value，HdfsFileDesc对象的splits就会存在多个成员；反之，如果文件只存在于1个block中，那么HdfsFileDesc的splits对象，则只会有1个成员。<br>除了file_descs_之外，还有一个成员也需要关注下：per_type_files_，这个成员变量的定义如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; hdfs-scan-node-base.h</span><br><span class="line">  &#x2F;&#x2F;&#x2F; File format &#x3D;&gt; file descriptors.</span><br><span class="line">  typedef std::map&lt;THdfsFileFormat::type, std::vector&lt;HdfsFileDesc*&gt;&gt;</span><br><span class="line">    FileFormatsMap;</span><br><span class="line">  FileFormatsMap per_type_files_;</span><br></pre></td></tr></table></figure>
<p>可以看到，这个per_type_files_保存的就是文件格式和HdfsFileDesc的集合，在上述处理file_descs_的代码中，我们也可以看到对per_type_files_的处理，根据当前这个文件所属分区的格式，加入到map value的vector中。</p>
<h4 id="关于unstarted-scan-ranges"><a href="#关于unstarted-scan-ranges" class="headerlink" title="关于unstarted_scan_ranges"></a>关于unstarted_scan_ranges</h4><p>上面我们介绍完了BE端的ScanRange对象，接下来我们来看一下PerDiskState中的unstarted_scan_ranges_成员，以及它是如何更新的。首先，我们还是先看下这个成员变量的定义：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;&#x2F; Queue of ranges that have not started being read.  This list is exclusive</span><br><span class="line">&#x2F;&#x2F;&#x2F; with in_flight_ranges.</span><br><span class="line">InternalQueue&lt;ScanRange&gt; unstarted_scan_ranges_;</span><br></pre></td></tr></table></figure>
<p>从注释我们可以看到，unstarted_scan_ranges_表示是还没有开始进行scan操作的ScanRange，这个解释比较空泛，我们接着看下unstarted_scan_ranges这个成员更新的相关函数调用（当前是针对parquet格式的表进行梳理）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">ExecFInstance(query-state.cc):697</span><br><span class="line">-Exec(fragment-instance-state.cc):98</span><br><span class="line">--ExecInternal(fragment-instance-state.cc):383</span><br><span class="line">---GetNext(hdfs-scan-node.cc):91</span><br><span class="line">----IssueInitialScanRanges(hdfs-scan-node-base.cc):636</span><br><span class="line">-----IssueInitialRanges(hdfs-parquet-scanner.cc):82</span><br><span class="line">------IssueFooterRanges(hdfs-scanner.cc):837</span><br><span class="line">-------AddDiskIoRanges(hdfs-scan-node.cc):212</span><br><span class="line">--------AddScanRanges(request-context.cc):404</span><br><span class="line">---------AddRangeToDisk(request-context.cc):357</span><br><span class="line">----------unstarted_scan_ranges()-&gt;Enqueue</span><br><span class="line">---------AddRangeToDisk(request-context.cc):362</span><br><span class="line">----------num_unstarted_scan_ranges_.Add(1)</span><br><span class="line">---------AddRangeToDisk(request-context.cc):366</span><br><span class="line">----------next_scan_range_to_start()&#x3D;null ScheduleContext(request-context.cc)</span><br><span class="line">---------AddRangeToDisk(request-context.cc):379</span><br><span class="line">----------num_remaining_ranges_++</span><br></pre></td></tr></table></figure>
<p>在HdfsScanNodeBase::IssueInitialScanRanges函数中，我们通过per_type_files_成员，获取所有的PARQUET格式的HdfsFileDesc集合，然后在HdfsScanner::IssueFooterRanges函数中，循环构造初始的ScanRange（不同的文件格式，这里的处理流程有所不同），由于当前是PARQUET文件，所以会构造每个文件footer的ScanRange，这里我们摘取一些主要的步骤看下（忽略其他的一些特殊情况）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">    &#x2F;&#x2F;这里FOOTER_SIZE是一个常量，为1024*100</span><br><span class="line">    int64_t footer_size &#x3D; min(FOOTER_SIZE, files[i]-&gt;file_length);</span><br><span class="line">    int64_t footer_start &#x3D; files[i]-&gt;file_length - footer_size;</span><br><span class="line"></span><br><span class="line">    ScanRange* footer_split &#x3D; FindFooterSplit(files[i]);</span><br><span class="line"></span><br><span class="line">    for (int j &#x3D; 0; j &lt; files[i]-&gt;splits.size(); ++j) &#123;</span><br><span class="line">      ScanRange* split &#x3D; files[i]-&gt;splits[j];</span><br><span class="line"></span><br><span class="line">      if (!scan_node-&gt;IsZeroSlotTableScan() || footer_split &#x3D;&#x3D; split) &#123;</span><br><span class="line">        ScanRangeMetadata* split_metadata &#x3D;</span><br><span class="line">            static_cast&lt;ScanRangeMetadata*&gt;(split-&gt;meta_data());</span><br><span class="line">        ScanRange* footer_range;</span><br><span class="line">        if (footer_split !&#x3D; nullptr) &#123;</span><br><span class="line">          footer_range &#x3D; scan_node-&gt;AllocateScanRange(files[i]-&gt;fs,</span><br><span class="line">              files[i]-&gt;filename.c_str(), footer_size, footer_start,</span><br><span class="line">              split_metadata-&gt;partition_id, footer_split-&gt;disk_id(),</span><br><span class="line">              footer_split-&gt;expected_local(), files[i]-&gt;is_erasure_coded, files[i]-&gt;mtime,</span><br><span class="line">              BufferOpts(footer_split-&gt;cache_options()), split);</span><br><span class="line">        &#125;</span><br><span class="line">        footer_ranges.push_back(footer_range);</span><br><span class="line">    &#125;</span><br><span class="line">  &#x2F;&#x2F; The threads that process the footer will also do the scan.</span><br><span class="line">  if (footer_ranges.size() &gt; 0) &#123;</span><br><span class="line">    RETURN_IF_ERROR(scan_node-&gt;AddDiskIoRanges(footer_ranges, EnqueueLocation::TAIL));</span><br><span class="line">  &#125;</span><br><span class="line">  return Status::OK();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们删除了其他的一些代码和注释，关注下主要的处理步骤，首先获取footer_size和footer_start，然后利用FindFooterSplit函数获取该file的footer split，判断逻辑就是从splits成员中找到：split.len+split.offset=file.len，可以理解为文件的最后一个split成员对象。然后遍历splits集合，当找到与footer_split对应的split时，我们就用这个footer_split和file的相关信息来构造一个ScanRange，作为footer ScanRange。这里需要注意的是一个file对应多个split（即多个block）的情况，此时在遍历某个file对应的split集合的时候，当满足如下的条件时候，我们就会用对应的split来构造foot ScanRange，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; HdfsScanner::IssueFooterRanges()</span><br><span class="line">      &#x2F;&#x2F; If there are no materialized slots (such as count(*) over the table), we can</span><br><span class="line">      &#x2F;&#x2F; get the result with the file metadata alone and don&#39;t need to read any row</span><br><span class="line">      &#x2F;&#x2F; groups. We only want a single node to process the file footer in this case,</span><br><span class="line">      &#x2F;&#x2F; which is the node with the footer split.  If it&#39;s not a count(*), we create a</span><br><span class="line">      &#x2F;&#x2F; footer range for the split always.</span><br><span class="line">      if (!scan_node-&gt;IsZeroSlotTableScan() || footer_split &#x3D;&#x3D; split) &#123;</span><br></pre></td></tr></table></figure>
<p>也就是说，当满足条件时，我们对于一个file的多个split，我们会分别构造一个footer ScanRange，而不是1个。但是这些footer ScanRange的len、offset、file信息都是一样的，唯一不同的就是meta_data_，该成员类型是void*，但是实际会被赋值为ScanRangeMetadata。meta_data_中的original_split会保存原始的split对应的ScanRange信息，也就是原始的len、offset。<br>当处理完成所有的文件之后，我们最终通过RequestContext::AddRangeToDisk函数，将这些footer的ScanRange加入到unstarted_scan_ranges_对象中，同时，每入队一个ScanRange对象，我们会将num_unstarted_scan_ranges_这个成员加1。也就是说，这个unstarted_scan_ranges_最终存放的是所有file文件的footer ScanRange。<br>上面我们介绍了unstarted_scan_ranges_这个队列的入队流程，接着我们看下出队的操作。在前面的文章中，我们提到了，IO thread会从RequestContext队列的头部取出一个RequestContext对象，然后通过该RequestContext对象获取一个ScanRange进行处理，相关处理函数如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">RequestRange* RequestContext::GetNextRequestRange(int disk_id) &#123;</span><br><span class="line">  PerDiskState* request_disk_state &#x3D; &amp;disk_states_[disk_id];</span><br><span class="line">  unique_lock&lt;mutex&gt; request_lock(lock_);</span><br><span class="line"></span><br><span class="line">  if (request_disk_state-&gt;next_scan_range_to_start() &#x3D;&#x3D; nullptr &amp;&amp;</span><br><span class="line">      !request_disk_state-&gt;unstarted_scan_ranges()-&gt;empty()) &#123;</span><br><span class="line">    ScanRange* new_range &#x3D; request_disk_state-&gt;unstarted_scan_ranges()-&gt;Dequeue();</span><br><span class="line">    num_unstarted_scan_ranges_.Add(-1);</span><br><span class="line">    ready_to_start_ranges_.Enqueue(new_range);</span><br><span class="line">    request_disk_state-&gt;set_next_scan_range_to_start(new_range);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  if (request_disk_state-&gt;in_flight_ranges()-&gt;empty()) &#123;</span><br><span class="line">    request_disk_state-&gt;DecrementDiskThread(request_lock, this);</span><br><span class="line">    return nullptr;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  RequestRange* range &#x3D; request_disk_state-&gt;in_flight_ranges()-&gt;Dequeue();</span><br><span class="line"></span><br><span class="line">  request_disk_state-&gt;ScheduleContext(request_lock, this, disk_id);</span><br><span class="line">  return range;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>同样我们删除了一些代码，方便阅读。首选获取对应的PerDiskState对象，然后将unstarted_scan_ranges_队列的头部对象出队，并将num_unstarted_scan_ranges_加1，同时入队到ready_to_start_ranges_中，这两个变量都是RequestContext的成员，这里我们先不展开说明。接着将出队的ScanRange对象设置到next_scan_range_to_start_成员，关于这个成员的用处，我们也在后面展开说明。<br>紧接着，会判断in_flight_ranges_队列是否为空，是则直接返回null，表示这次IO thead没取到ScanRange；否则，从in_flight_ranges_弹出头部的ScanRange对象，返回进行处理。</p>
<h4 id="unstarted-scan-ranges的后续处理"><a href="#unstarted-scan-ranges的后续处理" class="headerlink" title="unstarted_scan_ranges的后续处理"></a>unstarted_scan_ranges的后续处理</h4><p>前面我们提到了IO thread并不会直接获取unstarted_scan_ranges_队列上的ScanRange进行处理。先将unstarted_scan_ranges_的头部出队，然后入队到ready_to_start_ranges_队列中，同时设置到next_scan_range_to_start_成员。然后再从in_flight_ranges_队列中取出头部对象，进行后续的处理。由于这里涉及到的成员变量很多，我们将RequestContext和PerDiskState的成员进行了归纳，如下所示：<br><img src="https://img-blog.csdnimg.cn/20210416195856857.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70#pic_center" alt="1"><br>这里我们简单说明一下，RequestContex对象会包含多个PerDiskState对象，每一个PerDiskState对象表示一种disk queue，例如remote HDFS、S3等，所以RequestContex对象的这些成员，统计的是所有PerDiskState的相应成员的累加和，比如num_unstarted_scan_ranges_这个成员，统计的就是该RequestContex对象上的所有PerDiskState的unstarted_scan_ranges_的总和。这点需要注意。<br>下面我们来看下ready_to_start_ranges_和next_scan_range_to_start_的相关处理，函数调用如下所示：<br><img src="https://img-blog.csdnimg.cn/20210419202513821.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70#pic_center" alt="6"><br>由于这里涉及到了不同的调用路径，因此我们使用了上述图片的方式。可以看到，主要分为两条路径：左边路径的主要处理逻辑就是在HdfsScanNode的Open函数中，将回调函数ThreadTokenAvailableCb绑定到线程池；右边路径则会通过回调函数ThreadTokenAvailableCb启动专门的scanner线程来处理unstarted_scan_ranges。 最终在GetNextUnstartedRange函数中，会对next_scan_range_to_start_和ready_to_start_ranges_进行处理，关键代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; RequestContext::GetNextUnstartedRange()</span><br><span class="line">*range &#x3D; ready_to_start_ranges_.Dequeue();</span><br><span class="line">int disk_id &#x3D; (*range)-&gt;disk_id();</span><br><span class="line">disk_states_[disk_id].set_next_scan_range_to_start(nullptr);</span><br></pre></td></tr></table></figure>
<p>可以看到在GetNextUnstartedRange函数中，先将ready_to_start_ranges_队列中的头部对象弹出，然后将该ScanRange对应的PerDiskState的next_scan_range_to_start_对象设置为空，然后再继续后续的处理，这里省略了后续处理代码。关于回调函数和scanner线程，后面我们讲到in_flight_ranges_的时候，会再详细说明，这里简单了解下这个处理过程即可。</p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>到这里，关于unstarted_scan_ranges_的相关处理流程我们就介绍的差不多了。回顾一下，我们在本文中，首先介绍了BE端的ScanRange，相较于thrift的TScanRange结构体，ScanRange对象主要是在每个executor上进行实际scan操作时，需要用到的类。除此之外，我们还介绍了一个关键的对象：unstarted_scan_ranges_，这是一个ScanRange的队列，我们通过代码，一步一步了解了这个队列的更新情况，包括入队和出队，这个对象对于整个IO thread模型是比较重要的。现在读者看下来这两篇文章可能觉得比较琐碎，后面笔者会将各个成员串起来，整体看下Impala的这个IO thread的处理。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://skyyws.github.io/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8BScanRange%E8%AF%A6%E8%A7%A3%EF%BC%88%E5%9B%9B%EF%BC%89/" data-id="cknk9v6t00000n3x59fd23vef" data-title="Impala 3.4 SQL查询之ScanRange详解（四）" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Impala-SQL%E6%9F%A5%E8%AF%A2%E7%B3%BB%E5%88%97/" rel="tag">Impala SQL查询系列</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/olap/" rel="tag">olap</a></li></ul>

    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Impala-SQL%E6%9F%A5%E8%AF%A2%E7%B3%BB%E5%88%97/" rel="tag">Impala SQL查询系列</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Impala%E4%B9%8BHDFS-SCAN-NODE/" rel="tag">Impala之HDFS_SCAN_NODE</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kylin/" rel="tag">kylin</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/olap/" rel="tag">olap</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/" rel="tag">经验总结</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/" rel="tag">问题排查</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Impala-SQL%E6%9F%A5%E8%AF%A2%E7%B3%BB%E5%88%97/" style="font-size: 15px;">Impala SQL查询系列</a> <a href="/tags/Impala%E4%B9%8BHDFS-SCAN-NODE/" style="font-size: 10px;">Impala之HDFS_SCAN_NODE</a> <a href="/tags/impala/" style="font-size: 17.5px;">impala</a> <a href="/tags/kylin/" style="font-size: 10px;">kylin</a> <a href="/tags/olap/" style="font-size: 20px;">olap</a> <a href="/tags/%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/" style="font-size: 10px;">经验总结</a> <a href="/tags/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/" style="font-size: 12.5px;">问题排查</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/04/29/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8BScanRange%E6%B5%81%E7%A8%8B%E5%BD%92%E7%BA%B3%EF%BC%88%E5%85%AD%EF%BC%89/">Impala 3.4 SQL查询之ScanRange流程归纳（六）</a>
          </li>
        
          <li>
            <a href="/2021/04/28/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8BScanRange%E8%AF%A6%E8%A7%A3%EF%BC%88%E4%BA%94%EF%BC%89/">Impala 3.4 SQL查询之ScanRange详解（五）</a>
          </li>
        
          <li>
            <a href="/2021/04/16/%E5%85%B3%E4%BA%8EImpala%E7%9A%84use-local-tz-for-unix-timestamp-conversions%E5%8F%82%E6%95%B0%E6%8E%A2%E7%A9%B6/">关于Impala的use_local_tz_for_unix_timestamp_conversions参数探究</a>
          </li>
        
          <li>
            <a href="/2021/04/16/Impala%E9%85%8D%E7%BD%AERanger%E6%9C%8D%E5%8A%A1%E8%BF%9B%E8%A1%8C%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6/">Impala配置Ranger服务进行权限控制</a>
          </li>
        
          <li>
            <a href="/2021/04/16/%E8%AE%B0%E4%B8%80%E6%AC%A1Apache-Kylin%E7%9A%84%E6%85%A2%E6%9F%A5%E8%AF%A2%E6%8E%92%E6%9F%A5%E5%8F%8A%E4%BC%98%E5%8C%96/">记一次Apache Kylin的慢查询排查及优化</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 汪胜<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>