<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>skyyws的藏宝阁</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="skyyws的藏宝阁">
<meta property="og:url" content="https://skyyws.github.io/index.html">
<meta property="og:site_name" content="skyyws的藏宝阁">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="汪胜">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="skyyws的藏宝阁" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">skyyws的藏宝阁</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://skyyws.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-Impala-3-4-SQL查询之ScanRange详解（三）" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8BScanRange%E8%AF%A6%E8%A7%A3%EF%BC%88%E4%B8%89%EF%BC%89/" class="article-date">
  <time class="dt-published" datetime="2021-04-16T11:17:28.000Z" itemprop="datePublished">2021-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8BScanRange%E8%AF%A6%E8%A7%A3%EF%BC%88%E4%B8%89%EF%BC%89/">Impala 3.4 SQL查询之ScanRange详解（三）</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>我们在本系列的前两篇文章中，简单介绍了SQL查询的整个流程以及重写的相关知识。在接下来的这几篇中，会跟大家一起详细学习ScanRange的知识。由于涉及到的内容非常多，因此会分成几篇来讲解，主要会涉及到HDFS_SCAN_NODE、IO thread等知识。由于现在相关的文档比较少，这些文章都是笔者根据代码和实际调试结果整理出来的，如有错误，欢迎指正。默认情况下，本文涉及到的测试表都是HDFS上的parquet表，并且是以天为分区。</p>
<h4 id="关于ScanRange"><a href="#关于ScanRange" class="headerlink" title="关于ScanRange"></a>关于ScanRange</h4><p>ScanRange是Impala中一个非常基础的概念，对于HDFS_SCAN_NODE来说，一个ScanRange表示的就是一个HDFS文件上的一部分，一般用file_name、offset和len来表示，更多关于ScanRange的详细介绍，可以参考文章：<a target="_blank" rel="noopener" href="https://blog.csdn.net/huang_quanlong/article/details/53980132">Impala源码阅读——SimpleScheduler</a>。本文我们主要讲一下ScanRange的构造，以及在HDFS_SCAN_NODE过程中的一些处理，同时会涉及到IO thread模型相关的一些知识，感兴趣的同学，可以看看我的前两篇文章：<a href="https://skyyws.github.io/2021/04/16/Impala-HDFS-SCAN-NODE%E4%B9%8BIO-threads%E6%A8%A1%E5%9E%8B">Impala HDFS_SCAN_NODE之IO threads模型</a>和<a href="https://skyyws.github.io/2021/04/16/Impala-HDFS-SCAN-NODE%E4%B9%8BAverageHdfsReadThreadConcurrency">Impala HDFS_SCAN_NODE之AverageHdfsReadThreadConcurrency</a>。<br>当SQL提交到Impalad节点之后，会通过JNI调用，由FE模块进行执行计划的解析，最终会针对每个表，构建一个HDFS_SCAN_NODE，其中就会包含ScanRange的信息，相关的函数调用栈如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">ExecuteInternal(impala-server.cc):956</span><br><span class="line">-InitExecRequest(client-request-state.cc):1440</span><br><span class="line">--GetExecRequest(frontend.cc):230</span><br><span class="line">---createExecRequest(JniFrontend.java):154</span><br><span class="line">----createExecRequest(Frontend.java):1464</span><br><span class="line">-----getTExecRequest(Frontend.java):1494</span><br><span class="line">------doCreateExecRequest(Frontend.java):1600</span><br><span class="line">-------getPlannedExecRequest(Frontend.java):1734</span><br><span class="line">--------createExecRequest(Frontend.java):1413</span><br><span class="line">---------createPlans(Planner.java):264</span><br><span class="line">----------createPlanFragments(Planner.java):118</span><br><span class="line">-----------createSingleNodePlan(SingleNodePlanner.java):150</span><br><span class="line">------------createQueryPlan(SingleNodePlanner.java):268</span><br><span class="line">-------------createSelectPlan(SingleNodePlanner.java):669</span><br><span class="line">--------------createTableRefsPlan(SingleNodePlanner.java):845</span><br><span class="line">---------------createTableRefNode(SingleNodePlanner.java):1686</span><br><span class="line">----------------createScanNode(SingleNodePlanner.java)</span><br></pre></td></tr></table></figure>
<p>在FE端构造HdfsScanNode对象的时候，所有的ScanRange信息都存储在scanRangeSpecs_对象中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;HdfsScanNode.java</span><br><span class="line">&#x2F;&#x2F; Scan-range specs. Populated in init().</span><br><span class="line">protected TScanRangeSpec scanRangeSpecs_</span><br></pre></td></tr></table></figure>
<p>这里我们使用一个测试SQL，然后通过远程调试，查看这个变量的信息，如下所示：<br><img src="https://img-blog.csdnimg.cn/20210416105123682.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="1"><br>可以看到，这个scanRangeSpecs_对象中，就有232个TScanRangeLocationList对象。当FE端所有的处理都完成之后，最终会返回一个TExecRequest对象，我们同样通过远程调试，查看这个对象的信息，如下所示：<br><img src="https://img-blog.csdnimg.cn/20210416105156663.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="2"><br>通过上面的截图，我们可以看到，该测试SQL包含了两个TScanRangeSpec，分别对应两个HDFS_SCAN_NODE，一个包含了232个TScanRangeLocationList，另外一个包含了4816个，而每个TScanRangeLocationList就包含了一个TScanRange对象，这个TScanRange对象就是ScanRange在FE端的一个体现。对于HDFS_SCAN_NODE来说，TScanRange包含了1个THdfsFileSplit，其中就包含了path、offset、len等信息。当TExecRequest被传回到BE端之后，同样需要进行一系列的转换操作，相关的函数调用如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">ExecuteInternal(impala-server.cc):977</span><br><span class="line">-InitExecRequest(client-request-state.cc):1440</span><br><span class="line">-Exec(client-request-state.cc):197</span><br><span class="line">--ExecAsyncQueryOrDmlRequest(client-request-state.cc):508</span><br><span class="line">---FinishExecQueryOrDmlRequest(client-request-state.cc):518</span><br><span class="line">----SubmitForAdmission(admission-controller.cc):863         </span><br><span class="line">-----FindGroupToAdmitOrReject(admission-controller.cc):1271</span><br><span class="line">------ComputeGroupSchedules(admission-controller.cc):1248</span><br><span class="line">-------Schedule(scheduler.cc):769</span><br><span class="line">--------ComputeScanRangeAssignment(scheduler.cc):174</span><br><span class="line">---------schedule-&gt;GetFragmentExecParams(fragment.idx)-&gt;scan_range_assignment</span><br><span class="line">--------ComputeScanRangeAssignment(scheduler.cc):192</span><br><span class="line">---------ComputeScanRangeAssignment(scheduler.cc):600&#x2F;695</span><br><span class="line">----------RecordScanRangeAssignment(scheduler.cc):1090~1100</span><br><span class="line">-------Schedule(scheduler.cc):770</span><br><span class="line">--------ComputeFragmentExecParams(scheduler.cc)</span><br><span class="line">-------Schedule(scheduler.cc):771</span><br><span class="line">--------ComputeBackendExecParams(scheduler.cc)</span><br><span class="line">---FinishExecQueryOrDmlRequest(client-request-state.cc):539</span><br><span class="line">----Exec(coordinator.cc):167</span><br><span class="line">-----InitBackendStates(coordinator.cc)</span><br><span class="line">----Exec(coordinator.cc):181</span><br><span class="line">-----StartBackendExec(coordinator.cc):487</span><br><span class="line">------ExecAsync(coordinator-backend-state.cc):246</span><br><span class="line">-------SetRpcParams(coordinator-backend-state.cc):125-163</span><br></pre></td></tr></table></figure>
<p>上面这个函数调用栈比较长，而且涉及到的过程也比较复杂，这里我们就不一一展开解释。我们需要知道的是：TExecRequest中包含的这些ScanRange会被分配到各个executor上，每个executor对应的相关信息都被封装为一个BackendState对象，每个BackendState对象都包含一个BackendExecParams成员，这里就封装了ScanRange的相关信息，最终通过BackendState::ExecAsync函数在每个executor上执行真正的scan操作。我们将上述整个过程中涉及到的一些主要对象归纳为一张图，如下所示：<br><img src="https://img-blog.csdnimg.cn/20210416105228108.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70#pic_center" alt="3"><br>其中绿色部分表示的是typedef，比如PerNodeScanRanges对应的就是map&lt;TPlanNodeId, std::vector<TScanRangeParams>&gt;，黄色的部分表示的是当前这个calss/struct包含的一些关键成员，蓝色部分表示的是thrift变量以及包含关系。图中实线表示的是包含关系，箭头所指的是被包含的对象。虚线表示的是构建关系，例如我们通过TExecRequest中的plan_exec_info构造了fragment_exec_params遍变量。<br>最终，我们通过BackendState::SetRpcParams方法，将BackendState对象的相关信息封装成为TExecPlanFragmentInfo，然后发送到对应的executor进行实际的扫描。需要注意的是，每个BackendState的构造是在coordinator上进行的，而实际的scan操作是在各个executor上进行的。</p>
<h4 id="关于BackendState"><a href="#关于BackendState" class="headerlink" title="关于BackendState"></a>关于BackendState</h4><p>我们上面提到，每个executor需要的信息都会被封装成一个BackendState对象，每一个BackendState对象中，包含ScanRange信息的成员变量就是backend_exec_params_。这个变量是一个BackendExecParams的类型，可以通过上面的关系图追踪到相关的信息。为了方便理解，我们在源码中增加如下所示的DEBUG代码，可以看到整个查询的BackendState分布情况：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;在Coordinator::StartBackendExec()中进行增加</span><br><span class="line">  stringstream ss;</span><br><span class="line">  for (BackendState* backend_state: backend_states_) &#123;</span><br><span class="line">    ss &lt;&lt; &quot;Netease::BackendState: &quot; &lt;&lt; backend_state-&gt;impalad_address().hostname &lt;&lt; &quot;:&quot;</span><br><span class="line">        &lt;&lt; backend_state-&gt;impalad_address().port &lt;&lt; endl;</span><br><span class="line">    for(const FInstanceExecParams* params : backend_state-&gt;exec_params()-&gt;instance_params) &#123;</span><br><span class="line">        sss &lt;&lt; &quot;Netease::FInstanceExecParams: &quot; &lt;&lt; PrintId(params-&gt;instance_id) &lt;&lt; &quot; &quot;</span><br><span class="line">            &lt;&lt; params-&gt;host.hostname &lt;&lt; &quot;:&quot; &lt;&lt; params-&gt;host.port &lt;&lt; endl;</span><br><span class="line">        PerNodeScanRanges::const_iterator iter &#x3D; params-&gt;per_node_scan_ranges.begin();</span><br><span class="line">        while (iter !&#x3D; params-&gt;per_node_scan_ranges.end()) &#123;</span><br><span class="line">          vector&lt;TScanRangeParams&gt; scVector &#x3D; iter-&gt;second;</span><br><span class="line">          sss &lt;&lt; &quot;Netease::PlanId: &quot; &lt;&lt; iter-&gt;first &lt;&lt; &quot;, ScanRange Size: &quot;</span><br><span class="line">              &lt;&lt; scVector.size() &lt;&lt; endl;</span><br><span class="line">          iter++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  LOG(INFO) &lt;&lt; ss.str();</span><br></pre></td></tr></table></figure>
<p>其中某个BackendState的结果如下所示，可以看到该BackendState有5个fragment，其中两个包含了HDFS_SCAN，分别有345和16和ScanRange：<br><img src="https://img-blog.csdnimg.cn/20210416105254110.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="4"><br>我们直接使用某个instance id：c5478443d44931cc:767dad4400000003，在profile页面上进行搜到，可以看到该instance下的HDFS_SCAN_NODE对应的counter也是345：<br><img src="https://img-blog.csdnimg.cn/20210416105358406.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="5"></p>
<h4 id="关于ScanRangesComplete"><a href="#关于ScanRangesComplete" class="headerlink" title="关于ScanRangesComplete"></a>关于ScanRangesComplete</h4><p>在Impala的profile中，有一个ScanRangesComplete counter，我们将某个表的所有HDFS_SCAN_NODE中对应的ScanRangesComplete加在一起，就等于上面提到的TScanRangeLocationList对象数量，即232和4816。每个HDFS_SCAN_NODE的ScanRangesComplete，表示分发到这个executor上的ScanRange数量，我们对上面的测试SQL进行统计，如下所示：<br><img src="https://img-blog.csdnimg.cn/20210416105454820.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70#pic_center" alt="6"><br>从上图可以看到，一共有13个executor，分别有两个表的HDFS_SCAN_NODE。因此，我们可以将这个counter，理解为这个executor上操作的ScanRange数量，后续我们还会在提到。</p>
<h4 id="关于PerDiskState对象"><a href="#关于PerDiskState对象" class="headerlink" title="关于PerDiskState对象"></a>关于PerDiskState对象</h4><p>我们在<a href="https://skyyws.github.io/2021/04/16/Impala-HDFS-SCAN-NODE%E4%B9%8BIO-threads%E6%A8%A1%E5%9E%8B/">Impala HDFS_SCAN_NODE之IO threads模型</a>这篇文章中提到，IO thread会先获取一个RequestContext对象，每个对象都包含一个PerDiskState的集合：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;&#x2F; Per disk states to synchronize multiple disk threads accessing the same request</span><br><span class="line">&#x2F;&#x2F;&#x2F; context. One state per IoMgr disk queue.</span><br><span class="line">std::vector&lt;PerDiskState&gt; disk_states_;</span><br></pre></td></tr></table></figure>
<p>根据这个RequestContext对象的类型，获取指定的PerDiskState对象，比如remote hdfs、S3等，每个PerDiskState都包含了多个不同的ScanRange成员变量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class RequestContext::PerDiskState &#123;</span><br><span class="line">  DiskQueue* disk_queue_ &#x3D; nullptr;</span><br><span class="line">  bool done_ &#x3D; true;</span><br><span class="line">  AtomicInt32 is_on_queue_&#123;0&#125;;</span><br><span class="line">  int num_remaining_ranges_ &#x3D; 0;</span><br><span class="line">  InternalQueue&lt;ScanRange&gt; unstarted_scan_ranges_;</span><br><span class="line">  InternalQueue&lt;RequestRange&gt; in_flight_ranges_;</span><br><span class="line">  ScanRange* next_scan_range_to_start_ &#x3D; nullptr;</span><br><span class="line">  AtomicInt32 num_threads_in_op_&#123;0&#125;;</span><br><span class="line">  InternalQueue&lt;WriteRange&gt; unstarted_write_ranges_;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<p>这些成员变量都与Impala的IO thread处理流程紧密相关，下面我们就看下这些成员变量以及相关处理流程。<br>disk_queue_表示该PerDiskState所属的disk queue；done_表示这个RequestContext上的这个disk queue的扫描是否完成了；is_on_queue_表示当前这个RequestContext对象是否在队列上；num_threads_in_op_表示当前正在操作这个RequestContext对象的线程数。<br>当io thread从request_contexts_队列的头部获取一个RequestContext对象之后，就会进行对应的设置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; request-context.cc</span><br><span class="line">  void IncrementDiskThreadAfterDequeue() &#123;</span><br><span class="line">    &#x2F;&#x2F;&#x2F; Incrementing &#39;num_threads_in_op_&#39; first so that there is no window when other</span><br><span class="line">    &#x2F;&#x2F;&#x2F; threads see &#39;is_on_queue_ &#x3D;&#x3D; num_threads_in_op_ &#x3D;&#x3D; 0&#39; and think there are no</span><br><span class="line">    &#x2F;&#x2F;&#x2F; references left to this context.</span><br><span class="line">    num_threads_in_op_.Add(1);</span><br><span class="line">    is_on_queue_.Store(0);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>将num_threads_in_op_+1，然后is_on_queue_设置为0，表示该RequestContext对象已经不在队列中。当我们获取了对应的ScanRange之后，就会将is_on_queue_设置为1，并将RequestContext对象放到队尾，此时其他的io thread就可以有机会再次获取这个RequestContext对象进行处理：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; request-context.cc</span><br><span class="line">void RequestContext::PerDiskState::ScheduleContext(const unique_lock&lt;mutex&gt;&amp; context_lock,</span><br><span class="line">    RequestContext* context, int disk_id) &#123;</span><br><span class="line">  DCHECK(context_lock.mutex() &#x3D;&#x3D; &amp;context-&gt;lock_ &amp;&amp; context_lock.owns_lock());</span><br><span class="line">  if (is_on_queue_.Load() &#x3D;&#x3D; 0 &amp;&amp; !done_) &#123;</span><br><span class="line">    is_on_queue_.Store(1);</span><br><span class="line">    disk_queue_-&gt;EnqueueContext(context);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当我们处理完对应的ScanRange之后，才会将num_threads_in_op_减1，表示这个IO thread的本次处理已经完成。接着就会循环处理队列中的下一个RequestContext对象。<br>这里我们简单介绍了PerDiskState的几个成员变量，还有剩下的几个，例如unstarted_scan_ranges_、in_flight_ranges_等，相对比较复杂，由于篇幅原因，我们将在后续的文章中继续进行探究。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://skyyws.github.io/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8BScanRange%E8%AF%A6%E8%A7%A3%EF%BC%88%E4%B8%89%EF%BC%89/" data-id="cknk7yb0m0000zpx59363667q" data-title="Impala 3.4 SQL查询之ScanRange详解（三）" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Impala-SQL%E6%9F%A5%E8%AF%A2%E7%B3%BB%E5%88%97/" rel="tag">Impala SQL查询系列</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/olap/" rel="tag">olap</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Impala-HDFS-SCAN-NODE之AverageHdfsReadThreadConcurrency" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/16/Impala-HDFS-SCAN-NODE%E4%B9%8BAverageHdfsReadThreadConcurrency/" class="article-date">
  <time class="dt-published" datetime="2021-04-16T11:15:23.000Z" itemprop="datePublished">2021-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/16/Impala-HDFS-SCAN-NODE%E4%B9%8BAverageHdfsReadThreadConcurrency/">Impala HDFS_SCAN_NODE之AverageHdfsReadThreadConcurrency</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>在Impala的HDFS_SCAN_NODE中有一个counter，叫AverageHdfsReadThreadConcurrency，其相关解释如下所示：<br><img src="https://img-blog.csdnimg.cn/2021040815143591.png" alt="1"><br>简单理解为，这个值越高，那么同时参与hdfs scan的线程就会越多，在一定程度上，扫描就会更快；如果这个值比较小，那么就有可能是当前的查询比较多，导致线程被其他scan node给占用了。本文就结合代码来跟大家一起学习下，这个couter是如何计算和更新的。<br>关于这个Counter的初始化代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; hdfs-scan-node-base.h</span><br><span class="line">RuntimeProfile::Counter* average_hdfs_read_thread_concurrency_ &#x3D; nullptr;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; hdfs-scan-node-base.cc</span><br><span class="line">average_hdfs_read_thread_concurrency_ &#x3D;</span><br><span class="line">PROFILE_AverageHdfsReadThreadConcurrency.Instantiate(runtime_profile(),</span><br><span class="line">&amp;active_hdfs_read_thread_counter_);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; runtime-profile-counters.h</span><br><span class="line">#define PROFILE_DEFINE_SAMPLING_COUNTER(name, significance, desc) \</span><br><span class="line">  ::impala::SamplingCounterPrototype PROFILE_##name( \</span><br><span class="line">      #name, ::impala::ProfileEntryPrototype::Significance::significance, desc)</span><br></pre></td></tr></table></figure>
<p>上面这几行代码，首先通过一个宏定义，将“AverageHdfsReadThreadConcurrency”绑定到了一个<br>SamplingCounterPrototype，即PROFILE_AverageHdfsReadThreadConcurrency。然后利用这个Prototype来实例化产生SamplingCounter。关于Instantiate函数的具体实现，我们接着往下看：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; runtime-profile-counters.h</span><br><span class="line">  RuntimeProfile::Counter* Instantiate(RuntimeProfile* profile,</span><br><span class="line">      RuntimeProfile::Counter* src_counter) &#123;</span><br><span class="line">    return profile-&gt;AddSamplingCounter(name(), src_counter);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">RuntimeProfile::Counter* RuntimeProfile::AddSamplingCounter(</span><br><span class="line">    const string&amp; name, Counter* src_counter) &#123;</span><br><span class="line">  DCHECK(src_counter-&gt;unit() &#x3D;&#x3D; TUnit::UNIT);</span><br><span class="line">  lock_guard&lt;SpinLock&gt; l(counter_map_lock_);</span><br><span class="line">  bool created;</span><br><span class="line">  Counter* dst_counter &#x3D; AddCounterLocked(name, TUnit::DOUBLE_VALUE, &quot;&quot;, &amp;created);</span><br><span class="line">  if (!created) return dst_counter;</span><br><span class="line">  sampling_counters_.push_back(dst_counter);</span><br><span class="line">  PeriodicCounterUpdater::RegisterPeriodicCounter(src_counter, NULL, dst_counter,</span><br><span class="line">      PeriodicCounterUpdater::SAMPLING_COUNTER);</span><br><span class="line">  has_active_periodic_counters_ &#x3D; true;</span><br><span class="line">  return dst_counter;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在Instantiate函数中，主要就是调用了一个AddSamplingCounter函数，这个函数首先将当前的这个counter保存到名为sampling_counters_的vector中，这个vector后续会用来控制停止这些counter的采集、更新，后面会再提到。接着会将active_hdfs_read_thread_counter_和AverageHdfsReadThreadConcurrency通过RegisterPeriodicCounter函数，注册为一个SAMPLING_COUNTER类型的PeriodicCounter。如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; periodic-counter-updater.cc</span><br><span class="line">void PeriodicCounterUpdater::RegisterPeriodicCounter(</span><br><span class="line">    RuntimeProfile::Counter* src_counter,</span><br><span class="line">    RuntimeProfile::SampleFunction sample_fn,</span><br><span class="line">    RuntimeProfile::Counter* dst_counter, PeriodicCounterType type) &#123;</span><br><span class="line">  DCHECK(src_counter &#x3D;&#x3D; NULL || sample_fn &#x3D;&#x3D; NULL);</span><br><span class="line"></span><br><span class="line">  switch (type) &#123;</span><br><span class="line">    case RATE_COUNTER: &#123;</span><br><span class="line">      &#x2F;&#x2F; 省略部分代码</span><br><span class="line">    &#125;</span><br><span class="line">    case SAMPLING_COUNTER: &#123;</span><br><span class="line">      SamplingCounterInfo counter;</span><br><span class="line">      counter.src_counter &#x3D; src_counter;</span><br><span class="line">      counter.sample_fn &#x3D; sample_fn;</span><br><span class="line">      counter.num_sampled &#x3D; 0;</span><br><span class="line">      counter.total_sampled_value &#x3D; 0;</span><br><span class="line">      lock_guard&lt;SpinLock&gt; samplinglock(instance_-&gt;sampling_lock_);</span><br><span class="line">      instance_-&gt;sampling_counters_[dst_counter] &#x3D; counter;</span><br><span class="line">      break;</span><br><span class="line">    &#125;</span><br><span class="line">    default:</span><br><span class="line">      DCHECK(false) &lt;&lt; &quot;Unsupported PeriodicCounterType:&quot; &lt;&lt; type;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从上述代码我们可以看到，active_hdfs_read_thread_counter_被包装为了一个SamplingCounterInfo，这里主要保存的指标有两个：total_sampled_value和num_sampled，分别表示采集的value总和、采集次数。请注意，这里对应的是active_hdfs_read_thread_counter_这个counter的采集数据，而不是AverageHdfsReadThreadConcurrency。<br>所有的SAMPLING_COUNTER都会保存在一个名为sampling_counters_的map中，这个map的key对应的就是我们这里的AverageHdfsReadThreadConcurrency，而value则是一个SamplingCounterInfo，里面包含一个src的counter，表示数据采集的来源，在这里就是active_hdfs_read_thread_counter_。在启动impalad之后，会专门启动一个线程来定时处理这些SAMPLING_COUNTER，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; periodic-counter-updater.cc</span><br><span class="line">void PeriodicCounterUpdater::Init() &#123;</span><br><span class="line">  DCHECK(instance_ &#x3D;&#x3D; nullptr);</span><br><span class="line">  &#x2F;&#x2F; Create the singleton, which will live until the process terminates.</span><br><span class="line">  instance_ &#x3D; new PeriodicCounterUpdater;</span><br><span class="line">  instance_-&gt;update_thread_.reset(</span><br><span class="line">      new thread(&amp;PeriodicCounterUpdater::UpdateLoop, instance_));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于这些PeriodicCounter更新的处理逻辑都在UpdateLoop这个函数里面，除了SamplingCounter之外，还有RatingCounter、BucketingCounter等，这里我们关注下SamplingCounter的处理：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">void PeriodicCounterUpdater::UpdateLoop() &#123;</span><br><span class="line">  while (true) &#123;</span><br><span class="line">    &#123;</span><br><span class="line">      lock_guard&lt;SpinLock&gt; samplinglock(instance_-&gt;sampling_lock_);</span><br><span class="line">      for (SamplingCounterMap::iterator it &#x3D; sampling_counters_.begin();</span><br><span class="line">           it !&#x3D; sampling_counters_.end(); ++it) &#123;</span><br><span class="line">        ++it-&gt;second.num_sampled;</span><br><span class="line">        int64_t value;</span><br><span class="line">        if (it-&gt;second.src_counter !&#x3D; NULL) &#123;</span><br><span class="line">          value &#x3D; it-&gt;second.src_counter-&gt;value();</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          DCHECK(it-&gt;second.sample_fn !&#x3D; NULL);</span><br><span class="line">          value &#x3D; it-&gt;second.sample_fn();</span><br><span class="line">        &#125;</span><br><span class="line">        it-&gt;second.total_sampled_value +&#x3D; value;</span><br><span class="line">        double average &#x3D; static_cast&lt;double&gt;(it-&gt;second.total_sampled_value) &#x2F;</span><br><span class="line">            it-&gt;second.num_sampled;</span><br><span class="line">        it-&gt;first-&gt;Set(average);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F;省略其余代码</span><br></pre></td></tr></table></figure>
<p>从上面这段代码可以看到，每次采集更新的时候，active_hdfs_read_thread_counter_的total_sampled_value和num_sampled就会进行更新、累加。并且first的值（这里就是AverageHdfsReadThreadConcurrency）也会被更新为最新的average，即total_sampled_value/num_sampled。所以，即使查询正在执行中，如果我们刷新profile，也可以得到最新的average。<br>值得一提的是，采集间隔可以通过一个参数来进行配置，默认是500ms：<br><img src="https://img-blog.csdnimg.cn/20210408151519406.png" alt="2"><br>通过上面关于AverageHdfsReadThreadConcurrency这个counter的计算、更新介绍，我们都知道与active_hdfs_read_thread_counter_这个counter有关，下面我们就来看下这个变量是如何更新的。相关代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; hdfs-scan-node-base.h</span><br><span class="line">&#x2F;&#x2F;&#x2F; The number of active hdfs reading threads reading for this node.</span><br><span class="line">RuntimeProfile::Counter active_hdfs_read_thread_counter_;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; request_context.h</span><br><span class="line">&#x2F;&#x2F;&#x2F; Number of active read threads</span><br><span class="line">RuntimeProfile::Counter* active_read_thread_counter_ &#x3D; nullptr;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; </span><br><span class="line">reader_context_-&gt;set_active_read_thread_counter(&amp;active_hdfs_read_thread_counter_);</span><br></pre></td></tr></table></figure>
<p>active_hdfs_read_thread_counter_这个counter，我们通过注释可以知道，表示的是当前这个hdfs scan node活跃的io threads数量。在构造hdfs scan node的时候，将这个counter设置到RequestContext的active_read_thread_counter_。因此，我们目前的关注点就转换到了active_read_thread_counter_这个变量上。在上一篇文章中，我们提到了关于RequestContext和ScanRange的相关情况，没看过的读者可以简单浏览下：<a href="https://skyyws.github.io/2021/04/16/Impala-HDFS-SCAN-NODE%E4%B9%8BIO-threads%E6%A8%A1%E5%9E%8B">Impala HDFS_SCAN_NODE之IO threads模型</a>。在这篇文章中，我们提到了：io thread会首先从RequestContext队列中获取头部元素，接着通过该RequestContext对象获取一个ScanRange。相关调用栈如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DiskThreadLoop(disk-io-mrg.cc)</span><br><span class="line">-GetNextRequestRange(disk-io-mrg.cc)</span><br><span class="line">--GetNextRequestRange(request-context.cc)</span><br><span class="line">-DoRead(scan-range.cc)</span><br></pre></td></tr></table></figure>
<p>在DoRead方法中，就会对active_read_thread_counter_进行加减操作，这里我们只展示相关的代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; scan-ranger.cc</span><br><span class="line">ReadOutcome ScanRange::DoRead(DiskQueue* queue, int disk_id) &#123;</span><br><span class="line">  &#x2F;&#x2F;省略其余代码</span><br><span class="line">  Status read_status &#x3D; file_reader_-&gt;Open(use_file_handle_cache);</span><br><span class="line">  bool eof &#x3D; false;</span><br><span class="line">  if (read_status.ok()) &#123;</span><br><span class="line">    COUNTER_ADD_IF_NOT_NULL(reader_-&gt;active_read_thread_counter_, 1L);</span><br><span class="line">    COUNTER_BITOR_IF_NOT_NULL(reader_-&gt;disks_accessed_bitmap_, 1LL &lt;&lt; disk_id);</span><br><span class="line">    DebugScanRangeInfo();</span><br><span class="line"></span><br><span class="line">    if (sub_ranges_.empty()) &#123;</span><br><span class="line">      DCHECK(cache_.data &#x3D;&#x3D; nullptr);</span><br><span class="line">      read_status &#x3D; file_reader_-&gt;ReadFromPos(queue, offset_ + bytes_read_,</span><br><span class="line">          buffer_desc-&gt;buffer_,</span><br><span class="line">          min(bytes_to_read() - bytes_read_, buffer_desc-&gt;buffer_len_),</span><br><span class="line">          &amp;buffer_desc-&gt;len_, &amp;eof);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      read_status &#x3D; ReadSubRanges(queue, buffer_desc.get(), &amp;eof);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    COUNTER_ADD_IF_NOT_NULL(reader_-&gt;bytes_read_counter_, buffer_desc-&gt;len_);</span><br><span class="line">    COUNTER_ADD_IF_NOT_NULL(reader_-&gt;active_read_thread_counter_, -1L);</span><br><span class="line">  &#125;</span><br><span class="line">  &#x2F;&#x2F;省略其余代码</span><br></pre></td></tr></table></figure>
<p>当获取到指定的ScanRange之后，会首先调用Open方法打开文件，如果打开成功的话，则active_read_thread_counter_就会加1，表示当前已经有一个线程正在对某个ScanRange进行扫描操作。接着就会执行实际的扫描操作，关于hdfs file的扫描不是本身关注的重点，这里就不再展开描述。扫描完成之后，active_read_thread_counter_就会减1，表示这个线程对于ScanRange的扫描已经结束了。通过这些代码分析，我们可以知道，对于active_read_thread_counter_，就可以理解为当前有多少个io thread正在扫描ScanRange，而AverageHdfsReadThreadConcurrency表示的就是：某个hdfs scan node从开始执行到当前时间点为止，平均io thread并发数（采集到io thread总数/采集次数），这个值越大，表示同一时间，用于处理ScanRange的线程数就越多，相应的hdfs scan就会越快（这里指的是io thread scan阶段，不包括后续的scanner处理阶段）。如果这个值比较小的话，那么说明同时处理ScanRange的线程数就很少，那么可能就会导致扫描很慢，进而表现为整个的hdfs scan node节点很慢。<br>除了我们上面介绍的AverageHdfsReadThreadConcurrency这个counter，还有一个counter也值得看一下，即“Hdfs Read Thread Concurrency Bucket”，如下所示：<br><img src="https://img-blog.csdnimg.cn/2021040815155619.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="3"><br>这个counter与AverageHdfsReadThreadConcurrency有一定的联系，我们同样从代码层面看下该counter是如何进行计算的。相关函数初始化代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; hdfs-scan-node-hbase.h</span><br><span class="line">&#x2F;&#x2F;&#x2F; HDFS read thread concurrency bucket: bucket[i] refers to the number of sample</span><br><span class="line">&#x2F;&#x2F;&#x2F; taken where there are i concurrent hdfs read thread running. Created in Open().</span><br><span class="line">std::vector&lt;RuntimeProfile::Counter*&gt;* hdfs_read_thread_concurrency_bucket_ &#x3D; nullptr;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; hdfs-scan-node-hbase.cc</span><br><span class="line">hdfs_read_thread_concurrency_bucket_ &#x3D; runtime_profile()-&gt;AddBucketingCounters(</span><br><span class="line">&amp;active_hdfs_read_thread_counter_,</span><br><span class="line">ExecEnv::GetInstance()-&gt;disk_io_mgr()-&gt;num_total_disks() + 1);</span><br></pre></td></tr></table></figure>
<p>active_hdfs_read_thread_counter_被绑定到了一个BucketingCounter，其中桶的数量就是disk_queues_.size()+1，以上述截图为例：机器上有3块本地磁盘，REMOTE_NUM_DISKS的值为5，所以bucket数量为9个，序号是0～8。增加BucketingCounter的流程与上述的SamplingCounter类似，都是先Add，再Register。相关代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; hdfs-scan-node-hbase.cc</span><br><span class="line">&#x2F;&#x2F; 这里的bucketing_counters_是一个set，set中的每个元素都是一个vector</span><br><span class="line">vector&lt;RuntimeProfile::Counter*&gt;* RuntimeProfile::AddBucketingCounters(</span><br><span class="line">    Counter* src_counter, int num_buckets) &#123;</span><br><span class="line">  lock_guard&lt;SpinLock&gt; l(counter_map_lock_);</span><br><span class="line">  vector&lt;RuntimeProfile::Counter*&gt;* buckets &#x3D; pool_-&gt;Add(new vector&lt;Counter*&gt;);</span><br><span class="line">  for (int i &#x3D; 0; i &lt; num_buckets; ++i) &#123;</span><br><span class="line">      buckets-&gt;push_back(</span><br><span class="line">          pool_-&gt;Add(new RuntimeProfile::Counter(TUnit::DOUBLE_VALUE, 0)));</span><br><span class="line">  &#125;</span><br><span class="line">  bucketing_counters_.insert(buckets);</span><br><span class="line">  has_active_periodic_counters_ &#x3D; true;</span><br><span class="line">  PeriodicCounterUpdater::RegisterBucketingCounters(src_counter, buckets);</span><br><span class="line">  return buckets;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; periodic-counter-updater.cc</span><br><span class="line">&#x2F;&#x2F; 这里的bucketing_counters_是一个map，key是一个vector，value是一个BucketCountersInfo</span><br><span class="line">void PeriodicCounterUpdater::RegisterBucketingCounters(</span><br><span class="line">    RuntimeProfile::Counter* src_counter, vector&lt;RuntimeProfile::Counter*&gt;* buckets) &#123;</span><br><span class="line">  BucketCountersInfo info;</span><br><span class="line">  info.src_counter &#x3D; src_counter;</span><br><span class="line">  info.num_sampled &#x3D; 0;</span><br><span class="line">  lock_guard&lt;SpinLock&gt; bucketinglock(instance_-&gt;bucketing_lock_);</span><br><span class="line">  instance_-&gt;bucketing_counters_[buckets] &#x3D; info;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>先初始化一个vector，成员数量就是bucket数，每一个成员都是一个counter，这些counter的值都初始化为0，接着将这个vector保存到bucketing_counters_中，这里的bucketing_counters_也是用于控制后续的counter停止采集。然后我们再进行注册（类似上面的注册PeriodicCounter）。在进行注册的时候，首先会构造一个BucketCountersInfo来封装active_hdfs_read_thread_counter_，然后将这个info保存到bucketing_counters_中，这里的bucketing_counters_同样是一个map，map的key就是一个vector，比如上面代码中的buckets变量，而value则是一个BucketCountersInfo。之后线程就会通过UpdateLoop函数来循环处理bucketing_counters_，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; periodic-counter-updater.cc</span><br><span class="line">&#x2F;&#x2F; 与上面的sampling_counters_处理在同一个函数中</span><br><span class="line">void PeriodicCounterUpdater::UpdateLoop() &#123;</span><br><span class="line">  &#x2F;&#x2F;省略其余代码</span><br><span class="line">      &#123;</span><br><span class="line">      lock_guard&lt;SpinLock&gt; bucketinglock(instance_-&gt;bucketing_lock_);</span><br><span class="line">      for (BucketCountersMap::iterator it &#x3D; bucketing_counters_.begin();</span><br><span class="line">           it !&#x3D; bucketing_counters_.end(); ++it) &#123;</span><br><span class="line">        int64_t val &#x3D; it-&gt;second.src_counter-&gt;value();</span><br><span class="line">        if (val &gt;&#x3D; it-&gt;first-&gt;size()) val &#x3D; it-&gt;first-&gt;size() - 1;</span><br><span class="line">        it-&gt;first-&gt;at(val)-&gt;Add(1);</span><br><span class="line">        ++it-&gt;second.num_sampled;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F;省略其余代码</span><br></pre></td></tr></table></figure>
<p>上述代码的处理逻辑就是：获取当前active_hdfs_read_thread_counter_的值（即并发处理ScanRange的线程数量）保存为val，判断该值是否大于等于bucket数量（这里是9）。如果是的话，则将val更新为bucket数量减1，否则直接使用val。然后将vector中下标为val的counter加1。最后更新采集次数。基于上述的代码处理，笔者对于这个BucketingCounter的理解是：一共划分成disk_queues_.size()+1个bucket，序号从0～disk_queues_.size()，每个bucket对应的下标表示线程数。如果当前采集的线程数小于bucket数，则直接将下标对应的budcket进行累加；如果大于等于bucket数，则全部累加到下标为disk_queues_.size()的budcket，即最后一个bucket。也就是说用来统计各个线程并发数的比例，当并发线程数大于等于bucket数的时候，全部放到最后一个桶。但是为什么初始化的时候，设置disk_queues_.size()+1个bucket，笔者目前也没有完全弄清楚。<br>上面我们只是统计了bucket对应的线程数的出现次数，最后我们还需要再计算一个百分比，相关代码处理如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; periodic-counter-updater.cc</span><br><span class="line">void PeriodicCounterUpdater::StopBucketingCounters(</span><br><span class="line">    vector&lt;RuntimeProfile::Counter*&gt;* buckets) &#123;</span><br><span class="line">  &#x2F;&#x2F;省略其余代码</span><br><span class="line">  if (num_sampled &gt; 0) &#123;</span><br><span class="line">    for (RuntimeProfile::Counter* counter : *buckets) &#123;</span><br><span class="line">      double perc &#x3D; 100 * counter-&gt;value() &#x2F; (double)num_sampled;</span><br><span class="line">      counter-&gt;Set(perc);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>我们可以看到，这里主要就是遍历每个bucket对应的counter，然后用当前counter的累加值除以总的采样次数，就是该counter的占比。当scan node结束之后，就会停止所有的PeriodicCounter，包括SamplingCounter、RateCounter、BucketingCounter等，就会调用上述的函数。相关调用栈如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Close(hdfs-scan-node.cc)</span><br><span class="line">-StopAndFinalizeCounters(hdfs-scan-node.cc)</span><br><span class="line">--StopPeriodicCounters(runtime-profile.cc)</span><br><span class="line">---StopSamplingCounter(periodic-counter-updater.cc)</span><br><span class="line">---StopRateCounter(periodic-counter-updater.cc)</span><br><span class="line">---StopBucketingCounters(periodic-counter-updater.cc)</span><br></pre></td></tr></table></figure>
<p> 前面我们提到了sampling_counters_和bucketing_counters_这两个list集合是用来控制counter的停止采集，这里就是在StopPeriodicCounters方法中，通过for循环遍历，来逐个停掉这些counter的采集。<br>上面我们讲了hdfs_read_thread_concurrency_bucket_这个BucketingCounter的更新和计算，下面我们来看下最终是如何输出到Profile的，相关代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; hdfs-scan-node-base.cc</span><br><span class="line">&#x2F;&#x2F; Output hdfs read thread concurrency into info string</span><br><span class="line">stringstream ss;</span><br><span class="line">for (int i &#x3D; 0; i &lt; hdfs_read_thread_concurrency_bucket_-&gt;size(); ++i) &#123;</span><br><span class="line">  ss &lt;&lt; i &lt;&lt; &quot;:&quot; &lt;&lt; setprecision(4)</span><br><span class="line">     &lt;&lt; (*hdfs_read_thread_concurrency_bucket_)[i]-&gt;double_value() &lt;&lt; &quot;% &quot;;</span><br><span class="line">&#125;</span><br><span class="line">runtime_profile_-&gt;AddInfoString(&quot;Hdfs Read Thread Concurrency Bucket&quot;, ss.str());</span><br></pre></td></tr></table></figure>
<p>这段代码比较简单，就是循环打印每个bucket对应的counter中保存的value，就是百分比，最终拼接成一个字符串输出即可。<br>到这里，关于AverageHdfsReadThreadConcurrency这个counter以及“Hdfs Read Thread Concurrency Bucket”我们就介绍的差不多了。本文的介绍都是笔者基于代码的个人理解，如有问题，欢迎指正。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://skyyws.github.io/2021/04/16/Impala-HDFS-SCAN-NODE%E4%B9%8BAverageHdfsReadThreadConcurrency/" data-id="cknk7un7d0000w7x51hw36cho" data-title="Impala HDFS_SCAN_NODE之AverageHdfsReadThreadConcurrency" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Impala%E4%B9%8BHDFS-SCAN-NODE/" rel="tag">Impala之HDFS_SCAN_NODE</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/olap/" rel="tag">olap</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Impala-HDFS-SCAN-NODE之IO-threads模型" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/16/Impala-HDFS-SCAN-NODE%E4%B9%8BIO-threads%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time class="dt-published" datetime="2021-04-16T11:10:43.000Z" itemprop="datePublished">2021-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/16/Impala-HDFS-SCAN-NODE%E4%B9%8BIO-threads%E6%A8%A1%E5%9E%8B/">Impala HDFS_SCAN_NODE之IO threads模型</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>本文主要从代码出发，跟大家一起分享下Impala HDFS_SCAN_NODE中的IO threads模型。首先，在Impala中，有几个io threads相关的配置，通过对这几个参数进行配置，我们就可以增加处理io的线程数，相关的几个配置如下所示：<br><img src="https://img-blog.csdnimg.cn/20210331144325103.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="1"><br>以我们最常见的hdfs存储引擎为例，如果impalad节点与datanode节点在一台机器上，对于impala来说，就是可以通过本地的disk直接读取数据；如果impalad节点与datanode在不同的机器上，那么就是remote的读取。在我们内部的生产环境，大部分都是这样的情况：有一个公共的HDFS集群，业务所有的离线数据都存储在上面，我们需要单独部署一个Impala集群，对于HDFS集群上的某些数据进行Ad-hoc类的多维分析，此时impala就是通过remote来读取hdfs的数据，那么将num_remote_hdfs_io_threads配置项调整的大一些，就可以适当地加快hdfs scan的速度。<br>在正式开启介绍之前，我们需要知道Impala的scan node模型分为两层：1）IO threads，这层主要就是通过IO读取远端的hdfs数据，并且返回，通过配置num_remote_hdfs_io_threads参数，就可以调整读取的线程数，值得一提的是，一些谓词可以下推到远端的hdfs，减少扫描返回的数据量；2）Scanner，当数据从远端的HDFS返回之后，会由专门的scanner线程进行处理，可能的操作包括：数据解码、cast计算等。本文我们主要讲的就是第一层IO threads，其他更多的介绍可以参考：<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/63405729/why-impala-scan-node-is-very-slow-rowbatchqueuegetwaittime">Why Impala Scan Node is very slow</a>中Tim Armstrong的回答，这篇CSDN的博客也有介绍：<a target="_blank" rel="noopener" href="https://blog.csdn.net/yu616568/article/details/74996897?spm=1001.2014.3001.5501">Impala高性能探秘之HDFS数据访问</a>。<br>下面，我们就结合代码来简单看下这个参数是如何起作用的。在Impala的BE代码中，有一个类专门用来管理IO相关的操作，用于访问本地磁盘或者远端的文件系统，即DiskIoMgr。在这个类中，有一个disk_queues_成员，这是一个集合，每个成员都代表一个disk对应的队列，或者是一种远端文件系统，例如HDFS/S3等，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; disk-io-mrg.h</span><br><span class="line">  &#x2F;&#x2F;&#x2F; Per disk queues. This is static and created once at Init() time.  One queue is</span><br><span class="line">  &#x2F;&#x2F;&#x2F; allocated for each local disk on the system and for each remote filesystem type.</span><br><span class="line">  &#x2F;&#x2F;&#x2F; It is indexed by disk id.</span><br><span class="line">  std::vector&lt;DiskQueue*&gt; disk_queues_;</span><br></pre></td></tr></table></figure>
<p>首先会在构造函数中，对这个变量进行resize操作，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; disk-io-mrg.cc</span><br><span class="line">disk_queues_.resize(num_local_disks + REMOTE_NUM_DISKS);</span><br></pre></td></tr></table></figure>
<p>这里的num_local_disks指的就是本地磁盘的个数，而REMOTE_NUM_DISKS就是一个enum变量，用来控制远端访问的偏移：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; disk-io-mrg.h</span><br><span class="line">  &#x2F;&#x2F;&#x2F; &quot;Disk&quot; queue offsets for remote accesses.  Offset 0 corresponds to</span><br><span class="line">  &#x2F;&#x2F;&#x2F; disk ID (i.e. disk_queue_ index) of num_local_disks().</span><br><span class="line">  enum &#123;</span><br><span class="line">    REMOTE_DFS_DISK_OFFSET &#x3D; 0,</span><br><span class="line">    REMOTE_S3_DISK_OFFSET,</span><br><span class="line">    REMOTE_ADLS_DISK_OFFSET,</span><br><span class="line">    REMOTE_ABFS_DISK_OFFSET,</span><br><span class="line">    REMOTE_OZONE_DISK_OFFSET,</span><br><span class="line">    REMOTE_NUM_DISKS</span><br><span class="line">  &#125;;</span><br></pre></td></tr></table></figure>
<p>所以，impala将每一种远端的文件系统访问，也当成了一个disk，按照上述的enum顺序，放到disk_queues_中，作为一个成员变量。接着在Init函数中，会循环对这个disk_queues_变量进行初始化：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; disk-io-mrg.cc</span><br><span class="line">  for (int i &#x3D; 0; i &lt; disk_queues_.size(); ++i) &#123;</span><br><span class="line">    disk_queues_[i] &#x3D; new DiskQueue(i);</span><br><span class="line">    int num_threads_per_disk;</span><br><span class="line">    string device_name;</span><br><span class="line">    if (i &#x3D;&#x3D; RemoteDfsDiskId()) &#123;</span><br><span class="line">      num_threads_per_disk &#x3D; FLAGS_num_remote_hdfs_io_threads;</span><br><span class="line">      device_name &#x3D; &quot;HDFS remote&quot;;</span><br></pre></td></tr></table></figure>
<p>在整个for循环中，会根据id来判断是需要对哪一个队列进行操作，这里以HDFS为例，id就是本地磁盘的数量+HDFS在enum中的offset：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; disk-io-mrg.cc</span><br><span class="line">  &#x2F;&#x2F;&#x2F; The disk ID (and therefore disk_queues_ index) used for DFS accesses.</span><br><span class="line">  int RemoteDfsDiskId() const &#123; return num_local_disks() + REMOTE_DFS_DISK_OFFSET; &#125;</span><br></pre></td></tr></table></figure>
<p>如果是要访问远端的HDFS，那么对应的线程数量，即num_threads_per_disk，就是我们通过配置文件指定的num_remote_hdfs_io_threads的值，默认是8。表示会启动8个线程用于处理远端的HDFS访问操作。接着，impala就会循环创建对应数量的线程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; disk-io-mrg.cc</span><br><span class="line">    for (int j &#x3D; 0; j &lt; num_threads_per_disk; ++j) &#123;</span><br><span class="line">      stringstream ss;</span><br><span class="line">      ss &lt;&lt; &quot;work-loop(Disk: &quot; &lt;&lt; device_name &lt;&lt; &quot;, Thread: &quot; &lt;&lt; j &lt;&lt; &quot;)&quot;;</span><br><span class="line">      std::unique_ptr&lt;Thread&gt; t;</span><br><span class="line">      RETURN_IF_ERROR(Thread::Create(&quot;disk-io-mgr&quot;, ss.str(), &amp;DiskQueue::DiskThreadLoop,</span><br><span class="line">          disk_queues_[i], this, &amp;t));</span><br><span class="line">      disk_thread_group_.AddThread(move(t));</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>在进行线程创建的时候，将函数DiskQueue::DiskThreadLoop绑定到了该线程上，该函数就是通过一个while循环来不断的进行处理，相关的函数调用如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DiskThreadLoop(disk-io-mrg.cc)</span><br><span class="line">-GetNextRequestRange(disk-io-mrg.cc)</span><br><span class="line">--GetNextRequestRange(request-context.cc)</span><br><span class="line">-DoRead(scan-range.cc)&#x2F;Write(disk-io-mgr.cc)</span><br></pre></td></tr></table></figure>
<p>GetNextRequestRange函数就是用来获取当前这个DiskQueue（例如远端的HDFS访问queue）的下一个RequestRange，来进行具体的io操作。RequestRange代表一个文件中的连续字节序列，主要分为：ScanRange和WriteRange。每个disk线程一次只能处理一个RequestRange。这里impala采用了一个两层的设计，在GetNextRequestRange中，首先会需要获取一个RequestContext对象，RequestContext可以理解为一个查询的某个instance下的所有IO请求集合，可以简单理解为某个表的RequestRange集合都被封装在一个RequestContext对象中。获取RequestContext的代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">*request_context &#x3D; request_contexts_.front();</span><br><span class="line">request_contexts_.pop_front();</span><br><span class="line">DCHECK(*request_context !&#x3D; nullptr);</span><br></pre></td></tr></table></figure>
<p>request_contexts_是一个RequestContext类型的list，每一个DiskQueue都包含了这样一个队列，表示该DiskQueue上的所有的待处理的RequestContext列表。这里我们可以简单的理解为每个表的扫描请求，都在这个队列中等待处理。首先会从队列的头部取出一个RequestContext，然后将该对象弹出。该DiskQueue的其他线程就可以继续处理后续的RequestContext对象，这样就不会因为当前的RequestContext对象处理时间过长，而阻塞了其他的RequestContext对象处理。<br>关于request_contexts_队列成员更新，不是本文介绍的重点，只要知道：当提交查询的时候，impalad会自动进行解析，然后进行封装，最后添加到该队列中即可。<br>在获取到RequestContext对象之后，我们就可以通过该RequestContext的GetNextRequestRange方法获取具体的RequestRange对象进行实际的扫描操作了。<br>上面的描述可能不太容易理解，我们将上述的各个成员之间的包含关系以及操作流程进行了整理成了一张图，如下所示：<br><img src="https://img-blog.csdnimg.cn/20210331144401259.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="2"><br>最终获取到了一个RequestRange之后，会进行判断，是READ还是WRITE，进行相应地处理。这里我们以READ为例，相关函数调用如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DiskThreadLoop(disk-io-mrg.cc)</span><br><span class="line">-GetNextRequestRange(disk-io-mrg.cc)</span><br><span class="line">--GetNextRequestRange(request-context.cc)</span><br><span class="line">-DoRead(scan-range.cc)</span><br><span class="line">-ReadDone(request-context.cc)</span><br></pre></td></tr></table></figure>
<p>从上面的相关代码，我们可以知道，如果我们将num_remote_hdfs_io_threads参数配置的更大一些，那么就会有更多的线程并发的通过DiskThreadLoop获取到RequestRange进行处理，从而可以在一定程度上提到SCAN的速度，进而加快整个查询进程。<br>在Impala的profile中，对于HDFS的IO theads的指标，即AverageHdfsReadThreadConcurrency，相关介绍如下所示：<br><img src="https://img-blog.csdnimg.cn/2021033114441848.png" alt="3"><br>可以简单理解为该HDFS_SCAN_NODE有多少个IO线程用于处于读写请求操作。所以说，如果线上查询的这个指标很小，那么就要考虑适当调整num_remote_hdfs_io_threads这个参数了。与这个指标很相似的是AverageScannerThreadConcurrency，这个表示scanner线程的执行数量，与我前面提到的scan node两层模型中的scanner对应，这个之后再详细介绍。除此之外，还有其他的一些指标，例如ScannerIoWaitTime，表示scanner等到IO线程的数据就绪的时间，如果这个时间很长，那么说明IO线程存在瓶颈。还有很多指标，就不再一一展开描述。我们在线上排查慢查询的时候，这些指标都是非常有用的信息。<br>上面提到了profile中的指标信息。另外，在impala服务启动之后，我们也可以通过web页面上的/threadz页面查看“disk-io-mgr”这个组下面的线程信息，就可以看到用于处理远端HDFS读取的线程：<br><img src="https://img-blog.csdnimg.cn/20210331144429621.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="3"><br>上面的User/Kernel CPU和IO-wait的时间，都是直接从机器上读取的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; os-util.h</span><br><span class="line">&#x2F;&#x2F;&#x2F; Populates ThreadStats object for a given thread by reading from</span><br><span class="line">&#x2F;&#x2F;&#x2F; &#x2F;proc&#x2F;&lt;pid&gt;&#x2F;task&#x2F;&lt;tid&gt;&#x2F;stats. Returns OK unless the file cannot be read or is in an</span><br><span class="line">&#x2F;&#x2F;&#x2F; unrecognised format, or if the kernel version is not modern enough.</span><br><span class="line">Status GetThreadStats(int64_t tid, ThreadStats* stats);</span><br></pre></td></tr></table></figure>
<p>对于每个disk queue，impala还绑定了对应的metric信息，如下所示：<br><img src="https://img-blog.csdnimg.cn/20210331144443304.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="4"><br>这些metric代表的就是读取延时和大小的统计直方图信息。<br>到这里，关于HDFS_SCAN_NODE的IO threads就介绍的差不多了，我们通过代码分析，知道了Impala对于disk以及各种远端dfs的处理，这些都是属于IO threads部分，后续有时间再跟大家一起学习scanner模块的相关知识。本文涉及到的代码分析模块，都是笔者自己根据源码分析解读出来，如有错误，欢迎指正。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://skyyws.github.io/2021/04/16/Impala-HDFS-SCAN-NODE%E4%B9%8BIO-threads%E6%A8%A1%E5%9E%8B/" data-id="cknk7opbs0000okx5gva6gjur" data-title="Impala HDFS_SCAN_NODE之IO threads模型" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Impala%E4%B9%8BHDFS-SCAN-NODE/" rel="tag">Impala之HDFS_SCAN_NODE</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/olap/" rel="tag">olap</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Impala-3-4-SQL查询之重写（二）" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8B%E9%87%8D%E5%86%99%EF%BC%88%E4%BA%8C%EF%BC%89/" class="article-date">
  <time class="dt-published" datetime="2021-04-16T06:21:01.000Z" itemprop="datePublished">2021-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8B%E9%87%8D%E5%86%99%EF%BC%88%E4%BA%8C%EF%BC%89/">Impala 3.4 SQL查询之重写（二）</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>在上一篇文章中，我们介绍了Impala基本的SQL解析流程。本文我们将跟大家一起看下Impala中的一些SQL重写规则。这里，我们首先回顾下关于Analyzer的几个类的关系图，如下所示：<br><img src="https://img-blog.csdnimg.cn/20201229112124401.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="analyzer"></p>
<p>当SQL被解析为特定的StatementBase之后，紧接着会构造一个AnalysisContext对象，这个类可以理解为整个SQL解析过程的封装，包括了：parsing, analyzing and rewriting这几个过程。在AnalysisContext的analyze方法中，我们构造了Analyzer变量，完成了对StatementBase的analyze（在上篇文章中也已经介绍过）。相关函数调用过程如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Frontend.doCreateExecRequest()</span><br><span class="line">-AnalysisContext.analyzeAndAuthorize()</span><br><span class="line">--AnalysisContext.analyze()</span><br><span class="line">---AnalysisContext.createAnalyzer()</span><br><span class="line">----Analyzer.ctor()</span><br><span class="line">-----GlobalState.ctor()</span><br><span class="line">---StatementBase.analyze()</span><br><span class="line">---StatementBase.rewriteExprs()</span><br></pre></td></tr></table></figure>
<p>最终我们在Analyzer.GlobalState的构造函数中，将各种重写规则加入到了Analyzer中，相关代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; Analyzer.java</span><br><span class="line">public GlobalState(StmtTableCache stmtTableCache, TQueryCtx queryCtx,</span><br><span class="line">    AuthorizationFactory authzFactory) &#123;</span><br><span class="line">  this.stmtTableCache &#x3D; stmtTableCache;</span><br><span class="line">  this.queryCtx &#x3D; queryCtx;</span><br><span class="line">  this.authzFactory &#x3D; authzFactory;</span><br><span class="line">  this.lineageGraph &#x3D; new ColumnLineageGraph();</span><br><span class="line">  List&lt;ExprRewriteRule&gt; rules &#x3D; new ArrayList&lt;&gt;();</span><br><span class="line">  &#x2F;&#x2F; BetweenPredicates must be rewritten to be executable. Other non-essential</span><br><span class="line">  &#x2F;&#x2F; expr rewrites can be disabled via a query option. When rewrites are enabled</span><br><span class="line">  &#x2F;&#x2F; BetweenPredicates should be rewritten first to help trigger other rules.</span><br><span class="line">  rules.add(BetweenToCompoundRule.INSTANCE);</span><br><span class="line">  &#x2F;&#x2F; Binary predicates must be rewritten to a canonical form for both Kudu predicate</span><br><span class="line">  &#x2F;&#x2F; pushdown and Parquet row group pruning based on min&#x2F;max statistics.</span><br><span class="line">  rules.add(NormalizeBinaryPredicatesRule.INSTANCE);</span><br><span class="line">  if (queryCtx.getClient_request().getQuery_options().enable_expr_rewrites) &#123;</span><br><span class="line">    rules.add(FoldConstantsRule.INSTANCE);</span><br><span class="line">    rules.add(NormalizeExprsRule.INSTANCE);</span><br><span class="line">    rules.add(ExtractCommonConjunctRule.INSTANCE);</span><br><span class="line">    &#x2F;&#x2F; Relies on FoldConstantsRule and NormalizeExprsRule.</span><br><span class="line">    rules.add(SimplifyConditionalsRule.INSTANCE);</span><br><span class="line">    rules.add(EqualityDisjunctsToInRule.INSTANCE);</span><br><span class="line">    rules.add(NormalizeCountStarRule.INSTANCE);</span><br><span class="line">    rules.add(SimplifyDistinctFromRule.INSTANCE);</span><br><span class="line">    rules.add(SimplifyCastStringToTimestamp.INSTANCE);</span><br><span class="line">  &#125;</span><br><span class="line">  exprRewriter_ &#x3D; new ExprRewriter(rules);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个构造函数里面添加了很多重写规则，这些规则最终都会被应用于SQL的重写中。Impala目前包含了很多重写规则，相关类图如下所示：<br><img src="https://img-blog.csdnimg.cn/20201229112220768.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="rewrite_rule"></p>
<p>所有的重写规则都实现了ExprRewriteRule这个接口，接口本身只包含一个方法apply，接收一个Expr和Analyzer，返回是一个修改之后的Expr。关于Expr，我们在上篇文章中也已经提到了过了，这里就不再展开描述。需要注意的是，Impala还提供了一个query option，叫ENABLE_EXPR_REWRITES，默认为true，会启用更多的重写规则，对于SQL的查询性能提升有很大的帮助。<br>通过上述代码可以看到，在构造GlobalState成员变量的时候，会将所有的重写规则放到一个数组当中，然后构造一个ExprRewriter类，这个类的作用就是：使用重写规则的数组，对指定的Expr进行重写操作。在完成对应的Analyzer构造和StatementBase的解析之后，会调用StatementBase的rewriteExprs方法，来对这个statement的所有Exprs进行重写，这里我们以SelectStmt为例（StatementBase本身是抽象类，并没有实现这个方法），来看一下是如何对Expr进行重写的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; SelectStmt.java</span><br><span class="line">public void rewriteExprs(ExprRewriter rewriter) throws AnalysisException &#123;</span><br><span class="line">  Preconditions.checkState(isAnalyzed());</span><br><span class="line">  selectList_.rewriteExprs(rewriter, analyzer_);</span><br><span class="line">  for (TableRef ref: fromClause_.getTableRefs()) ref.rewriteExprs(rewriter, analyzer_);</span><br><span class="line">  if (whereClause_ !&#x3D; null) &#123;</span><br><span class="line">    whereClause_ &#x3D; rewriter.rewrite(whereClause_, analyzer_);</span><br><span class="line">    &#x2F;&#x2F; Also rewrite exprs in the statements of subqueries.</span><br><span class="line">    List&lt;Subquery&gt; subqueryExprs &#x3D; new ArrayList&lt;&gt;();</span><br><span class="line">    whereClause_.collect(Subquery.class, subqueryExprs);</span><br><span class="line">    for (Subquery s: subqueryExprs) s.getStatement().rewriteExprs(rewriter);</span><br><span class="line">  &#125;</span><br><span class="line">  if (havingClause_ !&#x3D; null) &#123;</span><br><span class="line">    havingClause_ &#x3D; rewriteCheckOrdinalResult(rewriter, havingClause_);</span><br><span class="line">  &#125;</span><br><span class="line">  if (groupingExprs_ !&#x3D; null) &#123;</span><br><span class="line">    for (int i &#x3D; 0; i &lt; groupingExprs_.size(); ++i) &#123;</span><br><span class="line">      groupingExprs_.set(i, rewriteCheckOrdinalResult(</span><br><span class="line">          rewriter, groupingExprs_.get(i)));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  if (orderByElements_ !&#x3D; null) &#123;</span><br><span class="line">    for (OrderByElement orderByElem: orderByElements_) &#123;</span><br><span class="line">      orderByElem.setExpr(rewriteCheckOrdinalResult(rewriter, orderByElem.getExpr()));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从上述代码可以看到，这个函数分别对各个部分调用了rewriteExprs函数，传入rewrite成员r，进行重写，包括：selectList_、fromClause_、whereClause_等，这些正是我们在上篇文章中介绍到的SelectStmt的各个部分。对于selectList_，又调用了SelectList的rewriteExprs方法，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; SelectList.java</span><br><span class="line">public void rewriteExprs(ExprRewriter rewriter, Analyzer analyzer)</span><br><span class="line">    throws AnalysisException &#123;</span><br><span class="line">  for (SelectListItem item: items_) &#123;</span><br><span class="line">    if (item.isStar()) continue;</span><br><span class="line">    item.setExpr(rewriter.rewrite(item.getExpr(), analyzer));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最终，我们可以看到，通过循环处理，对每个SelectListItem中的Expr进行了重写，这个Expr就是通过SelectListItem的getExpr和setExpr进行获取和更新的，其他fromClause_、whereClause_等各个部分，也是类似的处理流程。<br>除此之外，在3.4.0版本中，Impala还提供了对解析之后的SQL进行展示，我们来看一个简单的例子，原始SQL如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select user as name,count(2) from iceberg_partitioned</span><br><span class="line">where id between 2 and 10 group by user;</span><br></pre></td></tr></table></figure>
<p>执行完成之后，就可以在Impala的web页面看到如下所示的SQL解析之后的输出：<br><img src="https://img-blog.csdnimg.cn/20201229112403805.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="sql_rewrite"><br>可以看到，解析之后的SQL经过了重写和隐式转换：</p>
<ul>
<li>count(2)被转换成了count(*)</li>
<li>between被转换成了&gt;=和&lt;=</li>
<li>常量2和10加上了CAST的操作</li>
</ul>
<p>输出的格式主要是通过如下的这个enum来控制的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">public enum ToSqlOptions &#123;</span><br><span class="line">  &#x2F;**</span><br><span class="line">   * The default way of displaying the original SQL query without rewrites.</span><br><span class="line">   *&#x2F;</span><br><span class="line">  DEFAULT(false, false),</span><br><span class="line"></span><br><span class="line">  &#x2F;**</span><br><span class="line">   * Show rewritten query if it exists</span><br><span class="line">   *&#x2F;</span><br><span class="line">  REWRITTEN(true, false),</span><br><span class="line"></span><br><span class="line">  &#x2F;**</span><br><span class="line">   * Show Implicit Casts.</span><br><span class="line">   * To see implicit casts we must also show rewrites as otherwise we see original SQL.</span><br><span class="line">   * This does have the consequence that the sql with implict casts may possibly fail</span><br><span class="line">   * to parse if resubmitted as, for example, EXISTS queries that are rewritten as</span><br><span class="line">   * semi-joins are not legal SQL.</span><br><span class="line">   *&#x2F;</span><br><span class="line">  SHOW_IMPLICIT_CASTS(true, true);</span><br><span class="line"></span><br><span class="line">  private boolean rewritten_;</span><br><span class="line"></span><br><span class="line">  private boolean implictCasts_;</span><br><span class="line">  &#x2F;&#x2F; 省略余下代码</span><br></pre></td></tr></table></figure>
<p>一共有三个选项：DEFAULT、REWRITTEN和SHOW_IMPLICIT_CASTS，上述截图中的结果，就是使用了SHOW_IMPLICIT_CASTS之后的格式化结果。输出的函数就是我们在上篇文章中提到的ParseNode中的toSql，这个函数有两个版本，不带参数的默认是使用ToSqlOptions.DEFAULT。对于我们的SQL示例，是一个SELECT语句，所以解析后的SQL格式化，最终是由SelectStmt.toSql(ToSqlOptions options)函数完成的，输入参数就是SHOW_IMPLICIT_CASTS。<br>到这里，关于Impala的SQL规则重写基本就介绍完了，后续有时间的话，会跟大家继续分享Impala的SQL解析的其他知识。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://skyyws.github.io/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8B%E9%87%8D%E5%86%99%EF%BC%88%E4%BA%8C%EF%BC%89/" data-id="cknk05owo0000hex5hx73azei" data-title="Impala 3.4 SQL查询之重写（二）" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Impala-SQL%E6%9F%A5%E8%AF%A2%E7%B3%BB%E5%88%97/" rel="tag">Impala SQL查询系列</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/olap/" rel="tag">olap</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Impala-3-4-SQL查询梳理（一）" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E6%A2%B3%E7%90%86%EF%BC%88%E4%B8%80%EF%BC%89/" class="article-date">
  <time class="dt-published" datetime="2021-04-16T06:21:01.000Z" itemprop="datePublished">2021-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E6%A2%B3%E7%90%86%EF%BC%88%E4%B8%80%EF%BC%89/">Impala 3.4 SQL查询之梳理（一）</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>当通过jdbc请求连接至Impalad节点之后，我们提交的SQL会通过BE的JNI调用FE的api进行解析，主要的调用栈如下所示：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"> query&#x2F;executeAndWait(impala-beeswax-server.cc)</span><br><span class="line">-Execute(impala-server.cc)</span><br><span class="line">--ExecuteInternal(impala-server.cc)</span><br><span class="line">---InitExecRequest(client-request-state.cc)</span><br><span class="line">----RunFrontendPlanner(query-driver.cc)</span><br><span class="line">-----GetExecRequest(frontend.cc)</span><br><span class="line">------JniFrontendcreateExecRequest()</span><br><span class="line">-------Frontend.createExecRequest()</span><br><span class="line">--------Frontend.getTExecRequest()</span><br><span class="line">---------Frontend.doCreateExecRequest()</span><br></pre></td></tr></table></figure><br> 在doCreateExecRequest方法中，会通过调用Parse.parse()来对SQL进行解析，解析完成之后，SQL就会变成对应的结构，如下所示：<br><img src="https://img-blog.csdnimg.cn/20201224113640624.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="Impala-stmt"><br>从图中我们可以看到，Impala所有的SQL最终都是继承于StatementBase，包括select、alter、create等。这里我们以简单的select查询为例，最终SQL转换之后会被解析成SelectStmt这个类，而这个类其中又包含SelectList、FromClause等部分。通过Parse.parse()的解析，我们将一条普通的SQL转成了一个Impala的类。目前，Impala在进行SQL解析的时候，采用的是一个开源的框架antlr，关于这个框架不是本文描述的重点，这里就不再展开。<br> 对于图中涉及到的一些接口和类，我们摘取了一部分代码中的注释，供大家参考。</p>
<ul>
<li> ParseNode: divide into two broad categories: statement-like nodes and expression nodes;</li>
<li>StmtNode: Base interface for statements and statement-like nodes such as clauses;</li>
<li>Expr: Root of the expr node hierarchy;</li>
<li>StatementBase: Base class for all Impala SQL statements;</li>
<li>QueryStmt: Abstract base class for any statement that returns results via a list of result expressions;</li>
</ul>
<p>在解析出了具体的StatementBase之后（上述例子中就是SelectStmt），Impala接着会构造对应的Analyer，相关的类如下所示：<br><img src="https://img-blog.csdnimg.cn/20201224113720808.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="Analyzer"><br>同样，我们截取部分代码中的注释来看一看：</p>
<ul>
<li>AnalysisContext: Wrapper class for parsing, analyzing and rewriting a SQL stmt;</li>
<li>Analyzer: Repository of analysis state for single select block;</li>
<li>GlobalState: State shared between all objects of an Analyzer tree.</li>
</ul>
<p>这里最重要的类就是Analyzer，包括了单个select查询块的所有解析之后的状态集合。我们继续以SelectStmt为例来看下生成Analyzer的接口调用流程：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> Frontend.doCreateExecRequest()</span><br><span class="line">-AnalysisContext.analyzeAndAuthorize()</span><br><span class="line">--AnalysisContext.analyze()</span><br><span class="line">---SelectStmt.analyze()</span><br><span class="line">----SelectStmt.SelectAnalyzer.analyze()</span><br></pre></td></tr></table></figure><br> 我们可以看到，主要就是调用各个StatementBase子类的analyze()，来实现对各个查询的解析。这里简单看一下SelectStmt的analyze方法，如下所示：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"> &#x2F;&#x2F; SelectStmt.analyze()</span><br><span class="line"> public void analyze(Analyzer analyzer) throws AnalysisException &#123;</span><br><span class="line">  if (isAnalyzed()) return;</span><br><span class="line">  super.analyze(analyzer);</span><br><span class="line">  new SelectAnalyzer(analyzer).analyze();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; SelectStmt.SelectAnalyzer.analyze()</span><br><span class="line">private void analyze() throws AnalysisException &#123;</span><br><span class="line">    &#x2F;&#x2F; Start out with table refs to establish aliases.</span><br><span class="line">    fromClause_.analyze(analyzer_);</span><br><span class="line"></span><br><span class="line">    analyzeSelectClause();</span><br><span class="line">    verifyResultExprs();</span><br><span class="line">    registerViewColumnPrivileges();</span><br><span class="line">    analyzeWhereClause();</span><br><span class="line">    createSortInfo(analyzer_);</span><br><span class="line">    ......</span><br></pre></td></tr></table></figure><br> 可以看到，SelectStmt的解析，主要都在其私有类SelectAnalyzer的analyze中进行处理了，这里包括了对于FromClause的处理、WhereClause的处理等操作。其他的SQL也是类似处理流程，每一个具体的SQL类都有对应的analyze方法。解析完成之后，Impala就会根据解析的结果来生成相应地执行计划：首先是生成一个单机的执行计划，接着会根据单机的执行计划来生成分布式的执行计划。关于执行计划的生成这块，我们会在后续的文章里面陆续提到，这里就不再展开描述。执行计划生成之后，Backend模块就会根据这些执行计划执行实际的扫描、聚合运算等操作，最终返回结果。<br>我们从第一幅图可以看到，ParseNode主要分为了两个部分：1）StmtNode，这个主要包括查询以及相应的clause实现；2）Expr，我们接下来就看一看这个Expr相应的各个子类都是什么样的，下面就是一个简单的关于UML的类图：<br><img src="https://img-blog.csdnimg.cn/20201224113747574.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="Impala-expr"><br>从上图可以看到，有非常多的类都继承了Expr，这里我们看几个比较常见的类：</p>
<ul>
<li>Predicate，这个类就是用来保存各种谓词条件的，包括：BetweenPredicate、BinaryPredicate等，我们在上述的SelectStmt中提到的whereClasue_最终就会转换成一个Predicate，根据不同的条件转换成相应的Predicate；</li>
<li>LiteralExpr，用来保存各种常量的值，例如布尔保存在BoolLiteral中，字符串保存在StringLiteral中等等，目前主要就包括图中的这其中；</li>
<li>FunctionCallExpr，各种函数调用，最终都会转换成这个对象，例如常见的count、sum等；</li>
<li>SlotRef，这个可以简单理解为列的描述，SQL中涉及到列都会被转换成一个SlotRef对象，保存着这个列的相关信息；</li>
<li>其他还有一些例如AnalyticExpr、CastExpr等这里就不再展开描述，感兴趣的同学可以自行查看相关的源码。</li>
</ul>
<p>下面我们就从一个具体的SQL出来，来简单看一下上面提到的各个对象是如何解析的，SQL如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select id,user,count(1) from table_name</span><br><span class="line">where id&gt;&#x3D;5 and id&lt;&#x3D;10</span><br><span class="line">group by id,user order by id desc;</span><br></pre></td></tr></table></figure>
<p>结合上面的几个类图，我们可以看看上述的SQL会被解析成什么样的：</p>
<ul>
<li>SelectList包含三个SelectListItem，分别是：id、user和count(1)，而这三个item各自包含的Expr分别是：SlotRef、SlotRef和FunctionCallExpr，而这个FunctionCallExpr本身又包含一个NumericLiteral，对应count(1)里面的1；</li>
<li>fromClause_主要包括了一个表的集合，这里只有一个成员，就是table_name；</li>
<li>whereClasue_这里转换成了一个CompoundPredicate的谓词，表示组合的谓词，操作符是AND。它本身又包含两个BinaryPredicate，表示包含两个操作数的谓词，分别对应id&gt;=5和id&lt;=10。以第一个为例，它的操作符是&gt;=，本身又包含两个child，分别是id对应的SlotRef以及10对应的NumericLiteral；</li>
<li>groupingExprs_是一系列的group by成员集合，这里主要就是包括两个SlotRef，分别对应id和user；</li>
<li>orderByElements_是从QueryStmt继承而来，成员是一个OrderByElement类，而这个OrderByElement内部也是包含了一个Expr，这里对应的仍旧是一个SlotRef，即id列；<br>到这里，我们基本对于上述示例中的SQL各个部分的解析都已经完成了。<br>本文比较浅显地讲述了Impala SQL解析中的两个部分：StmtmentBase和Expr，整个SQL解析的大部分成员对象，最终都会转换成这两个类或者其子类。关于Analyzer类，本身没有过多讲述，只是稍微提了一下，后续有机会再跟大家一起深入分享。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://skyyws.github.io/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E6%A2%B3%E7%90%86%EF%BC%88%E4%B8%80%EF%BC%89/" data-id="cknk05owu0001hex57wwlfh1e" data-title="Impala 3.4 SQL查询之梳理（一）" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Impala-SQL%E6%9F%A5%E8%AF%A2%E7%B3%BB%E5%88%97/" rel="tag">Impala SQL查询系列</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/olap/" rel="tag">olap</a></li></ul>

    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Impala-SQL%E6%9F%A5%E8%AF%A2%E7%B3%BB%E5%88%97/" rel="tag">Impala SQL查询系列</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Impala%E4%B9%8BHDFS-SCAN-NODE/" rel="tag">Impala之HDFS_SCAN_NODE</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/olap/" rel="tag">olap</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Impala-SQL%E6%9F%A5%E8%AF%A2%E7%B3%BB%E5%88%97/" style="font-size: 15px;">Impala SQL查询系列</a> <a href="/tags/Impala%E4%B9%8BHDFS-SCAN-NODE/" style="font-size: 10px;">Impala之HDFS_SCAN_NODE</a> <a href="/tags/impala/" style="font-size: 20px;">impala</a> <a href="/tags/olap/" style="font-size: 20px;">olap</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8BScanRange%E8%AF%A6%E8%A7%A3%EF%BC%88%E4%B8%89%EF%BC%89/">Impala 3.4 SQL查询之ScanRange详解（三）</a>
          </li>
        
          <li>
            <a href="/2021/04/16/Impala-HDFS-SCAN-NODE%E4%B9%8BAverageHdfsReadThreadConcurrency/">Impala HDFS_SCAN_NODE之AverageHdfsReadThreadConcurrency</a>
          </li>
        
          <li>
            <a href="/2021/04/16/Impala-HDFS-SCAN-NODE%E4%B9%8BIO-threads%E6%A8%A1%E5%9E%8B/">Impala HDFS_SCAN_NODE之IO threads模型</a>
          </li>
        
          <li>
            <a href="/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8B%E9%87%8D%E5%86%99%EF%BC%88%E4%BA%8C%EF%BC%89/">Impala 3.4 SQL查询之重写（二）</a>
          </li>
        
          <li>
            <a href="/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E6%A2%B3%E7%90%86%EF%BC%88%E4%B8%80%EF%BC%89/">Impala 3.4 SQL查询之梳理（一）</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 汪胜<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>