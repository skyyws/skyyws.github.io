<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>skyyws的藏宝阁</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="skyyws的藏宝阁">
<meta property="og:url" content="https://skyyws.github.io/index.html">
<meta property="og:site_name" content="skyyws的藏宝阁">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="汪胜">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="skyyws的藏宝阁" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">skyyws的藏宝阁</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://skyyws.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-Impala-2-12-0与3-4-0版本的compute-stats兼容问题" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/16/Impala-2-12-0%E4%B8%8E3-4-0%E7%89%88%E6%9C%AC%E7%9A%84compute-stats%E5%85%BC%E5%AE%B9%E9%97%AE%E9%A2%98/" class="article-date">
  <time class="dt-published" datetime="2021-04-16T12:17:25.000Z" itemprop="datePublished">2021-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/16/Impala-2-12-0%E4%B8%8E3-4-0%E7%89%88%E6%9C%AC%E7%9A%84compute-stats%E5%85%BC%E5%AE%B9%E9%97%AE%E9%A2%98/">Impala 2.12.0与3.4.0版本的compute stats兼容问题</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>对于Impala来说，compute [incremental] stats [partition_spec]是我们经常会使用到的语句，这个语句的功能就是对表，执行统计信息计算。Impala在进行SQL解析的时候，就可以利用这些统计信息进行更好地优化，生成更高效地执行计划。但是，最近我们在将集群从2.12.0升级到3.4.0版本的时候，遇到了一些compute stats相关的问题。<br>本文在第一章和第三章分别描述了问题以及重现的步骤，第二章是详细的代码探究。如果不感兴趣的话，可以直接略过。</p>
<h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>当我们在3.4.0版本，对表的某个具体分区执行compute incremental stats table_name [partition_spec]时，发现执行过程中，会出现TableLoadingException的异常，如下所示：<br><img src="https://img-blog.csdnimg.cn/20201223153428925.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="exception"><br>这个exception主要是由于列统计信息不符合约束导致的，这里就是由于numNulls_的约束检查失败导致的。我们可以在相关的类中找到如下代码，该方法在2.12.0中是不存在的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 3.4.0  ColumnStats.java</span><br><span class="line">public void validate(Type colType) &#123;</span><br><span class="line">  &#x2F;&#x2F; avgSize_ and avgSerializedSize_ must be set together.</span><br><span class="line">  Preconditions.checkState(avgSize_ &gt;&#x3D; 0 &#x3D;&#x3D; avgSerializedSize_ &gt;&#x3D; 0, this);</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; Values must be either valid or -1.</span><br><span class="line">  Preconditions.checkState(avgSize_ &#x3D;&#x3D; -1 || avgSize_ &gt;&#x3D; 0, this);</span><br><span class="line">  Preconditions.checkState(avgSerializedSize_ &#x3D;&#x3D; -1 || avgSerializedSize_ &gt;&#x3D; 0, this);</span><br><span class="line">  Preconditions.checkState(maxSize_ &#x3D;&#x3D; -1 || maxSize_ &gt;&#x3D; 0, this);</span><br><span class="line">  Preconditions.checkState(numDistinctValues_ &#x3D;&#x3D; -1 || numDistinctValues_ &gt;&#x3D; 0, this);</span><br><span class="line">  Preconditions.checkState(numNulls_ &#x3D;&#x3D; -1 || numNulls_ &gt;&#x3D; 0, this);</span><br><span class="line">  if (colType !&#x3D; null &amp;&amp; colType.isFixedLengthType()) &#123;</span><br><span class="line">    Preconditions.checkState(avgSize_ &#x3D;&#x3D; colType.getSlotSize(), this);</span><br><span class="line">    Preconditions.checkState(avgSerializedSize_ &#x3D;&#x3D; colType.getSlotSize(), this);</span><br><span class="line">    Preconditions.checkState(maxSize_ &#x3D;&#x3D; colType.getSlotSize(), this);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们观察Impala的页面发现，compute stats的两条相关SQL执行是成功的，如下所示：<br><img src="https://img-blog.csdnimg.cn/20201223153455744.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="web"><br>这就说明，新版本部署启动之后，第一次加载这个表是正常的，并且compute stats相关的两条SQL都能正常执行成功。因此，问题应该是出在了计算完成之后，更新到metastore中导致的，我们通过查看元数据库对应的表发现，确实numNulls_对应的值是-6（这些统计信息位于元数据库的TAB_COL_STATS表，其中numNulls_对应的列是NUM_NULLS）。</p>
<h4 id="深入研究"><a href="#深入研究" class="headerlink" title="深入研究"></a>深入研究</h4><h5 id="两个版本统计信息对比"><a href="#两个版本统计信息对比" class="headerlink" title="两个版本统计信息对比"></a>两个版本统计信息对比</h5><p>当我们对表执行了compute stats之后，我们可以通过show column stats table_name来查看表的列统计信息，如下所示：<br><img src="https://img-blog.csdnimg.cn/20201223153514771.png" alt="初始状态"><br>以上是我们表的初始列统计信息状态，当我们执行了compute stats table_name之后，2.12.0版本的结果如下所示：<br><img src="https://img-blog.csdnimg.cn/20201223153534447.png" alt="在这里插入图片描述"><br>而3.4.0版本的结果如下所示：<br><img src="https://img-blog.csdnimg.cn/20201223153553993.png" alt="3.4.0版本"><br>通过上面两幅图对比，我们可以发现，“#Nulls”这一列在两个版本中的值是不一样的。2.12.0版本对于这一列，即使执行了compute stats之后，仍然是-1（除去分区列），而3.4.0版本则是实际的数值，是大于等于0的。这里的“#Nulls”列对应的就是异常日志中的“numNulls”。<br>值得一提的是，对于每一个分区（Hdfs表对应的是THdfsPartition结构体，这个thrift结构体包括了单个分区详细信息，如果存在多层分区的话，那么该结构体包含的是到最里层的分区，例如day=20200101/type=xxx/id=xxx这种），其中每一个THdfsPartition都有一个has_incremental_stats变量，这个变量默认是false（即没有执行compute stats），当我们执行相关SQL的时候，情况分别如下所示：</p>
<ul>
<li>执行compute stats table_name，所有分区对应的has_incremental_stats参数变为false；</li>
<li>执行compute incremental stats table_name，所有分区对应的has_incremental_stats参数变为true；</li>
<li>执行compute incremental stats table_name partition(day=’2020-12-01’)，这个分区对应的has_incremental_stats参数变为true，其他分区的仍然为false。</li>
</ul>
<p>我们这里讨论的前提是：这个表是默认没有进行任何的compute stats操作的，上述情况对于2.12.0和3.4.0都是同样的情况。因此，当has_incremental_stats为true，就表示对应的某个分区包含了增量的历史统计信息，而初始状态，或者compute stats table_name是不算做增量统计信息计算的。我们后续的分析，都是基于增量的统计信息计算。</p>
<h5 id="初始状态分析"><a href="#初始状态分析" class="headerlink" title="初始状态分析"></a>初始状态分析</h5><p>为了研究，到底是哪里导致的这个“#Nulls”的值小于-1，我们接下来跟着代码一步一步看下去（这里以3.4.0的代码为例）。<br>首先，假设表为初始状态，当我们第一次访问，加载表的时候，此时表没有任何统计信息，加载操作由catalogd执行。Catalogd会对表信息、分区信息等进行初始化，主要就是从metastore中进行加载，然后转换成Impala对应的各种类和结构体。我们这里主要关注HDFS表的加载，下面我们简单看下相关的函数调用栈：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">HdfsTable.load()</span><br><span class="line">-HdfsTable.loadAllPartitions()</span><br><span class="line">--HdfsTable.createPartition() 分区表会循环调用这个函数</span><br><span class="line">---HdfsPartition.ctor() public</span><br><span class="line">----HdfsPartition.ctor() private</span><br><span class="line">-----HdfsPartition.extractAndCompressPartStats()</span><br><span class="line">------PartitionStatsUtil.partStatsBytesFromParameters()</span><br><span class="line">------HdfsPartition.setPartitionStatsBytes() 使用上面函数的返回结果作为输入参数</span><br></pre></td></tr></table></figure>
<p>可以看到，在加载表分区的信息时，会调用partStatsBytesFromParameters这个函数，我们将相关的代码粘贴出来：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">  &#x2F;&#x2F; PartitionStatsUtil.java</span><br><span class="line">  public static final String INCREMENTAL_STATS_NUM_CHUNKS &#x3D;</span><br><span class="line">    &quot;impala_intermediate_stats_num_chunks&quot;;</span><br><span class="line"> &#x2F;&#x2F; 省略后续代码</span><br><span class="line">&#x2F;**</span><br><span class="line"> * Reconstructs the intermediate stats from chunks and returns the corresponding</span><br><span class="line"> * byte array. The output byte array is deflate-compressed. Sets hasIncrStats to</span><br><span class="line"> * &#39;true&#39; if the partition stats contain intermediate col stats.</span><br><span class="line"> *&#x2F;</span><br><span class="line">public static byte[] partStatsBytesFromParameters(</span><br><span class="line">    Map&lt;String, String&gt; hmsParameters, Reference&lt;Boolean&gt; hasIncrStats) throws</span><br><span class="line">    ImpalaException &#123;</span><br><span class="line">  if (hmsParameters &#x3D;&#x3D; null) return null;</span><br><span class="line">  String numChunksStr &#x3D; hmsParameters.get(INCREMENTAL_STATS_NUM_CHUNKS);</span><br><span class="line">  if (numChunksStr &#x3D;&#x3D; null) return null;</span><br><span class="line">  int numChunks &#x3D; Integer.parseInt(numChunksStr);</span><br><span class="line">  if (numChunks &#x3D;&#x3D; 0) return null;</span><br><span class="line"> &#x2F;&#x2F; 省略后续代码</span><br></pre></td></tr></table></figure>
<p>我们通过上述函数代码可以看到：当分区的参数列表中（分区的参数列表，可以直接从metastore中加载），没有INCREMENTAL_STATS_NUM_CHUNKS参数时，整个函数会返回null。这里先提一下，当函数初始没有计算统计信息的时候，就不会有这个参数，后续我们还会再提到这个参数。紧接着就会使用上述函数的返回结果，来执行setPartitionStatsBytes这个函数。这里又涉及到了两个成员变量：partitionStats_和hasIncrementalStats_。这里我们暂且不详细探究其含义，我们只需要知道，在初始状态下，对于具体的某个分区而言，partitionStats_为null，而hasIncrementalStats_为false。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; Binary representation of the TPartitionStats for this partition. Populated</span><br><span class="line">&#x2F;&#x2F; when the partition is loaded and updated using setPartitionStatsBytes().</span><br><span class="line">private byte[] partitionStats_;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; True if partitionStats_ has intermediate_col_stats populated.</span><br><span class="line">private boolean hasIncrementalStats_ ;</span><br><span class="line">&#x2F;&#x2F; 省略后续代码</span><br><span class="line">public void setPartitionStatsBytes(byte[] partitionStats, boolean hasIncrStats) &#123;</span><br><span class="line">  if (hasIncrStats) Preconditions.checkNotNull(partitionStats);</span><br><span class="line">  partitionStats_ &#x3D; partitionStats;</span><br><span class="line">  hasIncrementalStats_ &#x3D; hasIncrStats;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于初始的状态而言，2.12.0和3.4.0版本，虽然在代码处理逻辑上有所不同，但是总体而言，这些主要的结构和成员变量都是相差不大的。</p>
<h5 id="增量计算后的状态变化"><a href="#增量计算后的状态变化" class="headerlink" title="增量计算后的状态变化"></a>增量计算后的状态变化</h5><p>上面我们已经了解一些，初始状态下的分区相关统计信息。现在来看一下，当我们执行了compute [incremental] stats [partition_spec]之后，状态会发生哪些变化，而这也跟我们最初的问题有关系。<br>当我们提交了SQL之后，Impala会自动提交两条子SQL来进行相应信息的获取，相关的SQL我们可以在第一章的第二幅图中看到，3.4.0和2.12.0版本的两个SQL略有不同。从截图中我们可以看到，这两条SQL的执行是没有问题，因此我们当前不关注这两条SQL的生成以及执行，着重于后续的统计信息更新部分。<br>我们假设当前表处于初始状态，此时提交执行compute incremental stats table_name partition(day=’2020-12-01’)这种SQL。我们来看一下状态是如何更新的：<br>首先，当SQL提交到Impalad的时候，会进行一系列的计算操作，主要就是执行上述的两个子查询。计算完成之后，会生成相应的变量来保存信息，然后将变量传到catalogd进程进行元数据的更新。这里我们先看下catalogd的相关处理流程，主要的api调用如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ExecDdl(catalog-server.cc)</span><br><span class="line">-ExecDdl(catalog.cc)</span><br><span class="line">--execDdl(JniCatalog.java)</span><br><span class="line">---CatalogOpExecutor.execDdlRequest</span><br><span class="line">----CatalogOpExecutor.alterTable</span><br></pre></td></tr></table></figure>
<p>可以看到，catalogd首先是在c++端通过JNI调用了Java的api，最终执行了一个alterTable的函数，来更新表的元数据信息（这里主要是统计信息），这里我们涉及到了一些thrift结构体信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">CatalogService.TDdlExecRequest</span><br><span class="line">-alter_table_params: JniCatalog.TAlterTableParams</span><br><span class="line">--update_stats_params: JniCatalog.TAlterTableUpdateStatsParams</span><br><span class="line">---partition_stats: map&lt;list&lt;string&gt;, CatalogObjects.TPartitionStats&gt;</span><br><span class="line">----CatalogObjects.TPartitionStats</span><br><span class="line">-----intermediate_col_stats: map&lt;string, TIntermediateColumnStats&gt;</span><br><span class="line">------CatalogObjects.TIntermediateColumnStats</span><br><span class="line">---column_stats: map&lt;string, CatalogObjects.TColumnStats&gt;</span><br><span class="line">----CatalogObjects.TColumnStats</span><br></pre></td></tr></table></figure>
<p>我们将一些相关的结构体包含关系列了出来。从上面的包含关系可以看到：本次计算涉及到的分区都会保存在partition_stats这个数组中，数组的每一个成员都是一个TPartitionStats结构体，代表一个分区的信息。这个结构体主要包括两个成员：TTableStats和一个map，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; Per-partition statistics</span><br><span class="line">struct TPartitionStats &#123;</span><br><span class="line">  &#x2F;&#x2F; so would interfere with the non-incremental stats path</span><br><span class="line">  1: required TTableStats stats</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; Intermediate state for incremental statistics, one entry per column name.</span><br><span class="line">  2: optional map&lt;string, TIntermediateColumnStats&gt; intermediate_col_stats</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>每个TPartitionStats都包含一个map，叫intermediate_col_stats。这个map的key表示列名，而value就是TIntermediateColumnStats，表的每一列都会对应一条KV记录。其中TIntermediateColumnStats的结构体如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; Intermediate state for the computation of per-column stats. Impala can aggregate these</span><br><span class="line">&#x2F;&#x2F; structures together to produce final stats for a column.</span><br><span class="line">struct TIntermediateColumnStats &#123;</span><br><span class="line">  &#x2F;&#x2F; One byte for each bucket of the NDV HLL computation</span><br><span class="line">  1: optional binary intermediate_ndv</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; If true, intermediate_ndv is RLE-compressed</span><br><span class="line">  2: optional bool is_ndv_encoded</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; Number of nulls seen so far (or -1 if nulls are not counted)</span><br><span class="line">  3: optional i64 num_nulls</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; The maximum width, in bytes, of the column</span><br><span class="line">  4: optional i32 max_width</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; The average width (in bytes) of the column</span><br><span class="line">  5: optional double avg_width</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; The number of rows counted, needed to compute NDVs from intermediate_ndv</span><br><span class="line">  6: optional i64 num_rows</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果执行了增量的统计信息计算，那么partition_stats这个变量就会包含当前正在进行计算的各个分区信息，而每个分区又会包含各自的intermediate_col_stats成员，其中有相应的列的统计信息。这里需要注意的是，3.4.0版本和2.12.0版本是不一样的：</p>
<ul>
<li>在两个版本中，初始状态下，列的num_nulls都是-1；</li>
<li>在3.4.0版本，如果执行了统计信息计算，num_nulls是一个大于等于0的值；</li>
<li>在2.12.0版本，如果执行了统计信息计算，num_nulls仍然是-1；</li>
</ul>
<p>但是，如果我们执行的是compute stats，而不是增量的话，那么每个分区的intermediate_col_stats是空的（注意，partition_stats不为空，其包含的stats也不为空，只是intermediate_col_stats这个变量为空）。这块的处理主要是在BE端进行的，只有当执行增量统计信息计算的时候，才会将分区的列统计信息存入intermediate_col_stats中，相关代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; catalog-op-executor.cc ExecComputeStats()</span><br><span class="line">&#x2F;&#x2F; 其中FinalizePartitionedColumnStats方法就是用来构造intermediate_col_stats的相关信息</span><br><span class="line">&#x2F;&#x2F; col_stats_schema and col_stats_data will be empty if there was no column stats query.</span><br><span class="line">if (!col_stats_schema.columns.empty()) &#123;</span><br><span class="line">  if (compute_stats_params.is_incremental) &#123;</span><br><span class="line">    RuntimeProfile::Counter* incremental_finalize_timer &#x3D;</span><br><span class="line">        ADD_TIMER(profile_, &quot;FinalizeIncrementalStatsTimer&quot;);</span><br><span class="line">    SCOPED_TIMER(incremental_finalize_timer);</span><br><span class="line">    FinalizePartitionedColumnStats(col_stats_schema,</span><br><span class="line">        compute_stats_params.existing_part_stats,</span><br><span class="line">        compute_stats_params.expected_partitions,</span><br><span class="line">        col_stats_data, compute_stats_params.num_partition_cols, &amp;update_stats_params);</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    SetColumnStats(col_stats_schema, col_stats_data, &amp;update_stats_params);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里我们可以看到，只有执行增量统计信息计算的时候（is_incremental为true），FinalizePartitionedColumnStats函数才会被调用。我们接着上面的catalogd处理流程继续往下看：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">CatalogOpExecutor.alterTable</span><br><span class="line">-CatalogOpExecutor.alterTableUpdateStats</span><br><span class="line">--CatalogOpExecutor.alterTableUpdateStatsInner</span><br><span class="line">---CatalogOpExecutor.updatePartitionStats</span><br><span class="line">----PartitionStatsUtil.partStatsToPartition</span><br><span class="line">-----HdfsPartition.setPartitionStatsBytes</span><br><span class="line">---CatalogOpExecutor.bulkAlterPartitions</span><br><span class="line">----HdfsPartition.toHmsPartition</span><br><span class="line">-----PartitionStatsUtil.partStatsToParams</span><br></pre></td></tr></table></figure>
<p>统计信息计算的结果，最终通过catalogd对表的分区进行了元数据更新，上述updatePartitionStats函数调用后两步，刚好与我们第一节中，提到的HDFS表加载形成了呼应，我们提到的INCREMENTAL_STATS_NUM_CHUNKS参数也会在partStatsToParams函数中进行设置。之后如果表再重新加载元数据的话，partStatsBytesFromParameters就不会返回空了。<br>除此之外，我们之前提到的partitionStats_和hasIncrementalStats_，最终在这里也进行了设置。我们将本节中涉及到的partition_stats数组，通过循环处理，将数组中的成员TPartitionStats进行压缩，最终保存到了HdfsPartition的partitionStats_成员变量中；hasIncrementalStats_保存是一个布尔值：TPartitionStats中的intermediate_col_stats成员是否为空，就是我们是否对该分区执行了增量统计信息计算（上面的分析已经提到过，只有执行增量统计信息计算的时候，intermediate_col_stats才不会为空）。这里我们对几种情况进行归纳：<br>| 状态 | INCREMENTAL_STATS_NUM_CHUNKS | partition_stats | intermediate_col_stats | partitionStats_ | hasIncrementalStats_ |<br>| — | — | — | — | — | — |<br>| 初始状态 | 不包括 | 空 | 空 | 空 | false |<br>| compute stats | 包括 | 不为空 | 空 | 不为空 |false  |<br>| 增量compute stats | 包括 | 不为空 |不为空  | 不为空 | true |<br>这里有几个地方，我们需要注意一下：</p>
<ul>
<li>partition_stats包含了本次操作涉及到的分区信息集合，而partitionStats_和hasIncrementalStats_是针对单个分区的信息；</li>
<li>partitionStats_是由partition_stats中的单个成员，也就是TPartitionStats，经过处理之后得到的；</li>
<li>intermediate_col_stats是TPartitionStats的一个成员，所以它是否为空，不会影响TPartitionStats，继而也不会影响partitionStats_；</li>
</ul>
<p>总结一下，当我们执行compute incremental stats [partition_spec]的时候，会在Impalad的BE端根据SQL解析和计算的结果，构造一个TDdlExecRequest变量，并且传到catalogd端，catalogd通过JNI调用Java的api对表的元数据信息进行更新。之后如果再加载表的元数据时，就能获取到这些已经计算的增量统计信息。</p>
<h5 id="错误产生分析"><a href="#错误产生分析" class="headerlink" title="错误产生分析"></a>错误产生分析</h5><p>上一节提到，当我们执行了compute incremental stats [partition_spec]的时候，表就会包含一些增量的统计信息，例如partitionStats_。因为我们最开始是在2.12.0版本每天执行了增量的统计信息计算，当我们升级到3.4.0版本之后，HDFS表被加载起来之后，就会包含相关的历史增量统计信息。此时，当我们在3.4.0版本再次执行增量统计信息计算的时候，就会出现了第一章中的问题。接下来就结合代码来看一下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">executeAndWait(impala-beeswax-server.cc)</span><br><span class="line">-Execute(impala-server.cc)</span><br><span class="line">--ExecuteInternal(impala-server.cc)</span><br><span class="line">---Exec(client-request-stats.cc)</span><br><span class="line">----ExecDdlRequest(client-request-stats.cc)</span><br><span class="line">-----ExecAsync(child-query.cc)</span><br><span class="line">------ExecChildQueries(child-query.cc)</span><br><span class="line">-------ExecAndFetch(child-query.cc)</span><br><span class="line">-Wait(client-request-state.cc)</span><br><span class="line">--WaitInternal(client-request-state.cc)</span><br><span class="line">---UpdateTableAndColumnStats(client-request-state.cc)</span><br><span class="line">----ExecComputeStats(catalog-op-executor.cc)</span><br><span class="line">-----SetTableStats(catalog-op-executor.cc)</span><br><span class="line">-----FinalizePartitionedColumnStats(incr-stats-util.cc)</span><br><span class="line">------Update(incr-stats-util.cc)</span><br></pre></td></tr></table></figure>
<p>上述的代码调用都是属于BE模块的，这里主要分为两个分支流程：1）Execute函数，主要就是对两个子查询就行计算，并且保存相应地结果，这里我们不展开；2）Wait函数，这个后续的相关操作就是对统计信息的结构体进行更新。从上一节的代码中我们可以看到，在ExecComputeStats函数中，对FinalizePartitionedColumnStats进行了调用，其中涉及到了existing_part_stats这个成员变量。我们来看下相关的结构体：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Frontend.TExecRequest</span><br><span class="line">-catalog_op_request: Frontend.TCatalogOpRequest</span><br><span class="line">--ddl_params: CatalogService.TDdlExecRequest</span><br><span class="line">---compute_stats_params: JniCatalog.TComputeStatsParams</span><br><span class="line">----existing_part_stats: list&lt;CatalogObjects.TPartitionStats&gt;</span><br><span class="line">-----CatalogObjects.TPartitionStats</span><br><span class="line">------intermediate_col_stats: map&lt;string, TIntermediateColumnStats&gt;</span><br><span class="line">-------CatalogObjects.TIntermediateColumnStats</span><br></pre></td></tr></table></figure>
<p>从这里，我们就可以很明显的看出来，这个existing_part_stats变量，与我们上一节中提及到的partition_stats成员，其实是一样的内容。我们这里来看一下partition_stats是如何转换为existing_part_stats的：<br>首先，通过上一节的分析我们可以知道，如果某个分区进行了增量的统计信息计算，那么该分区包含的partitionStats_就不为空，并且hasIncrementalStats_为true（这两个成员变量都位于HdfsPartition类中）。<br>其次，Impala在进行SQL解析的时候，compute [incremental] stats [partiiton_spec]最终都会被解析为一个ComputeStatsStmt类，而这个类中就有一个变量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; The list of valid partition statistics that can be used in an incremental computation</span><br><span class="line">&#x2F;&#x2F; without themselves being recomputed. Populated in analyze().</span><br><span class="line">private final List&lt;TPartitionStats&gt; validPartStats_ &#x3D; new ArrayList&lt;&gt;();</span><br></pre></td></tr></table></figure>
<p>validPartStats_这个变量就是在解析的过程中，根据表的元数据信息（这里就是每个分区的partitionStats_），将partition_stats解析出来，并保存下来，相关的解析流程位于ComputeStatsStmt.analyze()。最后，再构造TComputeStatsParams，通过thrift传到BE端，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; ComputeStatsStmt.toThrift()</span><br><span class="line">public TComputeStatsParams toThrift() &#123;</span><br><span class="line">  TComputeStatsParams params &#x3D; new TComputeStatsParams();</span><br><span class="line">  params.setTable_name(new TTableName(table_.getDb().getName(), table_.getName()));</span><br><span class="line">  params.setTbl_stats_query(tableStatsQueryStr_);</span><br><span class="line">  if (columnStatsQueryStr_ !&#x3D; null) &#123;</span><br><span class="line">    params.setCol_stats_query(columnStatsQueryStr_);</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    params.setCol_stats_queryIsSet(false);</span><br><span class="line">  &#125;</span><br><span class="line">  params.setIs_incremental(isIncremental_);</span><br><span class="line">  params.setExisting_part_stats(validPartStats_);</span><br><span class="line">  params.setExpect_all_partitions(expectAllPartitions_);</span><br><span class="line">  &#x2F;&#x2F; 省略后续代码</span><br></pre></td></tr></table></figure>
<p>最终我们就在BE端获取到了existing_part_stats。也就是说，只有分区执行过增量的统计信息计算，existing_part_stats才不为空。最终在FinalizePartitionedColumnStats函数中，对existing_part_stats进行循环处理，调用了Update函数。我们来看下最后的Update函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; Updates all aggregate statistics with a new set of measurements.</span><br><span class="line">void Update(const string&amp; ndv, int64_t num_new_rows, double new_avg_width,</span><br><span class="line">    int32_t max_new_width, int64_t num_new_nulls) &#123;</span><br><span class="line">  DCHECK_EQ(intermediate_ndv.size(), ndv.size()) &lt;&lt; &quot;Incompatible intermediate NDVs&quot;;</span><br><span class="line">  DCHECK_GE(num_new_rows, 0);</span><br><span class="line">  DCHECK_GE(max_new_width, 0);</span><br><span class="line">  DCHECK_GE(new_avg_width, 0);</span><br><span class="line">  DCHECK_GE(num_new_nulls, 0);</span><br><span class="line">  for (int j &#x3D; 0; j &lt; ndv.size(); ++j) &#123;</span><br><span class="line">    intermediate_ndv[j] &#x3D; ::max(intermediate_ndv[j], ndv[j]);</span><br><span class="line">  &#125;</span><br><span class="line">  num_nulls +&#x3D; num_new_nulls;</span><br><span class="line">  max_width &#x3D; ::max(max_width, max_new_width);</span><br><span class="line">  avg_width +&#x3D; (new_avg_width * num_new_rows);</span><br><span class="line">  num_rows +&#x3D; num_new_rows;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们可以很明显的看到，这个函数里面都是对统计信息的更新，而其中就有num_nulls的处理。到这里，这个问题产生的原因基本就已经明了：我们通过2.12.0版本执行了compute incremental stats [partition_spec]，这些分区对应的列统计信息中，Nulls都是-1。当我们在3.4.0版本再次执行compute incremental stats [partition_spec]，会对之前的增量分区统计信息进行汇总（对于Nulls，是多个-1相加，最终结果小于-1），并写入到metastore中。当catalogd再次触发表的元数据加载时，由于Nulls的约束检查失败，导致了表的加载失败。<br>需要注意的是，当我们使用debug模式进行编译、调试的话，那么当执行到DCHECK_GE(num_new_nulls, 0)这一行代码的时候，Impalad会直接挂掉，只有使用release进行编译，才会发生最上面提到的异常。</p>
<h4 id="复现步骤"><a href="#复现步骤" class="headerlink" title="复现步骤"></a>复现步骤</h4><p>这里我们使用一个测试表进行测试，在2.12.0版本执行如下SQL：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE stats_test (id INT, name STRING)</span><br><span class="line">PARTITIONED BY (day STRING)</span><br><span class="line">STORED AS PARQUET;</span><br><span class="line">insert into stats_test partition(day&#x3D;&#39;2020-01-01&#39;) values(1,&#39;Jack&#39;);</span><br><span class="line">insert into stats_test partition(day&#x3D;&#39;2020-01-02&#39;) values(1,&#39;Jack&#39;);</span><br><span class="line">compute incremental stats stats_test;</span><br></pre></td></tr></table></figure>
<p>启动3.4.0版本之后，再执行如下的SQL：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">compute incremental stats stats_test partition(day&#x3D;&#39;2020-01-01&#39;);</span><br></pre></td></tr></table></figure>
<p>要触发这个错误，需要保证除当前待计算的分区之外，还有其他分区有增量的历史统计信息（如果我们在2.12.0中只对2020-12-02分区进行增量统计信息计算，3.4.0执行同样的SQL仍然会重现这个错误）。<br>目前的解决方法有两种：</p>
<ul>
<li>对于已经计算过统计信息的表，执行<strong>drop stats table_name</strong>，去掉已有的统计信息，然后再重新计算；</li>
<li>将社区<a target="_blank" rel="noopener" href="https://issues.apache.org/jira/browse/IMPALA-9699">IMPALA-9699</a>这个patch backport到较低的版本上来。<h4 id="后续补充"><a href="#后续补充" class="headerlink" title="后续补充"></a>后续补充</h4>后续我们发现，社区也已经有了类似的JIRA：<a target="_blank" rel="noopener" href="https://issues.apache.org/jira/browse/IMPALA-10230">IMPALA-10230</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://skyyws.github.io/2021/04/16/Impala-2-12-0%E4%B8%8E3-4-0%E7%89%88%E6%9C%AC%E7%9A%84compute-stats%E5%85%BC%E5%AE%B9%E9%97%AE%E9%A2%98/" data-id="cknka1qkk0000wsx5hwgl5a9k" data-title="Impala 2.12.0与3.4.0版本的compute stats兼容问题" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/olap/" rel="tag">olap</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/" rel="tag">问题排查</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Impala-cast-timestamp导致相同SQL查询不一致问题排查" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/16/Impala-cast-timestamp%E5%AF%BC%E8%87%B4%E7%9B%B8%E5%90%8CSQL%E6%9F%A5%E8%AF%A2%E4%B8%8D%E4%B8%80%E8%87%B4%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/" class="article-date">
  <time class="dt-published" datetime="2021-04-16T12:14:42.000Z" itemprop="datePublished">2021-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/16/Impala-cast-timestamp%E5%AF%BC%E8%87%B4%E7%9B%B8%E5%90%8CSQL%E6%9F%A5%E8%AF%A2%E4%B8%8D%E4%B8%80%E8%87%B4%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/">Impala cast timestamp导致相同SQL查询不一致问题排查</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>最近，线上业务在使用Impala进行查询的时候，遇到这种问题：同一个SQL执行，有时候提示AnalysisException，有时候执行正常，错误信息如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">org.apache.impala.common.AnalysisException: xxxxxxxxx org.apache.impala.analysis.SelectStmt$SelectAnalyzer.verifyAggregation(SelectStmt.java:832)</span><br><span class="line">	at org.apache.impala.analysis.SelectStmt$SelectAnalyzer.analyze(SelectStmt.java:233)</span><br><span class="line">	at org.apache.impala.analysis.SelectStmt$SelectAnalyzer.access$100(SelectStmt.java:199)</span><br><span class="line">	at org.apache.impala.analysis.SelectStmt.analyze(SelectStmt.java:192)</span><br><span class="line">	at org.apache.impala.analysis.AnalysisContext.analyze(AnalysisContext.java:518)</span><br><span class="line">	at org.apache.impala.analysis.AnalysisContext.analyzeAndAuthorize(AnalysisContext.java:426)</span><br></pre></td></tr></table></figure>
<p>我们在测试环境构造了测试表和SQL，如下所示，目前在2.12.0和3.4.0版本都碰到了同样的问题，但是4.0的开发环境目前没有出现过：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">create table test_table(dt STRING) partitioned by(day STRING) STORED AS PARQUET;</span><br><span class="line"></span><br><span class="line">SELECT</span><br><span class="line">	(CASE WHEN (DAYS_ADD(CAST(CAST(TO_DATE(TO_TIMESTAMP(&#96;t1&#96;.&#96;dt&#96;, &#39;yyyy-MM-dd&#39;)) AS TIMESTAMP) AS TIMESTAMP), 7) </span><br><span class="line">		&gt; CAST(&#39;2021-01-26&#39; AS TIMESTAMP))</span><br><span class="line">		THEN 0 ELSE 1 END) &#96;d1&#96;</span><br><span class="line">FROM</span><br><span class="line"> (SELECT dt FROM test_table</span><br><span class="line">  WHERE day&#x3D;to_date(days_sub(now(),1))</span><br><span class="line">  GROUP BY dt) &#96;t1&#96;</span><br><span class="line">GROUP BY (CASE WHEN (DAYS_ADD(CAST(CAST(TO_DATE(TO_TIMESTAMP(&#96;t1&#96;.&#96;dt&#96;, &#39;yyyy-MM-dd&#39;)) AS TIMESTAMP) AS TIMESTAMP), 7) </span><br><span class="line">	&gt; CAST(&#39;2021-01-26&#39; AS TIMESTAMP))</span><br><span class="line">	THEN 0 ELSE 1 END)</span><br><span class="line">LIMIT 20;</span><br></pre></td></tr></table></figure>
<p>如果我们设置enable_expr_rewrites为false，则SQL可以成功执行。Impala默认设置了enable_expr_rewrites为true，所以在解析完成之后，会对SQL进行重写，然后再次解析，接下来我们从错误出发，倒着来看问题产生的原因。</p>
<h4 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h4><p>首先，问题出现的地方是在SelectStmt.SelectAnalyzer.verifyAggregation函数中，当我们对SelectStmt进行了rewrite之后，再次analyze，会进行verify，相关代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">for (int i &#x3D; 0; i &lt; selectList_.getItems().size(); ++i) &#123;</span><br><span class="line">  if (!resultExprs_.get(i).isBound(multiAggInfo_.getResultTupleId())) &#123;</span><br><span class="line">    SelectListItem selectListItem &#x3D; selectList_.getItems().get(i);</span><br><span class="line">    throw new AnalysisException(</span><br><span class="line">        &quot;select list expression not produced by aggregation output &quot;</span><br><span class="line">        + &quot;(missing from GROUP BY clause?): &quot;</span><br><span class="line">        + selectListItem.toSql());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里主要就是通过对resultExprs_中的各个expr进行bound检查。经过多次测试和对比，我们发现，两种情况下的resultExprs_变量内容不同，导致了这种结果的差异。在SQL执行失败的情况下，resultExprs_的内容如下所示：<br><img src="https://img-blog.csdnimg.cn/20210208111534440.png#pic_center" alt="1"><br>而当SQL成功执行的情况下，resultExprs_的内容如下所示：<br><img src="https://img-blog.csdnimg.cn/20210208111604820.png#pic_center" alt="2">我们通过比较这两张截图可以看到，resultExprs_包含的expr在不同情况下，一个是CaseExpr，一个则是SlotRef，这两个成员对应的是SQL中的case when子句。正是这个CaseExpr造成了bound的检查失败。这个bound检查就是通过递归，不断对这个CaseExpr以及其children进行检查，我们将这个CaseExpr及其children的树状关系画出来了，如下所示：<br><img src="https://img-blog.csdnimg.cn/20210208111627874.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70#pic_center" alt="tree">最终在左下角的SlotRef中，bound检查失败。需要注意的是，这是我们经过ExprRewrite之后的再次执行SelectStmt.analyze()。重写之前的SelectStmt.analyze()是没有问题的，如下所示：<br><img src="https://img-blog.csdnimg.cn/20210208111652750.png#pic_center" alt="3">需要注意的是，由于这里还没有经过重写，因此截图里面显示的仍然是CAST(‘2021-01-26’ AS TIMESTAMP)，与图一和图二中的不一样。现在，我们的关注点就在于：为什么重写之后，这个resultExprs_包含的这个expr，有时候会是CaseExpr，有时候是SlotRef。而这正是SQL执行有时候成功，有时候失败的关键。<br>为了弄清楚这个问题，我们需要关注下resultExprs_这个变量是如何来的。我们查看这个变量的定义：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; QueryStmt.java</span><br><span class="line">  &#x2F;&#x2F; For a select statment:</span><br><span class="line">  &#x2F;&#x2F; original list of exprs in select clause (star-expanded, ordinals and</span><br><span class="line">  &#x2F;&#x2F; aliases substituted, agg output substituted)</span><br><span class="line">  &#x2F;&#x2F; For a union statement:</span><br><span class="line">  &#x2F;&#x2F; list of slotrefs into the tuple materialized by the union.</span><br><span class="line">  protected List&lt;Expr&gt; resultExprs_ &#x3D; new ArrayList&lt;&gt;();</span><br></pre></td></tr></table></figure>
<p>首先，会在SelectStmt.SelectAnalyzer.analyzeSelectClause()方法中，将SelectList的成员对应的expr依次加入到resultExprs_中，这里SQL解析的SelectList，第二个成员对应的expr是CaseExpr，这里是没有问题的。紧接着，会在SelectStmt.SelectAnalyzer.buildResultExprs()方法中进行substitute操作，相关调用栈如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">SelectStmt.analyze()</span><br><span class="line">-SelectStmt.SelectAnalyzer.analyze()</span><br><span class="line">--SelectStmt.SelectAnalyzer.buildResultExprs()</span><br><span class="line">---Expr.substituteList()</span><br><span class="line">----Expr.trySubstituteList()</span><br><span class="line">-----Expr.trySubstitute()</span><br><span class="line">------Expr.substituteImpl()</span><br><span class="line">-------ExprSubstitutionMap.get()</span><br><span class="line">--------Expr.equals()</span><br><span class="line">---------Expr.matches()</span><br><span class="line">----------TimestampLiteral.localEquals()</span><br></pre></td></tr></table></figure>
<p>由于这里涉及到的调用路径比较长，我这里简单的总结下：当进行substitute操作的时候，会从一个ExprSubstitutionMap中进行匹配，如果匹配上了，则使用ExprSubstitutionMap中的expr来替代原先的expr，相关代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; ExprSubstitutionMap.java</span><br><span class="line">  public Expr get(Expr lhsExpr) &#123;</span><br><span class="line">    for (int i &#x3D; 0; i &lt; lhs_.size(); ++i) &#123;</span><br><span class="line">      if (lhsExpr.equals(lhs_.get(i))) return rhs_.get(i);</span><br><span class="line">    &#125;</span><br><span class="line">    return null;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>关于ExprSubstitutionMap这里我们不用展开说明。可以看到，当lhs_中能匹配到时，则返回rhs_中对应的成员。这里我们就是用CaseExpr进行匹配。所以，当匹配到了，就会将resultExprs_中的CaseExpr替换为SlotRef（来自rhs_），此时SQL就能执行成功；如果匹配不到，则保持原先的CaseExpr不变，此时SQL执行报错。<br>所以问题就在于这个equals方法，代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; Expr.java</span><br><span class="line">  public final boolean equals(Object obj) &#123;</span><br><span class="line">    return obj instanceof Expr &amp;&amp; matches((Expr) obj, SlotRef.SLOTREF_EQ_CMP);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>在Expr.matches()方法中，就是对expr的各个children进行比较，我们发现，有时候TIMESTAMP ‘2021-01-26’这个TimestmapLiteral的比较失败（由CAST(‘2021-01-26’ AS TIMESTAMP)重写得到），导致SQL执行失败；有时候，能够比较成功，则SQL能执行成功。这里的TimestmapLiteral就对应树状图中的黄色节点部分。<br>我们发现ExprSubstitutionMap中的CaseExpr的TimestmapLiteral内容总是如下所示：<br><img src="https://img-blog.csdnimg.cn/20210208111724985.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70#pic_center" alt="4">而resultExpr_中的CaseExpr包含的TimestmapLiteral，有时候与上述一样，有时候则不一样，如下所示：<br><img src="https://img-blog.csdnimg.cn/20210208111739392.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70#pic_center" alt="5">我们可以看到，后四位是明显不一样的，正是因为这个不一样，导致ExprSubstitutionMap匹配为空，进而影响CaseExpr没有替换为SlotRef，最终影响了SQL的执行。所以现在的问题就是要搞清楚，为什么这个TimestmapLiteral包含的16位字节数组，多次执行的结果不一致。<br>目前的问题，主要就是对CAST(‘2021-01-26’ AS TIMESTAMP)的处理导致的，在进行重写的时候，这个表达式会通过FoldConstantsRule这个规则进行重写，这其中会调用到BE端的代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">FoldConstantsRule.apply()</span><br><span class="line">-LiteralExpr.createBounded()</span><br><span class="line">--FeSupport.EvalExprWithoutRowBounded()</span><br><span class="line">---FeSupport.EvalExprsWithoutRowBounded()</span><br><span class="line">----FeSupport.NativeEvalExprsWithoutRow()</span><br><span class="line">------Java_org_apache_impala_service_FeSupport_NativeEvalExprsWithoutRow fe-support.cc</span><br></pre></td></tr></table></figure>
<p>最终在BE端，通过这个函数进行了计算和转换，得到对应的TColumnValue，然后在FE端转换成相应的TimestmapLiteral，在BE端的主要转换流程如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TExpr-&gt;ScalarExpr-&gt;ScalarExprEvaluator-&gt;TimestampVal-&gt;TimestampValue-&gt;TColumnValue-&gt;TResultRow</span><br></pre></td></tr></table></figure>
<p>最终将构造好的TResultRow序列化传到FE端。在Java_org_apache_impala_service_FeSupport_NativeEvalExprsWithoutRow方法中，我们通过GDB打印TColumnValue包含的binary_val（最终会使用这个来构造TimestmapLiteral），发现SQL执行失败的情况下，最后几个字节确实会有问题：<br><img src="https://img-blog.csdnimg.cn/20210208111814366.png#pic_center" alt="6"><br>而SQL执行成功的时候，最后几个节点是这样的：<br><img src="https://img-blog.csdnimg.cn/20210208111831850.png#pic_center" alt="7"><br>这与我们在java的ide进行远程调试的时候，看到的TimestmapLiteral包含的字节数组的最后几位也是一致的，这就说明我们在BE端构造TColumnValue的时候就已经有问题了。<br>我们继续往上追溯发现，TColumnValue的binary_val构造代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; fe-support.cc SetTColumnValue()</span><br><span class="line">    case TYPE_TIMESTAMP: &#123;</span><br><span class="line">      const uint8_t* uint8_val &#x3D; reinterpret_cast&lt;const uint8_t*&gt;(value);</span><br><span class="line">      col_val-&gt;binary_val.assign(uint8_val, uint8_val + type.GetSlotSize());</span><br><span class="line">      col_val-&gt;__isset.binary_val &#x3D; true;</span><br><span class="line">      RawValue::PrintValue(value, type, -1, &amp;col_val-&gt;string_val);</span><br><span class="line">      col_val-&gt;__isset.string_val &#x3D; true;</span><br><span class="line">      break;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>这里的value是一个void*，在当前情况下，对应的是TimestampValue类型的指针；对于TImestmap类型，type.GetSlotSize()会返回16。TimestampValue的构造代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; ScalarExprEvaluator.cc GetValue()</span><br><span class="line">    case TYPE_TIMESTAMP: &#123;</span><br><span class="line">      impala_udf::TimestampVal v &#x3D; expr.GetTimestampVal(this, row);</span><br><span class="line">      if (v.is_null) return nullptr;</span><br><span class="line">      result_.timestamp_val &#x3D; TimestampValue::FromTimestampVal(v);</span><br><span class="line">      return &amp;result_.timestamp_val;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>我们通过GDB来打印result_.timestamp_val的最后4个字节内容，如下所示：<br><img src="https://img-blog.csdnimg.cn/2021020811185329.png#pic_center" alt="8"><br>从这里可以看到，在构造完TimestampValue之后，最后几个节点对应的值就已经有问题了。我们继续查看TimestampVal发现最后几个字节都是0，也就是说TimestampVal构造没有问题，但是在构造result_.timestamp_val的时候，出现了问题：<br><img src="https://img-blog.csdnimg.cn/20210208112126488.png#pic_center" alt="10"><br>result_.timestamp_val是一个TimestampValue的便利，包含2个成员变量：4个字节的date_和8个字节的time_，由于对齐机制，一共16个字节。通过调试我们发现：对于TimestampValue变量，0～7字节存储的是time_，对于“2021-01-26 00:00:00”而言，一直为0，所以0～7字节的值一直是0；8～11字节存储的是date_，对应截图中的：105、-122、37、0；最后的8<del>15是填充字节，而正是这四个字节的不同，导致了整个TimestampValue的不同。<br>经过调试发现，对于代码：result_.timestamp_val = TimestampValue::FromTimestampVal(v)，timestamp_val变量在被赋值之前，就已经有内容了，由于最后8</del>15这四个填充字节的不同，导致了返回到FE端的字节数组不同。<br>这个result_属于ScalarExprEvaluator，是一个ExprValue类型的变量，初始化流程如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Java_org_apache_impala_service_FeSupport_NativeEvalExprsWithoutRow fe-support.cc</span><br><span class="line">-Create() scalar-expr-evaluator.cc</span><br><span class="line">-ctor() scalar-expr-evaluator.cc</span><br><span class="line">-ctor() expr-value.h</span><br></pre></td></tr></table></figure>
<p>对于timestamp_val的初始化直接使用了timestamp_val ()，目前我怀疑是因为初始化该SQL的时候，timestamp_val分配的内存没有置0。为验证这个猜想，我们在ExprValue的构造函数中显示对timestamp_val的内存进行清空，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">ExprValue()</span><br><span class="line">  : bool_val(false),</span><br><span class="line">    tinyint_val(0),</span><br><span class="line">    smallint_val(0),</span><br><span class="line">    int_val(0),</span><br><span class="line">    bigint_val(0),</span><br><span class="line">    float_val(0.0),</span><br><span class="line">    double_val(0.0),</span><br><span class="line">    string_val(NULL, 0),</span><br><span class="line">    timestamp_val(),</span><br><span class="line">    decimal4_val(),</span><br><span class="line">    decimal8_val(),</span><br><span class="line">    decimal16_val(),</span><br><span class="line">    collection_val(),</span><br><span class="line">    date_val(0) &#123;</span><br><span class="line">  memset(&amp;timestamp_val, 0, sizeof(timestamp_val));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>重新编译之后，再测试，多次执行SQL没有再出现同样的问题了。</p>
<h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><p>目前，针对这种情况，由于社区的4.x开发版本，我们无法复现该问题，并且我们也没有看到相关的patch，因此怀疑是4.0依赖的编译器之类的，会保证在new的时候，直接对分配的内存空间置0，所以不会出现该问题。我们已经将问题反馈到社区，等待社区的相关回复：<a target="_blank" rel="noopener" href="https://issues.apache.org/jira/browse/IMPALA-10461">IMPALA-10461</a><br>针对3.4.0版本的问题，我们目前的解决方案有两种：</p>
<ol>
<li>上面其实已经提到了，就是在ExprValue的构造函数中，显示地对Timestamp置0：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">memset(&amp;timestamp_val, 0, sizeof(timestamp_val));</span><br></pre></td></tr></table></figure></li>
<li>由于是最后的4个padding字节导致的问题，因此我们可以对FE端的TimestampLiteral.localEquals方法进行调整，只比较前12个字节：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> public boolean localEquals(Expr that) &#123;</span><br><span class="line">  return super.localEquals(that) &amp;&amp;</span><br><span class="line">      &#x2F;&#x2F; Arrays.equals(value_, ((TimestampLiteral) that).value_);</span><br><span class="line">      Arrays.equals(Arrays.copyOfRange(value_, 0, 12),</span><br><span class="line">          Arrays.copyOfRange(((TimestampLiteral) that).value_, 0, 12));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://skyyws.github.io/2021/04/16/Impala-cast-timestamp%E5%AF%BC%E8%87%B4%E7%9B%B8%E5%90%8CSQL%E6%9F%A5%E8%AF%A2%E4%B8%8D%E4%B8%80%E8%87%B4%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/" data-id="cknk9zwi20000tcx579kt0v0c" data-title="Impala cast timestamp导致相同SQL查询不一致问题排查" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/olap/" rel="tag">olap</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/" rel="tag">问题排查</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Impala-3-4-SQL查询之ScanRange详解（四）" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8BScanRange%E8%AF%A6%E8%A7%A3%EF%BC%88%E5%9B%9B%EF%BC%89/" class="article-date">
  <time class="dt-published" datetime="2021-04-16T12:11:20.000Z" itemprop="datePublished">2021-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8BScanRange%E8%AF%A6%E8%A7%A3%EF%BC%88%E5%9B%9B%EF%BC%89/">Impala 3.4 SQL查询之ScanRange详解（四）</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>在上篇文章中，我们主要介绍了ScanRange的构造，以及在FE和BE端的一些处理流程。同时，我们还介绍了IO thead处理模型中一个比较重要的对象RequestContext::PerDiskState，以及部分成员变量的含义，在本篇文章中，我们将介绍其中一个比较重要的成员：unstarted_scan_ranges_。</p>
<h4 id="关于BE端的ScanRange"><a href="#关于BE端的ScanRange" class="headerlink" title="关于BE端的ScanRange"></a>关于BE端的ScanRange</h4><p>在上篇文章中，我们提到，在FE端的ScanRange信息，主要通过TScanRange传到BE端，然后构造为TPlanFragmentInstanceCtx中的TScanRangeParams，传到各个executor进行实际的扫描操作，那么当各个executor接收到请求之后，就会根据这些信息，构造相应的ScanRange类。ScanRange是继承RequestRange这个类的，另外WriteRange也是继承了RequestRange对象的。从名字就可以看出，WriteRange主要是针对写入的情况，这里我们不展开介绍，主要看下ScanRange对象。首先，RequestRange主要包含了file、offset、len这些基本信息。而ScanRange则增加了一些额外的信息，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class ScanRange : public RequestRange &#123;</span><br><span class="line">    struct SubRange &#123;</span><br><span class="line">    int64_t offset;</span><br><span class="line">    int64_t length;</span><br><span class="line">  &#125;;</span><br><span class="line">  </span><br><span class="line">  DiskIoMgr* io_mgr_ &#x3D; nullptr;</span><br><span class="line">  RequestContext* reader_ &#x3D; nullptr;</span><br><span class="line">  bool read_in_flight_ &#x3D; false;</span><br><span class="line">  int64_t bytes_read_ &#x3D; 0;</span><br><span class="line">  std::vector&lt;SubRange&gt; sub_ranges_;</span><br><span class="line">  ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>关于这些成员变量的含义，我们这里先不一一介绍了，后面在相应的场景下，我们再一一展开说明。<br>当我们将TPlanFragmentInstanceCtx的信息传到对应的executor的时候，对应的executor节点就会构造相应的HdfsScanNode，然后在HdfsScanNodeBase::Prepare函数中，会循环遍历每个TScanRangeParams，然后初始化下面的这个map成员：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; hdfs-scan-node-base.h</span><br><span class="line">&#x2F;&#x2F;&#x2F; This is a pair for partition ID and filename</span><br><span class="line">typedef pair&lt;int64_t, std::string&gt; PartitionFileKey;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;&#x2F; partition_id, File path &#x3D;&gt; file descriptor (which includes the file&#39;s splits)</span><br><span class="line">typedef std::unordered_map&lt;PartitionFileKey, HdfsFileDesc*, pair_hash&gt; FileDescMap;</span><br><span class="line">FileDescMap file_descs_;</span><br><span class="line"></span><br><span class="line">struct HdfsFileDesc &#123;</span><br><span class="line">  hdfsFS fs;</span><br><span class="line">  std::string filename;</span><br><span class="line">  int64_t file_length;</span><br><span class="line">  int64_t mtime</span><br><span class="line">  THdfsCompression::type file_compression;</span><br><span class="line">  bool is_erasure_coded;</span><br><span class="line">  std::vector&lt;io::ScanRange*&gt; splits;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>file_descs_是一个map，用分区id和文件名来作为map的key，value是一个HdfsFileDesc对象。当循环遍历TScanRangeParams对象的时候，Impala会用其中包含的THdfsFileSplit对象的信息，来构造一个HdfsFileDesc对象，填充其中的fs、filename等信息，关键代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">for (const TScanRangeParams&amp; params: *scan_range_params_) &#123;</span><br><span class="line">  const THdfsFileSplit&amp; split &#x3D; params.scan_range.hdfs_file_split;</span><br><span class="line">  partition_ids_.insert(split.partition_id);</span><br><span class="line">  HdfsPartitionDescriptor* partition_desc &#x3D;</span><br><span class="line">      hdfs_table_-&gt;GetPartition(split.partition_id);</span><br><span class="line"></span><br><span class="line">  filesystem::path file_path(partition_desc-&gt;location());</span><br><span class="line">  file_path.append(split.relative_path, filesystem::path::codecvt());</span><br><span class="line">  const string&amp; native_file_path &#x3D; file_path.native();</span><br><span class="line"></span><br><span class="line">  auto file_desc_map_key &#x3D; make_pair(partition_desc-&gt;id(), native_file_path);</span><br><span class="line">  HdfsFileDesc* file_desc &#x3D; NULL;</span><br><span class="line">  FileDescMap::iterator file_desc_it &#x3D; file_descs_.find(file_desc_map_key);</span><br><span class="line">  if (file_desc_it &#x3D;&#x3D; file_descs_.end()) &#123;</span><br><span class="line">    &#x2F;&#x2F; Add new file_desc to file_descs_ and per_type_files_</span><br><span class="line">    file_descs_[file_desc_map_key] &#x3D; file_desc;</span><br><span class="line">    &#x2F;&#x2F; 省略其余代码</span><br><span class="line">    file_desc &#x3D; runtime_state_-&gt;obj_pool()-&gt;Add(new HdfsFileDesc(native_file_path));</span><br><span class="line">    per_type_files_[partition_desc-&gt;file_format()].push_back(file_desc);</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    &#x2F;&#x2F; File already processed</span><br><span class="line">    file_desc &#x3D; file_desc_it-&gt;second;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  file_desc-&gt;splits.push_back(</span><br><span class="line">      AllocateScanRange(file_desc-&gt;fs, file_desc-&gt;filename.c_str(), split.length,</span><br><span class="line">          split.offset, split.partition_id, params.volume_id, expected_local,</span><br><span class="line">          file_desc-&gt;is_erasure_coded, file_desc-&gt;mtime, BufferOpts(cache_options)));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们删除了部分代码，只保留了关键的部分。可以看到，当file_descs_中，不存在指定key时，我们构造新的key和value，加入到map中。这里关注下对于splits这个vector的处理。对于分区的某个指定文件，在map中会有一条记录，如果这个文件对应多个TScanRangeParams，那么这个map的value对应的splits则会有多个成员，但是这条key-value记录只有一条。我们前面说过了，一个ScanRange在HDFS_SCAN_NODE代表一个block，所以如果文件跨越了多个block，那么就会分成多个ScanRange，此时map的value，HdfsFileDesc对象的splits就会存在多个成员；反之，如果文件只存在于1个block中，那么HdfsFileDesc的splits对象，则只会有1个成员。<br>除了file_descs_之外，还有一个成员也需要关注下：per_type_files_，这个成员变量的定义如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; hdfs-scan-node-base.h</span><br><span class="line">  &#x2F;&#x2F;&#x2F; File format &#x3D;&gt; file descriptors.</span><br><span class="line">  typedef std::map&lt;THdfsFileFormat::type, std::vector&lt;HdfsFileDesc*&gt;&gt;</span><br><span class="line">    FileFormatsMap;</span><br><span class="line">  FileFormatsMap per_type_files_;</span><br></pre></td></tr></table></figure>
<p>可以看到，这个per_type_files_保存的就是文件格式和HdfsFileDesc的集合，在上述处理file_descs_的代码中，我们也可以看到对per_type_files_的处理，根据当前这个文件所属分区的格式，加入到map value的vector中。</p>
<h4 id="关于unstarted-scan-ranges"><a href="#关于unstarted-scan-ranges" class="headerlink" title="关于unstarted_scan_ranges"></a>关于unstarted_scan_ranges</h4><p>上面我们介绍完了BE端的ScanRange对象，接下来我们来看一下PerDiskState中的unstarted_scan_ranges_成员，以及它是如何更新的。首先，我们还是先看下这个成员变量的定义：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;&#x2F; Queue of ranges that have not started being read.  This list is exclusive</span><br><span class="line">&#x2F;&#x2F;&#x2F; with in_flight_ranges.</span><br><span class="line">InternalQueue&lt;ScanRange&gt; unstarted_scan_ranges_;</span><br></pre></td></tr></table></figure>
<p>从注释我们可以看到，unstarted_scan_ranges_表示是还没有开始进行scan操作的ScanRange，这个解释比较空泛，我们接着看下unstarted_scan_ranges这个成员更新的相关函数调用（当前是针对parquet格式的表进行梳理）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">ExecFInstance(query-state.cc):697</span><br><span class="line">-Exec(fragment-instance-state.cc):98</span><br><span class="line">--ExecInternal(fragment-instance-state.cc):383</span><br><span class="line">---GetNext(hdfs-scan-node.cc):91</span><br><span class="line">----IssueInitialScanRanges(hdfs-scan-node-base.cc):636</span><br><span class="line">-----IssueInitialRanges(hdfs-parquet-scanner.cc):82</span><br><span class="line">------IssueFooterRanges(hdfs-scanner.cc):837</span><br><span class="line">-------AddDiskIoRanges(hdfs-scan-node.cc):212</span><br><span class="line">--------AddScanRanges(request-context.cc):404</span><br><span class="line">---------AddRangeToDisk(request-context.cc):357</span><br><span class="line">----------unstarted_scan_ranges()-&gt;Enqueue</span><br><span class="line">---------AddRangeToDisk(request-context.cc):362</span><br><span class="line">----------num_unstarted_scan_ranges_.Add(1)</span><br><span class="line">---------AddRangeToDisk(request-context.cc):366</span><br><span class="line">----------next_scan_range_to_start()&#x3D;null ScheduleContext(request-context.cc)</span><br><span class="line">---------AddRangeToDisk(request-context.cc):379</span><br><span class="line">----------num_remaining_ranges_++</span><br></pre></td></tr></table></figure>
<p>在HdfsScanNodeBase::IssueInitialScanRanges函数中，我们通过per_type_files_成员，获取所有的PARQUET格式的HdfsFileDesc集合，然后在HdfsScanner::IssueFooterRanges函数中，循环构造初始的ScanRange（不同的文件格式，这里的处理流程有所不同），由于当前是PARQUET文件，所以会构造每个文件footer的ScanRange，这里我们摘取一些主要的步骤看下（忽略其他的一些特殊情况）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">    &#x2F;&#x2F;这里FOOTER_SIZE是一个常量，为1024*100</span><br><span class="line">    int64_t footer_size &#x3D; min(FOOTER_SIZE, files[i]-&gt;file_length);</span><br><span class="line">    int64_t footer_start &#x3D; files[i]-&gt;file_length - footer_size;</span><br><span class="line"></span><br><span class="line">    ScanRange* footer_split &#x3D; FindFooterSplit(files[i]);</span><br><span class="line"></span><br><span class="line">    for (int j &#x3D; 0; j &lt; files[i]-&gt;splits.size(); ++j) &#123;</span><br><span class="line">      ScanRange* split &#x3D; files[i]-&gt;splits[j];</span><br><span class="line"></span><br><span class="line">      if (!scan_node-&gt;IsZeroSlotTableScan() || footer_split &#x3D;&#x3D; split) &#123;</span><br><span class="line">        ScanRangeMetadata* split_metadata &#x3D;</span><br><span class="line">            static_cast&lt;ScanRangeMetadata*&gt;(split-&gt;meta_data());</span><br><span class="line">        ScanRange* footer_range;</span><br><span class="line">        if (footer_split !&#x3D; nullptr) &#123;</span><br><span class="line">          footer_range &#x3D; scan_node-&gt;AllocateScanRange(files[i]-&gt;fs,</span><br><span class="line">              files[i]-&gt;filename.c_str(), footer_size, footer_start,</span><br><span class="line">              split_metadata-&gt;partition_id, footer_split-&gt;disk_id(),</span><br><span class="line">              footer_split-&gt;expected_local(), files[i]-&gt;is_erasure_coded, files[i]-&gt;mtime,</span><br><span class="line">              BufferOpts(footer_split-&gt;cache_options()), split);</span><br><span class="line">        &#125;</span><br><span class="line">        footer_ranges.push_back(footer_range);</span><br><span class="line">    &#125;</span><br><span class="line">  &#x2F;&#x2F; The threads that process the footer will also do the scan.</span><br><span class="line">  if (footer_ranges.size() &gt; 0) &#123;</span><br><span class="line">    RETURN_IF_ERROR(scan_node-&gt;AddDiskIoRanges(footer_ranges, EnqueueLocation::TAIL));</span><br><span class="line">  &#125;</span><br><span class="line">  return Status::OK();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们删除了其他的一些代码和注释，关注下主要的处理步骤，首先获取footer_size和footer_start，然后利用FindFooterSplit函数获取该file的footer split，判断逻辑就是从splits成员中找到：split.len+split.offset=file.len，可以理解为文件的最后一个split成员对象。然后遍历splits集合，当找到与footer_split对应的split时，我们就用这个footer_split和file的相关信息来构造一个ScanRange，作为footer ScanRange，当处理完成所有的文件之后，我们最终通过RequestContext::AddRangeToDisk函数，将这些footer的ScanRange加入到unstarted_scan_ranges_对象中，同时，每入队一个ScanRange对象，我们会将num_unstarted_scan_ranges_这个成员加1。也就是说，这个unstarted_scan_ranges_最终存放的是所有file文件的footer ScanRange。<br>上面我们介绍了unstarted_scan_ranges_这个队列的入队流程，接着我们看下出队的操作。在前面的文章中，我们提到了，IO thread会从RequestContext队列的头部取出一个RequestContext对象，然后通过该RequestContext对象获取一个ScanRange进行处理，相关处理函数如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">RequestRange* RequestContext::GetNextRequestRange(int disk_id) &#123;</span><br><span class="line">  PerDiskState* request_disk_state &#x3D; &amp;disk_states_[disk_id];</span><br><span class="line">  unique_lock&lt;mutex&gt; request_lock(lock_);</span><br><span class="line"></span><br><span class="line">  if (request_disk_state-&gt;next_scan_range_to_start() &#x3D;&#x3D; nullptr &amp;&amp;</span><br><span class="line">      !request_disk_state-&gt;unstarted_scan_ranges()-&gt;empty()) &#123;</span><br><span class="line">    ScanRange* new_range &#x3D; request_disk_state-&gt;unstarted_scan_ranges()-&gt;Dequeue();</span><br><span class="line">    num_unstarted_scan_ranges_.Add(-1);</span><br><span class="line">    ready_to_start_ranges_.Enqueue(new_range);</span><br><span class="line">    request_disk_state-&gt;set_next_scan_range_to_start(new_range);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  if (request_disk_state-&gt;in_flight_ranges()-&gt;empty()) &#123;</span><br><span class="line">    request_disk_state-&gt;DecrementDiskThread(request_lock, this);</span><br><span class="line">    return nullptr;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  RequestRange* range &#x3D; request_disk_state-&gt;in_flight_ranges()-&gt;Dequeue();</span><br><span class="line"></span><br><span class="line">  request_disk_state-&gt;ScheduleContext(request_lock, this, disk_id);</span><br><span class="line">  return range;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>同样我们删除了一些代码，方便阅读。首选获取对应的PerDiskState对象，然后将unstarted_scan_ranges_队列的头部对象出队，并将num_unstarted_scan_ranges_加1，同时入队到ready_to_start_ranges_中，这两个变量都是RequestContext的成员，这里我们先不展开说明。接着将出队的ScanRange对象设置到next_scan_range_to_start_成员，关于这个成员的用处，我们也在后面展开说明。<br>紧接着，会判断in_flight_ranges_队列是否为空，是则直接返回null，表示这次IO thead没取到ScanRange；否则，从in_flight_ranges_弹出头部的ScanRange对象，返回进行处理。</p>
<h4 id="unstarted-scan-ranges的后续处理"><a href="#unstarted-scan-ranges的后续处理" class="headerlink" title="unstarted_scan_ranges的后续处理"></a>unstarted_scan_ranges的后续处理</h4><p>前面我们提到了IO thread并不会直接获取unstarted_scan_ranges_队列上的ScanRange进行处理。先将unstarted_scan_ranges_的头部出队，然后入队到ready_to_start_ranges_队列中，同时设置到next_scan_range_to_start_成员。然后再从in_flight_ranges_队列中取出头部对象，进行后续的处理。由于这里涉及到的成员变量很多，我们将RequestContext和PerDiskState的成员进行了归纳，如下所示：<br><img src="https://img-blog.csdnimg.cn/20210416195856857.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70#pic_center" alt="1"><br>这里我们简单说明一下，RequestContex对象会包含多个PerDiskState对象，每一个PerDiskState对象表示一种disk queue，例如remote HDFS、S3等，所以RequestContex对象的这些成员，统计的是所有PerDiskState的相应成员的累加和，比如num_unstarted_scan_ranges_这个成员，统计的就是该RequestContex对象上的所有PerDiskState的unstarted_scan_ranges_的总和。这点需要注意。<br>下面我们来看下ready_to_start_ranges_和next_scan_range_to_start_的相关处理，函数调用如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Open(hdfs-scan-node.cc):170</span><br><span class="line">-ThreadTokenAvailableCb(hdfs-scan-node.cc):338</span><br><span class="line">--ScannerThread(hdfs-scan-node.cc):403</span><br><span class="line">---StartNextScanRange(hdfs-scan-node-base.cc):679</span><br><span class="line">----GetNextUnstartedRange(request-context.cc):442</span><br></pre></td></tr></table></figure>
<p>通过上述的函数调用栈我们可以看到，在HdfsScanNode的Open函数中，会启动一个专门的scanner线程来处理unstarted_scan_ranges。最终在RequestContext::GetNextUnstartedRange函数中，会对next_scan_range_to_start_和ready_to_start_ranges_进行处理，关键代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; RequestContext::GetNextUnstartedRange()</span><br><span class="line">*range &#x3D; ready_to_start_ranges_.Dequeue();</span><br><span class="line">int disk_id &#x3D; (*range)-&gt;disk_id();</span><br><span class="line">disk_states_[disk_id].set_next_scan_range_to_start(nullptr);</span><br><span class="line">ScheduleScanRange(lock, *range);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; void RequestContext::ScheduleScanRange()</span><br><span class="line">RequestContext::PerDiskState&amp; state &#x3D; disk_states_[range-&gt;disk_id()];</span><br><span class="line">state.in_flight_ranges()-&gt;Enqueue(range);</span><br></pre></td></tr></table></figure>
<p>可以看到在GetNextUnstartedRange函数中，先将ready_to_start_ranges_队列中的头部对象弹出，然后将该ScanRange对应的PerDiskState的next_scan_range_to_start_对象设置为空。然后在RequestContext::ScheduleScanRange函数中，将该ScanRange插入到对应PerDiskState的in_flight_ranges_队列中。</p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>到这里，关于unstarted_scan_ranges_的相关处理流程我们就介绍的差不多了。回顾一下，我们在本文中，首先介绍了BE端的ScanRange，相较于thrift的TScanRange结构体，ScanRange对象主要是在每个executor上进行实际scan操作时，需要用到的类。除此之外，我们还介绍了一个关键的对象：unstarted_scan_ranges_，这是一个ScanRange的队列，我们通过代码，一步一步了解了这个队列的更新情况，包括入队和出队，这个对象对于整个IO thread模型是比较重要的。现在读者看下来这两篇文章可能觉得比较琐碎，后面笔者会将各个成员串起来，整体看下Impala的这个IO thread的处理。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://skyyws.github.io/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8BScanRange%E8%AF%A6%E8%A7%A3%EF%BC%88%E5%9B%9B%EF%BC%89/" data-id="cknk9v6t00000n3x59fd23vef" data-title="Impala 3.4 SQL查询之ScanRange详解（四）" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Impala-SQL%E6%9F%A5%E8%AF%A2%E7%B3%BB%E5%88%97/" rel="tag">Impala SQL查询系列</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/olap/" rel="tag">olap</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Impala-3-4-SQL查询之ScanRange详解（三）" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8BScanRange%E8%AF%A6%E8%A7%A3%EF%BC%88%E4%B8%89%EF%BC%89/" class="article-date">
  <time class="dt-published" datetime="2021-04-16T11:17:28.000Z" itemprop="datePublished">2021-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8BScanRange%E8%AF%A6%E8%A7%A3%EF%BC%88%E4%B8%89%EF%BC%89/">Impala 3.4 SQL查询之ScanRange详解（三）</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>我们在本系列的前两篇文章中，简单介绍了SQL查询的整个流程以及重写的相关知识。在接下来的这几篇中，会跟大家一起详细学习ScanRange的知识。由于涉及到的内容非常多，因此会分成几篇来讲解，主要会涉及到HDFS_SCAN_NODE、IO thread等知识。由于现在相关的文档比较少，这些文章都是笔者根据代码和实际调试结果整理出来的，如有错误，欢迎指正。默认情况下，本文涉及到的测试表都是HDFS上的parquet表，并且是以天为分区。</p>
<h4 id="关于ScanRange"><a href="#关于ScanRange" class="headerlink" title="关于ScanRange"></a>关于ScanRange</h4><p>ScanRange是Impala中一个非常基础的概念，对于HDFS_SCAN_NODE来说，一个ScanRange表示的就是一个HDFS文件上的一部分，一般用file_name、offset和len来表示，更多关于ScanRange的详细介绍，可以参考文章：<a target="_blank" rel="noopener" href="https://blog.csdn.net/huang_quanlong/article/details/53980132">Impala源码阅读——SimpleScheduler</a>。本文我们主要讲一下ScanRange的构造，以及在HDFS_SCAN_NODE过程中的一些处理，同时会涉及到IO thread模型相关的一些知识，感兴趣的同学，可以看看我的前两篇文章：<a href="https://skyyws.github.io/2021/04/16/Impala-HDFS-SCAN-NODE%E4%B9%8BIO-threads%E6%A8%A1%E5%9E%8B">Impala HDFS_SCAN_NODE之IO threads模型</a>和<a href="https://skyyws.github.io/2021/04/16/Impala-HDFS-SCAN-NODE%E4%B9%8BAverageHdfsReadThreadConcurrency">Impala HDFS_SCAN_NODE之AverageHdfsReadThreadConcurrency</a>。<br>当SQL提交到Impalad节点之后，会通过JNI调用，由FE模块进行执行计划的解析，最终会针对每个表，构建一个HDFS_SCAN_NODE，其中就会包含ScanRange的信息，相关的函数调用栈如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">ExecuteInternal(impala-server.cc):956</span><br><span class="line">-InitExecRequest(client-request-state.cc):1440</span><br><span class="line">--GetExecRequest(frontend.cc):230</span><br><span class="line">---createExecRequest(JniFrontend.java):154</span><br><span class="line">----createExecRequest(Frontend.java):1464</span><br><span class="line">-----getTExecRequest(Frontend.java):1494</span><br><span class="line">------doCreateExecRequest(Frontend.java):1600</span><br><span class="line">-------getPlannedExecRequest(Frontend.java):1734</span><br><span class="line">--------createExecRequest(Frontend.java):1413</span><br><span class="line">---------createPlans(Planner.java):264</span><br><span class="line">----------createPlanFragments(Planner.java):118</span><br><span class="line">-----------createSingleNodePlan(SingleNodePlanner.java):150</span><br><span class="line">------------createQueryPlan(SingleNodePlanner.java):268</span><br><span class="line">-------------createSelectPlan(SingleNodePlanner.java):669</span><br><span class="line">--------------createTableRefsPlan(SingleNodePlanner.java):845</span><br><span class="line">---------------createTableRefNode(SingleNodePlanner.java):1686</span><br><span class="line">----------------createScanNode(SingleNodePlanner.java)</span><br></pre></td></tr></table></figure>
<p>在FE端构造HdfsScanNode对象的时候，所有的ScanRange信息都存储在scanRangeSpecs_对象中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;HdfsScanNode.java</span><br><span class="line">&#x2F;&#x2F; Scan-range specs. Populated in init().</span><br><span class="line">protected TScanRangeSpec scanRangeSpecs_</span><br></pre></td></tr></table></figure>
<p>这里我们使用一个测试SQL，然后通过远程调试，查看这个变量的信息，如下所示：<br><img src="https://img-blog.csdnimg.cn/20210416105123682.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="1"><br>可以看到，这个scanRangeSpecs_对象中，就有232个TScanRangeLocationList对象。当FE端所有的处理都完成之后，最终会返回一个TExecRequest对象，我们同样通过远程调试，查看这个对象的信息，如下所示：<br><img src="https://img-blog.csdnimg.cn/20210416105156663.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="2"><br>通过上面的截图，我们可以看到，该测试SQL包含了两个TScanRangeSpec，分别对应两个HDFS_SCAN_NODE，一个包含了232个TScanRangeLocationList，另外一个包含了4816个，而每个TScanRangeLocationList就包含了一个TScanRange对象，这个TScanRange对象就是ScanRange在FE端的一个体现。对于HDFS_SCAN_NODE来说，TScanRange包含了1个THdfsFileSplit，其中就包含了path、offset、len等信息。当TExecRequest被传回到BE端之后，同样需要进行一系列的转换操作，相关的函数调用如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">ExecuteInternal(impala-server.cc):977</span><br><span class="line">-InitExecRequest(client-request-state.cc):1440</span><br><span class="line">-Exec(client-request-state.cc):197</span><br><span class="line">--ExecAsyncQueryOrDmlRequest(client-request-state.cc):508</span><br><span class="line">---FinishExecQueryOrDmlRequest(client-request-state.cc):518</span><br><span class="line">----SubmitForAdmission(admission-controller.cc):863         </span><br><span class="line">-----FindGroupToAdmitOrReject(admission-controller.cc):1271</span><br><span class="line">------ComputeGroupSchedules(admission-controller.cc):1248</span><br><span class="line">-------Schedule(scheduler.cc):769</span><br><span class="line">--------ComputeScanRangeAssignment(scheduler.cc):174</span><br><span class="line">---------schedule-&gt;GetFragmentExecParams(fragment.idx)-&gt;scan_range_assignment</span><br><span class="line">--------ComputeScanRangeAssignment(scheduler.cc):192</span><br><span class="line">---------ComputeScanRangeAssignment(scheduler.cc):600&#x2F;695</span><br><span class="line">----------RecordScanRangeAssignment(scheduler.cc):1090~1100</span><br><span class="line">-------Schedule(scheduler.cc):770</span><br><span class="line">--------ComputeFragmentExecParams(scheduler.cc)</span><br><span class="line">-------Schedule(scheduler.cc):771</span><br><span class="line">--------ComputeBackendExecParams(scheduler.cc)</span><br><span class="line">---FinishExecQueryOrDmlRequest(client-request-state.cc):539</span><br><span class="line">----Exec(coordinator.cc):167</span><br><span class="line">-----InitBackendStates(coordinator.cc)</span><br><span class="line">----Exec(coordinator.cc):181</span><br><span class="line">-----StartBackendExec(coordinator.cc):487</span><br><span class="line">------ExecAsync(coordinator-backend-state.cc):246</span><br><span class="line">-------SetRpcParams(coordinator-backend-state.cc):125-163</span><br></pre></td></tr></table></figure>
<p>上面这个函数调用栈比较长，而且涉及到的过程也比较复杂，这里我们就不一一展开解释。我们需要知道的是：TExecRequest中包含的这些ScanRange会被分配到各个executor上，每个executor对应的相关信息都被封装为一个BackendState对象，每个BackendState对象都包含一个BackendExecParams成员，这里就封装了ScanRange的相关信息，最终通过BackendState::ExecAsync函数在每个executor上执行真正的scan操作。我们将上述整个过程中涉及到的一些主要对象归纳为一张图，如下所示：<br><img src="https://img-blog.csdnimg.cn/20210416105228108.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70#pic_center" alt="3"><br>其中绿色部分表示的是typedef，比如PerNodeScanRanges对应的就是map&lt;TPlanNodeId, std::vector<TScanRangeParams>&gt;，黄色的部分表示的是当前这个calss/struct包含的一些关键成员，蓝色部分表示的是thrift变量以及包含关系。图中实线表示的是包含关系，箭头所指的是被包含的对象。虚线表示的是构建关系，例如我们通过TExecRequest中的plan_exec_info构造了fragment_exec_params遍变量。<br>最终，我们通过BackendState::SetRpcParams方法，将BackendState对象的相关信息封装成为TExecPlanFragmentInfo，然后发送到对应的executor进行实际的扫描。需要注意的是，每个BackendState的构造是在coordinator上进行的，而实际的scan操作是在各个executor上进行的。</p>
<h4 id="关于BackendState"><a href="#关于BackendState" class="headerlink" title="关于BackendState"></a>关于BackendState</h4><p>我们上面提到，每个executor需要的信息都会被封装成一个BackendState对象，每一个BackendState对象中，包含ScanRange信息的成员变量就是backend_exec_params_。这个变量是一个BackendExecParams的类型，可以通过上面的关系图追踪到相关的信息。为了方便理解，我们在源码中增加如下所示的DEBUG代码，可以看到整个查询的BackendState分布情况：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;在Coordinator::StartBackendExec()中进行增加</span><br><span class="line">  stringstream ss;</span><br><span class="line">  for (BackendState* backend_state: backend_states_) &#123;</span><br><span class="line">    ss &lt;&lt; &quot;Netease::BackendState: &quot; &lt;&lt; backend_state-&gt;impalad_address().hostname &lt;&lt; &quot;:&quot;</span><br><span class="line">        &lt;&lt; backend_state-&gt;impalad_address().port &lt;&lt; endl;</span><br><span class="line">    for(const FInstanceExecParams* params : backend_state-&gt;exec_params()-&gt;instance_params) &#123;</span><br><span class="line">        sss &lt;&lt; &quot;Netease::FInstanceExecParams: &quot; &lt;&lt; PrintId(params-&gt;instance_id) &lt;&lt; &quot; &quot;</span><br><span class="line">            &lt;&lt; params-&gt;host.hostname &lt;&lt; &quot;:&quot; &lt;&lt; params-&gt;host.port &lt;&lt; endl;</span><br><span class="line">        PerNodeScanRanges::const_iterator iter &#x3D; params-&gt;per_node_scan_ranges.begin();</span><br><span class="line">        while (iter !&#x3D; params-&gt;per_node_scan_ranges.end()) &#123;</span><br><span class="line">          vector&lt;TScanRangeParams&gt; scVector &#x3D; iter-&gt;second;</span><br><span class="line">          sss &lt;&lt; &quot;Netease::PlanId: &quot; &lt;&lt; iter-&gt;first &lt;&lt; &quot;, ScanRange Size: &quot;</span><br><span class="line">              &lt;&lt; scVector.size() &lt;&lt; endl;</span><br><span class="line">          iter++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  LOG(INFO) &lt;&lt; ss.str();</span><br></pre></td></tr></table></figure>
<p>其中某个BackendState的结果如下所示，可以看到该BackendState有5个fragment，其中两个包含了HDFS_SCAN，分别有345和16和ScanRange：<br><img src="https://img-blog.csdnimg.cn/20210416105254110.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="4"><br>我们直接使用某个instance id：c5478443d44931cc:767dad4400000003，在profile页面上进行搜到，可以看到该instance下的HDFS_SCAN_NODE对应的counter也是345：<br><img src="https://img-blog.csdnimg.cn/20210416105358406.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="5"></p>
<h4 id="关于ScanRangesComplete"><a href="#关于ScanRangesComplete" class="headerlink" title="关于ScanRangesComplete"></a>关于ScanRangesComplete</h4><p>在Impala的profile中，有一个ScanRangesComplete counter，我们将某个表的所有HDFS_SCAN_NODE中对应的ScanRangesComplete加在一起，就等于上面提到的TScanRangeLocationList对象数量，即232和4816。每个HDFS_SCAN_NODE的ScanRangesComplete，表示分发到这个executor上的ScanRange数量，我们对上面的测试SQL进行统计，如下所示：<br><img src="https://img-blog.csdnimg.cn/20210416105454820.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70#pic_center" alt="6"><br>从上图可以看到，一共有13个executor，分别有两个表的HDFS_SCAN_NODE。因此，我们可以将这个counter，理解为这个executor上操作的ScanRange数量，后续我们还会在提到。</p>
<h4 id="关于PerDiskState对象"><a href="#关于PerDiskState对象" class="headerlink" title="关于PerDiskState对象"></a>关于PerDiskState对象</h4><p>我们在<a href="https://skyyws.github.io/2021/04/16/Impala-HDFS-SCAN-NODE%E4%B9%8BIO-threads%E6%A8%A1%E5%9E%8B/">Impala HDFS_SCAN_NODE之IO threads模型</a>这篇文章中提到，IO thread会先获取一个RequestContext对象，每个对象都包含一个PerDiskState的集合：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;&#x2F; Per disk states to synchronize multiple disk threads accessing the same request</span><br><span class="line">&#x2F;&#x2F;&#x2F; context. One state per IoMgr disk queue.</span><br><span class="line">std::vector&lt;PerDiskState&gt; disk_states_;</span><br></pre></td></tr></table></figure>
<p>根据这个RequestContext对象的类型，获取指定的PerDiskState对象，比如remote hdfs、S3等，每个PerDiskState都包含了多个不同的ScanRange成员变量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class RequestContext::PerDiskState &#123;</span><br><span class="line">  DiskQueue* disk_queue_ &#x3D; nullptr;</span><br><span class="line">  bool done_ &#x3D; true;</span><br><span class="line">  AtomicInt32 is_on_queue_&#123;0&#125;;</span><br><span class="line">  int num_remaining_ranges_ &#x3D; 0;</span><br><span class="line">  InternalQueue&lt;ScanRange&gt; unstarted_scan_ranges_;</span><br><span class="line">  InternalQueue&lt;RequestRange&gt; in_flight_ranges_;</span><br><span class="line">  ScanRange* next_scan_range_to_start_ &#x3D; nullptr;</span><br><span class="line">  AtomicInt32 num_threads_in_op_&#123;0&#125;;</span><br><span class="line">  InternalQueue&lt;WriteRange&gt; unstarted_write_ranges_;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<p>这些成员变量都与Impala的IO thread处理流程紧密相关，下面我们就看下这些成员变量以及相关处理流程。<br>disk_queue_表示该PerDiskState所属的disk queue；done_表示这个RequestContext上的这个disk queue的扫描是否完成了；is_on_queue_表示当前这个RequestContext对象是否在队列上；num_threads_in_op_表示当前正在操作这个RequestContext对象的线程数。<br>当io thread从request_contexts_队列的头部获取一个RequestContext对象之后，就会进行对应的设置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; request-context.cc</span><br><span class="line">  void IncrementDiskThreadAfterDequeue() &#123;</span><br><span class="line">    &#x2F;&#x2F;&#x2F; Incrementing &#39;num_threads_in_op_&#39; first so that there is no window when other</span><br><span class="line">    &#x2F;&#x2F;&#x2F; threads see &#39;is_on_queue_ &#x3D;&#x3D; num_threads_in_op_ &#x3D;&#x3D; 0&#39; and think there are no</span><br><span class="line">    &#x2F;&#x2F;&#x2F; references left to this context.</span><br><span class="line">    num_threads_in_op_.Add(1);</span><br><span class="line">    is_on_queue_.Store(0);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>将num_threads_in_op_+1，然后is_on_queue_设置为0，表示该RequestContext对象已经不在队列中。当我们获取了对应的ScanRange之后，就会将is_on_queue_设置为1，并将RequestContext对象放到队尾，此时其他的io thread就可以有机会再次获取这个RequestContext对象进行处理：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; request-context.cc</span><br><span class="line">void RequestContext::PerDiskState::ScheduleContext(const unique_lock&lt;mutex&gt;&amp; context_lock,</span><br><span class="line">    RequestContext* context, int disk_id) &#123;</span><br><span class="line">  DCHECK(context_lock.mutex() &#x3D;&#x3D; &amp;context-&gt;lock_ &amp;&amp; context_lock.owns_lock());</span><br><span class="line">  if (is_on_queue_.Load() &#x3D;&#x3D; 0 &amp;&amp; !done_) &#123;</span><br><span class="line">    is_on_queue_.Store(1);</span><br><span class="line">    disk_queue_-&gt;EnqueueContext(context);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当我们处理完对应的ScanRange之后，才会将num_threads_in_op_减1，表示这个IO thread的本次处理已经完成。接着就会循环处理队列中的下一个RequestContext对象。<br>这里我们简单介绍了PerDiskState的几个成员变量，还有剩下的几个，例如unstarted_scan_ranges_、in_flight_ranges_等，相对比较复杂，由于篇幅原因，我们将在后续的文章中继续进行探究。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://skyyws.github.io/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8BScanRange%E8%AF%A6%E8%A7%A3%EF%BC%88%E4%B8%89%EF%BC%89/" data-id="cknk7yb0m0000zpx59363667q" data-title="Impala 3.4 SQL查询之ScanRange详解（三）" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Impala-SQL%E6%9F%A5%E8%AF%A2%E7%B3%BB%E5%88%97/" rel="tag">Impala SQL查询系列</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/olap/" rel="tag">olap</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Impala-HDFS-SCAN-NODE之AverageHdfsReadThreadConcurrency" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/16/Impala-HDFS-SCAN-NODE%E4%B9%8BAverageHdfsReadThreadConcurrency/" class="article-date">
  <time class="dt-published" datetime="2021-04-16T11:15:23.000Z" itemprop="datePublished">2021-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/16/Impala-HDFS-SCAN-NODE%E4%B9%8BAverageHdfsReadThreadConcurrency/">Impala HDFS_SCAN_NODE之AverageHdfsReadThreadConcurrency</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>在Impala的HDFS_SCAN_NODE中有一个counter，叫AverageHdfsReadThreadConcurrency，其相关解释如下所示：<br><img src="https://img-blog.csdnimg.cn/2021040815143591.png" alt="1"><br>简单理解为，这个值越高，那么同时参与hdfs scan的线程就会越多，在一定程度上，扫描就会更快；如果这个值比较小，那么就有可能是当前的查询比较多，导致线程被其他scan node给占用了。本文就结合代码来跟大家一起学习下，这个couter是如何计算和更新的。<br>关于这个Counter的初始化代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; hdfs-scan-node-base.h</span><br><span class="line">RuntimeProfile::Counter* average_hdfs_read_thread_concurrency_ &#x3D; nullptr;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; hdfs-scan-node-base.cc</span><br><span class="line">average_hdfs_read_thread_concurrency_ &#x3D;</span><br><span class="line">PROFILE_AverageHdfsReadThreadConcurrency.Instantiate(runtime_profile(),</span><br><span class="line">&amp;active_hdfs_read_thread_counter_);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; runtime-profile-counters.h</span><br><span class="line">#define PROFILE_DEFINE_SAMPLING_COUNTER(name, significance, desc) \</span><br><span class="line">  ::impala::SamplingCounterPrototype PROFILE_##name( \</span><br><span class="line">      #name, ::impala::ProfileEntryPrototype::Significance::significance, desc)</span><br></pre></td></tr></table></figure>
<p>上面这几行代码，首先通过一个宏定义，将“AverageHdfsReadThreadConcurrency”绑定到了一个<br>SamplingCounterPrototype，即PROFILE_AverageHdfsReadThreadConcurrency。然后利用这个Prototype来实例化产生SamplingCounter。关于Instantiate函数的具体实现，我们接着往下看：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; runtime-profile-counters.h</span><br><span class="line">  RuntimeProfile::Counter* Instantiate(RuntimeProfile* profile,</span><br><span class="line">      RuntimeProfile::Counter* src_counter) &#123;</span><br><span class="line">    return profile-&gt;AddSamplingCounter(name(), src_counter);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">RuntimeProfile::Counter* RuntimeProfile::AddSamplingCounter(</span><br><span class="line">    const string&amp; name, Counter* src_counter) &#123;</span><br><span class="line">  DCHECK(src_counter-&gt;unit() &#x3D;&#x3D; TUnit::UNIT);</span><br><span class="line">  lock_guard&lt;SpinLock&gt; l(counter_map_lock_);</span><br><span class="line">  bool created;</span><br><span class="line">  Counter* dst_counter &#x3D; AddCounterLocked(name, TUnit::DOUBLE_VALUE, &quot;&quot;, &amp;created);</span><br><span class="line">  if (!created) return dst_counter;</span><br><span class="line">  sampling_counters_.push_back(dst_counter);</span><br><span class="line">  PeriodicCounterUpdater::RegisterPeriodicCounter(src_counter, NULL, dst_counter,</span><br><span class="line">      PeriodicCounterUpdater::SAMPLING_COUNTER);</span><br><span class="line">  has_active_periodic_counters_ &#x3D; true;</span><br><span class="line">  return dst_counter;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在Instantiate函数中，主要就是调用了一个AddSamplingCounter函数，这个函数首先将当前的这个counter保存到名为sampling_counters_的vector中，这个vector后续会用来控制停止这些counter的采集、更新，后面会再提到。接着会将active_hdfs_read_thread_counter_和AverageHdfsReadThreadConcurrency通过RegisterPeriodicCounter函数，注册为一个SAMPLING_COUNTER类型的PeriodicCounter。如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; periodic-counter-updater.cc</span><br><span class="line">void PeriodicCounterUpdater::RegisterPeriodicCounter(</span><br><span class="line">    RuntimeProfile::Counter* src_counter,</span><br><span class="line">    RuntimeProfile::SampleFunction sample_fn,</span><br><span class="line">    RuntimeProfile::Counter* dst_counter, PeriodicCounterType type) &#123;</span><br><span class="line">  DCHECK(src_counter &#x3D;&#x3D; NULL || sample_fn &#x3D;&#x3D; NULL);</span><br><span class="line"></span><br><span class="line">  switch (type) &#123;</span><br><span class="line">    case RATE_COUNTER: &#123;</span><br><span class="line">      &#x2F;&#x2F; 省略部分代码</span><br><span class="line">    &#125;</span><br><span class="line">    case SAMPLING_COUNTER: &#123;</span><br><span class="line">      SamplingCounterInfo counter;</span><br><span class="line">      counter.src_counter &#x3D; src_counter;</span><br><span class="line">      counter.sample_fn &#x3D; sample_fn;</span><br><span class="line">      counter.num_sampled &#x3D; 0;</span><br><span class="line">      counter.total_sampled_value &#x3D; 0;</span><br><span class="line">      lock_guard&lt;SpinLock&gt; samplinglock(instance_-&gt;sampling_lock_);</span><br><span class="line">      instance_-&gt;sampling_counters_[dst_counter] &#x3D; counter;</span><br><span class="line">      break;</span><br><span class="line">    &#125;</span><br><span class="line">    default:</span><br><span class="line">      DCHECK(false) &lt;&lt; &quot;Unsupported PeriodicCounterType:&quot; &lt;&lt; type;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从上述代码我们可以看到，active_hdfs_read_thread_counter_被包装为了一个SamplingCounterInfo，这里主要保存的指标有两个：total_sampled_value和num_sampled，分别表示采集的value总和、采集次数。请注意，这里对应的是active_hdfs_read_thread_counter_这个counter的采集数据，而不是AverageHdfsReadThreadConcurrency。<br>所有的SAMPLING_COUNTER都会保存在一个名为sampling_counters_的map中，这个map的key对应的就是我们这里的AverageHdfsReadThreadConcurrency，而value则是一个SamplingCounterInfo，里面包含一个src的counter，表示数据采集的来源，在这里就是active_hdfs_read_thread_counter_。在启动impalad之后，会专门启动一个线程来定时处理这些SAMPLING_COUNTER，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; periodic-counter-updater.cc</span><br><span class="line">void PeriodicCounterUpdater::Init() &#123;</span><br><span class="line">  DCHECK(instance_ &#x3D;&#x3D; nullptr);</span><br><span class="line">  &#x2F;&#x2F; Create the singleton, which will live until the process terminates.</span><br><span class="line">  instance_ &#x3D; new PeriodicCounterUpdater;</span><br><span class="line">  instance_-&gt;update_thread_.reset(</span><br><span class="line">      new thread(&amp;PeriodicCounterUpdater::UpdateLoop, instance_));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于这些PeriodicCounter更新的处理逻辑都在UpdateLoop这个函数里面，除了SamplingCounter之外，还有RatingCounter、BucketingCounter等，这里我们关注下SamplingCounter的处理：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">void PeriodicCounterUpdater::UpdateLoop() &#123;</span><br><span class="line">  while (true) &#123;</span><br><span class="line">    &#123;</span><br><span class="line">      lock_guard&lt;SpinLock&gt; samplinglock(instance_-&gt;sampling_lock_);</span><br><span class="line">      for (SamplingCounterMap::iterator it &#x3D; sampling_counters_.begin();</span><br><span class="line">           it !&#x3D; sampling_counters_.end(); ++it) &#123;</span><br><span class="line">        ++it-&gt;second.num_sampled;</span><br><span class="line">        int64_t value;</span><br><span class="line">        if (it-&gt;second.src_counter !&#x3D; NULL) &#123;</span><br><span class="line">          value &#x3D; it-&gt;second.src_counter-&gt;value();</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          DCHECK(it-&gt;second.sample_fn !&#x3D; NULL);</span><br><span class="line">          value &#x3D; it-&gt;second.sample_fn();</span><br><span class="line">        &#125;</span><br><span class="line">        it-&gt;second.total_sampled_value +&#x3D; value;</span><br><span class="line">        double average &#x3D; static_cast&lt;double&gt;(it-&gt;second.total_sampled_value) &#x2F;</span><br><span class="line">            it-&gt;second.num_sampled;</span><br><span class="line">        it-&gt;first-&gt;Set(average);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F;省略其余代码</span><br></pre></td></tr></table></figure>
<p>从上面这段代码可以看到，每次采集更新的时候，active_hdfs_read_thread_counter_的total_sampled_value和num_sampled就会进行更新、累加。并且first的值（这里就是AverageHdfsReadThreadConcurrency）也会被更新为最新的average，即total_sampled_value/num_sampled。所以，即使查询正在执行中，如果我们刷新profile，也可以得到最新的average。<br>值得一提的是，采集间隔可以通过一个参数来进行配置，默认是500ms：<br><img src="https://img-blog.csdnimg.cn/20210408151519406.png" alt="2"><br>通过上面关于AverageHdfsReadThreadConcurrency这个counter的计算、更新介绍，我们都知道与active_hdfs_read_thread_counter_这个counter有关，下面我们就来看下这个变量是如何更新的。相关代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; hdfs-scan-node-base.h</span><br><span class="line">&#x2F;&#x2F;&#x2F; The number of active hdfs reading threads reading for this node.</span><br><span class="line">RuntimeProfile::Counter active_hdfs_read_thread_counter_;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; request_context.h</span><br><span class="line">&#x2F;&#x2F;&#x2F; Number of active read threads</span><br><span class="line">RuntimeProfile::Counter* active_read_thread_counter_ &#x3D; nullptr;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; </span><br><span class="line">reader_context_-&gt;set_active_read_thread_counter(&amp;active_hdfs_read_thread_counter_);</span><br></pre></td></tr></table></figure>
<p>active_hdfs_read_thread_counter_这个counter，我们通过注释可以知道，表示的是当前这个hdfs scan node活跃的io threads数量。在构造hdfs scan node的时候，将这个counter设置到RequestContext的active_read_thread_counter_。因此，我们目前的关注点就转换到了active_read_thread_counter_这个变量上。在上一篇文章中，我们提到了关于RequestContext和ScanRange的相关情况，没看过的读者可以简单浏览下：<a href="https://skyyws.github.io/2021/04/16/Impala-HDFS-SCAN-NODE%E4%B9%8BIO-threads%E6%A8%A1%E5%9E%8B">Impala HDFS_SCAN_NODE之IO threads模型</a>。在这篇文章中，我们提到了：io thread会首先从RequestContext队列中获取头部元素，接着通过该RequestContext对象获取一个ScanRange。相关调用栈如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DiskThreadLoop(disk-io-mrg.cc)</span><br><span class="line">-GetNextRequestRange(disk-io-mrg.cc)</span><br><span class="line">--GetNextRequestRange(request-context.cc)</span><br><span class="line">-DoRead(scan-range.cc)</span><br></pre></td></tr></table></figure>
<p>在DoRead方法中，就会对active_read_thread_counter_进行加减操作，这里我们只展示相关的代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; scan-ranger.cc</span><br><span class="line">ReadOutcome ScanRange::DoRead(DiskQueue* queue, int disk_id) &#123;</span><br><span class="line">  &#x2F;&#x2F;省略其余代码</span><br><span class="line">  Status read_status &#x3D; file_reader_-&gt;Open(use_file_handle_cache);</span><br><span class="line">  bool eof &#x3D; false;</span><br><span class="line">  if (read_status.ok()) &#123;</span><br><span class="line">    COUNTER_ADD_IF_NOT_NULL(reader_-&gt;active_read_thread_counter_, 1L);</span><br><span class="line">    COUNTER_BITOR_IF_NOT_NULL(reader_-&gt;disks_accessed_bitmap_, 1LL &lt;&lt; disk_id);</span><br><span class="line">    DebugScanRangeInfo();</span><br><span class="line"></span><br><span class="line">    if (sub_ranges_.empty()) &#123;</span><br><span class="line">      DCHECK(cache_.data &#x3D;&#x3D; nullptr);</span><br><span class="line">      read_status &#x3D; file_reader_-&gt;ReadFromPos(queue, offset_ + bytes_read_,</span><br><span class="line">          buffer_desc-&gt;buffer_,</span><br><span class="line">          min(bytes_to_read() - bytes_read_, buffer_desc-&gt;buffer_len_),</span><br><span class="line">          &amp;buffer_desc-&gt;len_, &amp;eof);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      read_status &#x3D; ReadSubRanges(queue, buffer_desc.get(), &amp;eof);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    COUNTER_ADD_IF_NOT_NULL(reader_-&gt;bytes_read_counter_, buffer_desc-&gt;len_);</span><br><span class="line">    COUNTER_ADD_IF_NOT_NULL(reader_-&gt;active_read_thread_counter_, -1L);</span><br><span class="line">  &#125;</span><br><span class="line">  &#x2F;&#x2F;省略其余代码</span><br></pre></td></tr></table></figure>
<p>当获取到指定的ScanRange之后，会首先调用Open方法打开文件，如果打开成功的话，则active_read_thread_counter_就会加1，表示当前已经有一个线程正在对某个ScanRange进行扫描操作。接着就会执行实际的扫描操作，关于hdfs file的扫描不是本身关注的重点，这里就不再展开描述。扫描完成之后，active_read_thread_counter_就会减1，表示这个线程对于ScanRange的扫描已经结束了。通过这些代码分析，我们可以知道，对于active_read_thread_counter_，就可以理解为当前有多少个io thread正在扫描ScanRange，而AverageHdfsReadThreadConcurrency表示的就是：某个hdfs scan node从开始执行到当前时间点为止，平均io thread并发数（采集到io thread总数/采集次数），这个值越大，表示同一时间，用于处理ScanRange的线程数就越多，相应的hdfs scan就会越快（这里指的是io thread scan阶段，不包括后续的scanner处理阶段）。如果这个值比较小的话，那么说明同时处理ScanRange的线程数就很少，那么可能就会导致扫描很慢，进而表现为整个的hdfs scan node节点很慢。<br>除了我们上面介绍的AverageHdfsReadThreadConcurrency这个counter，还有一个counter也值得看一下，即“Hdfs Read Thread Concurrency Bucket”，如下所示：<br><img src="https://img-blog.csdnimg.cn/2021040815155619.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="3"><br>这个counter与AverageHdfsReadThreadConcurrency有一定的联系，我们同样从代码层面看下该counter是如何进行计算的。相关函数初始化代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; hdfs-scan-node-hbase.h</span><br><span class="line">&#x2F;&#x2F;&#x2F; HDFS read thread concurrency bucket: bucket[i] refers to the number of sample</span><br><span class="line">&#x2F;&#x2F;&#x2F; taken where there are i concurrent hdfs read thread running. Created in Open().</span><br><span class="line">std::vector&lt;RuntimeProfile::Counter*&gt;* hdfs_read_thread_concurrency_bucket_ &#x3D; nullptr;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; hdfs-scan-node-hbase.cc</span><br><span class="line">hdfs_read_thread_concurrency_bucket_ &#x3D; runtime_profile()-&gt;AddBucketingCounters(</span><br><span class="line">&amp;active_hdfs_read_thread_counter_,</span><br><span class="line">ExecEnv::GetInstance()-&gt;disk_io_mgr()-&gt;num_total_disks() + 1);</span><br></pre></td></tr></table></figure>
<p>active_hdfs_read_thread_counter_被绑定到了一个BucketingCounter，其中桶的数量就是disk_queues_.size()+1，以上述截图为例：机器上有3块本地磁盘，REMOTE_NUM_DISKS的值为5，所以bucket数量为9个，序号是0～8。增加BucketingCounter的流程与上述的SamplingCounter类似，都是先Add，再Register。相关代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; hdfs-scan-node-hbase.cc</span><br><span class="line">&#x2F;&#x2F; 这里的bucketing_counters_是一个set，set中的每个元素都是一个vector</span><br><span class="line">vector&lt;RuntimeProfile::Counter*&gt;* RuntimeProfile::AddBucketingCounters(</span><br><span class="line">    Counter* src_counter, int num_buckets) &#123;</span><br><span class="line">  lock_guard&lt;SpinLock&gt; l(counter_map_lock_);</span><br><span class="line">  vector&lt;RuntimeProfile::Counter*&gt;* buckets &#x3D; pool_-&gt;Add(new vector&lt;Counter*&gt;);</span><br><span class="line">  for (int i &#x3D; 0; i &lt; num_buckets; ++i) &#123;</span><br><span class="line">      buckets-&gt;push_back(</span><br><span class="line">          pool_-&gt;Add(new RuntimeProfile::Counter(TUnit::DOUBLE_VALUE, 0)));</span><br><span class="line">  &#125;</span><br><span class="line">  bucketing_counters_.insert(buckets);</span><br><span class="line">  has_active_periodic_counters_ &#x3D; true;</span><br><span class="line">  PeriodicCounterUpdater::RegisterBucketingCounters(src_counter, buckets);</span><br><span class="line">  return buckets;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; periodic-counter-updater.cc</span><br><span class="line">&#x2F;&#x2F; 这里的bucketing_counters_是一个map，key是一个vector，value是一个BucketCountersInfo</span><br><span class="line">void PeriodicCounterUpdater::RegisterBucketingCounters(</span><br><span class="line">    RuntimeProfile::Counter* src_counter, vector&lt;RuntimeProfile::Counter*&gt;* buckets) &#123;</span><br><span class="line">  BucketCountersInfo info;</span><br><span class="line">  info.src_counter &#x3D; src_counter;</span><br><span class="line">  info.num_sampled &#x3D; 0;</span><br><span class="line">  lock_guard&lt;SpinLock&gt; bucketinglock(instance_-&gt;bucketing_lock_);</span><br><span class="line">  instance_-&gt;bucketing_counters_[buckets] &#x3D; info;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>先初始化一个vector，成员数量就是bucket数，每一个成员都是一个counter，这些counter的值都初始化为0，接着将这个vector保存到bucketing_counters_中，这里的bucketing_counters_也是用于控制后续的counter停止采集。然后我们再进行注册（类似上面的注册PeriodicCounter）。在进行注册的时候，首先会构造一个BucketCountersInfo来封装active_hdfs_read_thread_counter_，然后将这个info保存到bucketing_counters_中，这里的bucketing_counters_同样是一个map，map的key就是一个vector，比如上面代码中的buckets变量，而value则是一个BucketCountersInfo。之后线程就会通过UpdateLoop函数来循环处理bucketing_counters_，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; periodic-counter-updater.cc</span><br><span class="line">&#x2F;&#x2F; 与上面的sampling_counters_处理在同一个函数中</span><br><span class="line">void PeriodicCounterUpdater::UpdateLoop() &#123;</span><br><span class="line">  &#x2F;&#x2F;省略其余代码</span><br><span class="line">      &#123;</span><br><span class="line">      lock_guard&lt;SpinLock&gt; bucketinglock(instance_-&gt;bucketing_lock_);</span><br><span class="line">      for (BucketCountersMap::iterator it &#x3D; bucketing_counters_.begin();</span><br><span class="line">           it !&#x3D; bucketing_counters_.end(); ++it) &#123;</span><br><span class="line">        int64_t val &#x3D; it-&gt;second.src_counter-&gt;value();</span><br><span class="line">        if (val &gt;&#x3D; it-&gt;first-&gt;size()) val &#x3D; it-&gt;first-&gt;size() - 1;</span><br><span class="line">        it-&gt;first-&gt;at(val)-&gt;Add(1);</span><br><span class="line">        ++it-&gt;second.num_sampled;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F;省略其余代码</span><br></pre></td></tr></table></figure>
<p>上述代码的处理逻辑就是：获取当前active_hdfs_read_thread_counter_的值（即并发处理ScanRange的线程数量）保存为val，判断该值是否大于等于bucket数量（这里是9）。如果是的话，则将val更新为bucket数量减1，否则直接使用val。然后将vector中下标为val的counter加1。最后更新采集次数。基于上述的代码处理，笔者对于这个BucketingCounter的理解是：一共划分成disk_queues_.size()+1个bucket，序号从0～disk_queues_.size()，每个bucket对应的下标表示线程数。如果当前采集的线程数小于bucket数，则直接将下标对应的budcket进行累加；如果大于等于bucket数，则全部累加到下标为disk_queues_.size()的budcket，即最后一个bucket。也就是说用来统计各个线程并发数的比例，当并发线程数大于等于bucket数的时候，全部放到最后一个桶。但是为什么初始化的时候，设置disk_queues_.size()+1个bucket，笔者目前也没有完全弄清楚。<br>上面我们只是统计了bucket对应的线程数的出现次数，最后我们还需要再计算一个百分比，相关代码处理如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; periodic-counter-updater.cc</span><br><span class="line">void PeriodicCounterUpdater::StopBucketingCounters(</span><br><span class="line">    vector&lt;RuntimeProfile::Counter*&gt;* buckets) &#123;</span><br><span class="line">  &#x2F;&#x2F;省略其余代码</span><br><span class="line">  if (num_sampled &gt; 0) &#123;</span><br><span class="line">    for (RuntimeProfile::Counter* counter : *buckets) &#123;</span><br><span class="line">      double perc &#x3D; 100 * counter-&gt;value() &#x2F; (double)num_sampled;</span><br><span class="line">      counter-&gt;Set(perc);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>我们可以看到，这里主要就是遍历每个bucket对应的counter，然后用当前counter的累加值除以总的采样次数，就是该counter的占比。当scan node结束之后，就会停止所有的PeriodicCounter，包括SamplingCounter、RateCounter、BucketingCounter等，就会调用上述的函数。相关调用栈如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Close(hdfs-scan-node.cc)</span><br><span class="line">-StopAndFinalizeCounters(hdfs-scan-node.cc)</span><br><span class="line">--StopPeriodicCounters(runtime-profile.cc)</span><br><span class="line">---StopSamplingCounter(periodic-counter-updater.cc)</span><br><span class="line">---StopRateCounter(periodic-counter-updater.cc)</span><br><span class="line">---StopBucketingCounters(periodic-counter-updater.cc)</span><br></pre></td></tr></table></figure>
<p> 前面我们提到了sampling_counters_和bucketing_counters_这两个list集合是用来控制counter的停止采集，这里就是在StopPeriodicCounters方法中，通过for循环遍历，来逐个停掉这些counter的采集。<br>上面我们讲了hdfs_read_thread_concurrency_bucket_这个BucketingCounter的更新和计算，下面我们来看下最终是如何输出到Profile的，相关代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; hdfs-scan-node-base.cc</span><br><span class="line">&#x2F;&#x2F; Output hdfs read thread concurrency into info string</span><br><span class="line">stringstream ss;</span><br><span class="line">for (int i &#x3D; 0; i &lt; hdfs_read_thread_concurrency_bucket_-&gt;size(); ++i) &#123;</span><br><span class="line">  ss &lt;&lt; i &lt;&lt; &quot;:&quot; &lt;&lt; setprecision(4)</span><br><span class="line">     &lt;&lt; (*hdfs_read_thread_concurrency_bucket_)[i]-&gt;double_value() &lt;&lt; &quot;% &quot;;</span><br><span class="line">&#125;</span><br><span class="line">runtime_profile_-&gt;AddInfoString(&quot;Hdfs Read Thread Concurrency Bucket&quot;, ss.str());</span><br></pre></td></tr></table></figure>
<p>这段代码比较简单，就是循环打印每个bucket对应的counter中保存的value，就是百分比，最终拼接成一个字符串输出即可。<br>到这里，关于AverageHdfsReadThreadConcurrency这个counter以及“Hdfs Read Thread Concurrency Bucket”我们就介绍的差不多了。本文的介绍都是笔者基于代码的个人理解，如有问题，欢迎指正。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://skyyws.github.io/2021/04/16/Impala-HDFS-SCAN-NODE%E4%B9%8BAverageHdfsReadThreadConcurrency/" data-id="cknk7un7d0000w7x51hw36cho" data-title="Impala HDFS_SCAN_NODE之AverageHdfsReadThreadConcurrency" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Impala%E4%B9%8BHDFS-SCAN-NODE/" rel="tag">Impala之HDFS_SCAN_NODE</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/olap/" rel="tag">olap</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Impala-HDFS-SCAN-NODE之IO-threads模型" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/16/Impala-HDFS-SCAN-NODE%E4%B9%8BIO-threads%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time class="dt-published" datetime="2021-04-16T11:10:43.000Z" itemprop="datePublished">2021-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/16/Impala-HDFS-SCAN-NODE%E4%B9%8BIO-threads%E6%A8%A1%E5%9E%8B/">Impala HDFS_SCAN_NODE之IO threads模型</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>本文主要从代码出发，跟大家一起分享下Impala HDFS_SCAN_NODE中的IO threads模型。首先，在Impala中，有几个io threads相关的配置，通过对这几个参数进行配置，我们就可以增加处理io的线程数，相关的几个配置如下所示：<br><img src="https://img-blog.csdnimg.cn/20210331144325103.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="1"><br>以我们最常见的hdfs存储引擎为例，如果impalad节点与datanode节点在一台机器上，对于impala来说，就是可以通过本地的disk直接读取数据；如果impalad节点与datanode在不同的机器上，那么就是remote的读取。在我们内部的生产环境，大部分都是这样的情况：有一个公共的HDFS集群，业务所有的离线数据都存储在上面，我们需要单独部署一个Impala集群，对于HDFS集群上的某些数据进行Ad-hoc类的多维分析，此时impala就是通过remote来读取hdfs的数据，那么将num_remote_hdfs_io_threads配置项调整的大一些，就可以适当地加快hdfs scan的速度。<br>在正式开启介绍之前，我们需要知道Impala的scan node模型分为两层：1）IO threads，这层主要就是通过IO读取远端的hdfs数据，并且返回，通过配置num_remote_hdfs_io_threads参数，就可以调整读取的线程数，值得一提的是，一些谓词可以下推到远端的hdfs，减少扫描返回的数据量；2）Scanner，当数据从远端的HDFS返回之后，会由专门的scanner线程进行处理，可能的操作包括：数据解码、cast计算等。本文我们主要讲的就是第一层IO threads，其他更多的介绍可以参考：<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/63405729/why-impala-scan-node-is-very-slow-rowbatchqueuegetwaittime">Why Impala Scan Node is very slow</a>中Tim Armstrong的回答，这篇CSDN的博客也有介绍：<a target="_blank" rel="noopener" href="https://blog.csdn.net/yu616568/article/details/74996897?spm=1001.2014.3001.5501">Impala高性能探秘之HDFS数据访问</a>。<br>下面，我们就结合代码来简单看下这个参数是如何起作用的。在Impala的BE代码中，有一个类专门用来管理IO相关的操作，用于访问本地磁盘或者远端的文件系统，即DiskIoMgr。在这个类中，有一个disk_queues_成员，这是一个集合，每个成员都代表一个disk对应的队列，或者是一种远端文件系统，例如HDFS/S3等，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; disk-io-mrg.h</span><br><span class="line">  &#x2F;&#x2F;&#x2F; Per disk queues. This is static and created once at Init() time.  One queue is</span><br><span class="line">  &#x2F;&#x2F;&#x2F; allocated for each local disk on the system and for each remote filesystem type.</span><br><span class="line">  &#x2F;&#x2F;&#x2F; It is indexed by disk id.</span><br><span class="line">  std::vector&lt;DiskQueue*&gt; disk_queues_;</span><br></pre></td></tr></table></figure>
<p>首先会在构造函数中，对这个变量进行resize操作，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; disk-io-mrg.cc</span><br><span class="line">disk_queues_.resize(num_local_disks + REMOTE_NUM_DISKS);</span><br></pre></td></tr></table></figure>
<p>这里的num_local_disks指的就是本地磁盘的个数，而REMOTE_NUM_DISKS就是一个enum变量，用来控制远端访问的偏移：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; disk-io-mrg.h</span><br><span class="line">  &#x2F;&#x2F;&#x2F; &quot;Disk&quot; queue offsets for remote accesses.  Offset 0 corresponds to</span><br><span class="line">  &#x2F;&#x2F;&#x2F; disk ID (i.e. disk_queue_ index) of num_local_disks().</span><br><span class="line">  enum &#123;</span><br><span class="line">    REMOTE_DFS_DISK_OFFSET &#x3D; 0,</span><br><span class="line">    REMOTE_S3_DISK_OFFSET,</span><br><span class="line">    REMOTE_ADLS_DISK_OFFSET,</span><br><span class="line">    REMOTE_ABFS_DISK_OFFSET,</span><br><span class="line">    REMOTE_OZONE_DISK_OFFSET,</span><br><span class="line">    REMOTE_NUM_DISKS</span><br><span class="line">  &#125;;</span><br></pre></td></tr></table></figure>
<p>所以，impala将每一种远端的文件系统访问，也当成了一个disk，按照上述的enum顺序，放到disk_queues_中，作为一个成员变量。接着在Init函数中，会循环对这个disk_queues_变量进行初始化：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; disk-io-mrg.cc</span><br><span class="line">  for (int i &#x3D; 0; i &lt; disk_queues_.size(); ++i) &#123;</span><br><span class="line">    disk_queues_[i] &#x3D; new DiskQueue(i);</span><br><span class="line">    int num_threads_per_disk;</span><br><span class="line">    string device_name;</span><br><span class="line">    if (i &#x3D;&#x3D; RemoteDfsDiskId()) &#123;</span><br><span class="line">      num_threads_per_disk &#x3D; FLAGS_num_remote_hdfs_io_threads;</span><br><span class="line">      device_name &#x3D; &quot;HDFS remote&quot;;</span><br></pre></td></tr></table></figure>
<p>在整个for循环中，会根据id来判断是需要对哪一个队列进行操作，这里以HDFS为例，id就是本地磁盘的数量+HDFS在enum中的offset：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; disk-io-mrg.cc</span><br><span class="line">  &#x2F;&#x2F;&#x2F; The disk ID (and therefore disk_queues_ index) used for DFS accesses.</span><br><span class="line">  int RemoteDfsDiskId() const &#123; return num_local_disks() + REMOTE_DFS_DISK_OFFSET; &#125;</span><br></pre></td></tr></table></figure>
<p>如果是要访问远端的HDFS，那么对应的线程数量，即num_threads_per_disk，就是我们通过配置文件指定的num_remote_hdfs_io_threads的值，默认是8。表示会启动8个线程用于处理远端的HDFS访问操作。接着，impala就会循环创建对应数量的线程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; disk-io-mrg.cc</span><br><span class="line">    for (int j &#x3D; 0; j &lt; num_threads_per_disk; ++j) &#123;</span><br><span class="line">      stringstream ss;</span><br><span class="line">      ss &lt;&lt; &quot;work-loop(Disk: &quot; &lt;&lt; device_name &lt;&lt; &quot;, Thread: &quot; &lt;&lt; j &lt;&lt; &quot;)&quot;;</span><br><span class="line">      std::unique_ptr&lt;Thread&gt; t;</span><br><span class="line">      RETURN_IF_ERROR(Thread::Create(&quot;disk-io-mgr&quot;, ss.str(), &amp;DiskQueue::DiskThreadLoop,</span><br><span class="line">          disk_queues_[i], this, &amp;t));</span><br><span class="line">      disk_thread_group_.AddThread(move(t));</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>在进行线程创建的时候，将函数DiskQueue::DiskThreadLoop绑定到了该线程上，该函数就是通过一个while循环来不断的进行处理，相关的函数调用如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DiskThreadLoop(disk-io-mrg.cc)</span><br><span class="line">-GetNextRequestRange(disk-io-mrg.cc)</span><br><span class="line">--GetNextRequestRange(request-context.cc)</span><br><span class="line">-DoRead(scan-range.cc)&#x2F;Write(disk-io-mgr.cc)</span><br></pre></td></tr></table></figure>
<p>GetNextRequestRange函数就是用来获取当前这个DiskQueue（例如远端的HDFS访问queue）的下一个RequestRange，来进行具体的io操作。RequestRange代表一个文件中的连续字节序列，主要分为：ScanRange和WriteRange。每个disk线程一次只能处理一个RequestRange。这里impala采用了一个两层的设计，在GetNextRequestRange中，首先会需要获取一个RequestContext对象，RequestContext可以理解为一个查询的某个instance下的所有IO请求集合，可以简单理解为某个表的RequestRange集合都被封装在一个RequestContext对象中。获取RequestContext的代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">*request_context &#x3D; request_contexts_.front();</span><br><span class="line">request_contexts_.pop_front();</span><br><span class="line">DCHECK(*request_context !&#x3D; nullptr);</span><br></pre></td></tr></table></figure>
<p>request_contexts_是一个RequestContext类型的list，每一个DiskQueue都包含了这样一个队列，表示该DiskQueue上的所有的待处理的RequestContext列表。这里我们可以简单的理解为每个表的扫描请求，都在这个队列中等待处理。首先会从队列的头部取出一个RequestContext，然后将该对象弹出。该DiskQueue的其他线程就可以继续处理后续的RequestContext对象，这样就不会因为当前的RequestContext对象处理时间过长，而阻塞了其他的RequestContext对象处理。<br>关于request_contexts_队列成员更新，不是本文介绍的重点，只要知道：当提交查询的时候，impalad会自动进行解析，然后进行封装，最后添加到该队列中即可。<br>在获取到RequestContext对象之后，我们就可以通过该RequestContext的GetNextRequestRange方法获取具体的RequestRange对象进行实际的扫描操作了。<br>上面的描述可能不太容易理解，我们将上述的各个成员之间的包含关系以及操作流程进行了整理成了一张图，如下所示：<br><img src="https://img-blog.csdnimg.cn/20210331144401259.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="2"><br>最终获取到了一个RequestRange之后，会进行判断，是READ还是WRITE，进行相应地处理。这里我们以READ为例，相关函数调用如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DiskThreadLoop(disk-io-mrg.cc)</span><br><span class="line">-GetNextRequestRange(disk-io-mrg.cc)</span><br><span class="line">--GetNextRequestRange(request-context.cc)</span><br><span class="line">-DoRead(scan-range.cc)</span><br><span class="line">-ReadDone(request-context.cc)</span><br></pre></td></tr></table></figure>
<p>从上面的相关代码，我们可以知道，如果我们将num_remote_hdfs_io_threads参数配置的更大一些，那么就会有更多的线程并发的通过DiskThreadLoop获取到RequestRange进行处理，从而可以在一定程度上提到SCAN的速度，进而加快整个查询进程。<br>在Impala的profile中，对于HDFS的IO theads的指标，即AverageHdfsReadThreadConcurrency，相关介绍如下所示：<br><img src="https://img-blog.csdnimg.cn/2021033114441848.png" alt="3"><br>可以简单理解为该HDFS_SCAN_NODE有多少个IO线程用于处于读写请求操作。所以说，如果线上查询的这个指标很小，那么就要考虑适当调整num_remote_hdfs_io_threads这个参数了。与这个指标很相似的是AverageScannerThreadConcurrency，这个表示scanner线程的执行数量，与我前面提到的scan node两层模型中的scanner对应，这个之后再详细介绍。除此之外，还有其他的一些指标，例如ScannerIoWaitTime，表示scanner等到IO线程的数据就绪的时间，如果这个时间很长，那么说明IO线程存在瓶颈。还有很多指标，就不再一一展开描述。我们在线上排查慢查询的时候，这些指标都是非常有用的信息。<br>上面提到了profile中的指标信息。另外，在impala服务启动之后，我们也可以通过web页面上的/threadz页面查看“disk-io-mgr”这个组下面的线程信息，就可以看到用于处理远端HDFS读取的线程：<br><img src="https://img-blog.csdnimg.cn/20210331144429621.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="3"><br>上面的User/Kernel CPU和IO-wait的时间，都是直接从机器上读取的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; os-util.h</span><br><span class="line">&#x2F;&#x2F;&#x2F; Populates ThreadStats object for a given thread by reading from</span><br><span class="line">&#x2F;&#x2F;&#x2F; &#x2F;proc&#x2F;&lt;pid&gt;&#x2F;task&#x2F;&lt;tid&gt;&#x2F;stats. Returns OK unless the file cannot be read or is in an</span><br><span class="line">&#x2F;&#x2F;&#x2F; unrecognised format, or if the kernel version is not modern enough.</span><br><span class="line">Status GetThreadStats(int64_t tid, ThreadStats* stats);</span><br></pre></td></tr></table></figure>
<p>对于每个disk queue，impala还绑定了对应的metric信息，如下所示：<br><img src="https://img-blog.csdnimg.cn/20210331144443304.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="4"><br>这些metric代表的就是读取延时和大小的统计直方图信息。<br>到这里，关于HDFS_SCAN_NODE的IO threads就介绍的差不多了，我们通过代码分析，知道了Impala对于disk以及各种远端dfs的处理，这些都是属于IO threads部分，后续有时间再跟大家一起学习scanner模块的相关知识。本文涉及到的代码分析模块，都是笔者自己根据源码分析解读出来，如有错误，欢迎指正。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://skyyws.github.io/2021/04/16/Impala-HDFS-SCAN-NODE%E4%B9%8BIO-threads%E6%A8%A1%E5%9E%8B/" data-id="cknk7opbs0000okx5gva6gjur" data-title="Impala HDFS_SCAN_NODE之IO threads模型" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Impala%E4%B9%8BHDFS-SCAN-NODE/" rel="tag">Impala之HDFS_SCAN_NODE</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/olap/" rel="tag">olap</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Impala-3-4-SQL查询之重写（二）" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8B%E9%87%8D%E5%86%99%EF%BC%88%E4%BA%8C%EF%BC%89/" class="article-date">
  <time class="dt-published" datetime="2021-04-16T06:21:01.000Z" itemprop="datePublished">2021-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8B%E9%87%8D%E5%86%99%EF%BC%88%E4%BA%8C%EF%BC%89/">Impala 3.4 SQL查询之重写（二）</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>在上一篇文章中，我们介绍了Impala基本的SQL解析流程。本文我们将跟大家一起看下Impala中的一些SQL重写规则。这里，我们首先回顾下关于Analyzer的几个类的关系图，如下所示：<br><img src="https://img-blog.csdnimg.cn/20201229112124401.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="analyzer"></p>
<p>当SQL被解析为特定的StatementBase之后，紧接着会构造一个AnalysisContext对象，这个类可以理解为整个SQL解析过程的封装，包括了：parsing, analyzing and rewriting这几个过程。在AnalysisContext的analyze方法中，我们构造了Analyzer变量，完成了对StatementBase的analyze（在上篇文章中也已经介绍过）。相关函数调用过程如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Frontend.doCreateExecRequest()</span><br><span class="line">-AnalysisContext.analyzeAndAuthorize()</span><br><span class="line">--AnalysisContext.analyze()</span><br><span class="line">---AnalysisContext.createAnalyzer()</span><br><span class="line">----Analyzer.ctor()</span><br><span class="line">-----GlobalState.ctor()</span><br><span class="line">---StatementBase.analyze()</span><br><span class="line">---StatementBase.rewriteExprs()</span><br></pre></td></tr></table></figure>
<p>最终我们在Analyzer.GlobalState的构造函数中，将各种重写规则加入到了Analyzer中，相关代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; Analyzer.java</span><br><span class="line">public GlobalState(StmtTableCache stmtTableCache, TQueryCtx queryCtx,</span><br><span class="line">    AuthorizationFactory authzFactory) &#123;</span><br><span class="line">  this.stmtTableCache &#x3D; stmtTableCache;</span><br><span class="line">  this.queryCtx &#x3D; queryCtx;</span><br><span class="line">  this.authzFactory &#x3D; authzFactory;</span><br><span class="line">  this.lineageGraph &#x3D; new ColumnLineageGraph();</span><br><span class="line">  List&lt;ExprRewriteRule&gt; rules &#x3D; new ArrayList&lt;&gt;();</span><br><span class="line">  &#x2F;&#x2F; BetweenPredicates must be rewritten to be executable. Other non-essential</span><br><span class="line">  &#x2F;&#x2F; expr rewrites can be disabled via a query option. When rewrites are enabled</span><br><span class="line">  &#x2F;&#x2F; BetweenPredicates should be rewritten first to help trigger other rules.</span><br><span class="line">  rules.add(BetweenToCompoundRule.INSTANCE);</span><br><span class="line">  &#x2F;&#x2F; Binary predicates must be rewritten to a canonical form for both Kudu predicate</span><br><span class="line">  &#x2F;&#x2F; pushdown and Parquet row group pruning based on min&#x2F;max statistics.</span><br><span class="line">  rules.add(NormalizeBinaryPredicatesRule.INSTANCE);</span><br><span class="line">  if (queryCtx.getClient_request().getQuery_options().enable_expr_rewrites) &#123;</span><br><span class="line">    rules.add(FoldConstantsRule.INSTANCE);</span><br><span class="line">    rules.add(NormalizeExprsRule.INSTANCE);</span><br><span class="line">    rules.add(ExtractCommonConjunctRule.INSTANCE);</span><br><span class="line">    &#x2F;&#x2F; Relies on FoldConstantsRule and NormalizeExprsRule.</span><br><span class="line">    rules.add(SimplifyConditionalsRule.INSTANCE);</span><br><span class="line">    rules.add(EqualityDisjunctsToInRule.INSTANCE);</span><br><span class="line">    rules.add(NormalizeCountStarRule.INSTANCE);</span><br><span class="line">    rules.add(SimplifyDistinctFromRule.INSTANCE);</span><br><span class="line">    rules.add(SimplifyCastStringToTimestamp.INSTANCE);</span><br><span class="line">  &#125;</span><br><span class="line">  exprRewriter_ &#x3D; new ExprRewriter(rules);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个构造函数里面添加了很多重写规则，这些规则最终都会被应用于SQL的重写中。Impala目前包含了很多重写规则，相关类图如下所示：<br><img src="https://img-blog.csdnimg.cn/20201229112220768.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="rewrite_rule"></p>
<p>所有的重写规则都实现了ExprRewriteRule这个接口，接口本身只包含一个方法apply，接收一个Expr和Analyzer，返回是一个修改之后的Expr。关于Expr，我们在上篇文章中也已经提到了过了，这里就不再展开描述。需要注意的是，Impala还提供了一个query option，叫ENABLE_EXPR_REWRITES，默认为true，会启用更多的重写规则，对于SQL的查询性能提升有很大的帮助。<br>通过上述代码可以看到，在构造GlobalState成员变量的时候，会将所有的重写规则放到一个数组当中，然后构造一个ExprRewriter类，这个类的作用就是：使用重写规则的数组，对指定的Expr进行重写操作。在完成对应的Analyzer构造和StatementBase的解析之后，会调用StatementBase的rewriteExprs方法，来对这个statement的所有Exprs进行重写，这里我们以SelectStmt为例（StatementBase本身是抽象类，并没有实现这个方法），来看一下是如何对Expr进行重写的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; SelectStmt.java</span><br><span class="line">public void rewriteExprs(ExprRewriter rewriter) throws AnalysisException &#123;</span><br><span class="line">  Preconditions.checkState(isAnalyzed());</span><br><span class="line">  selectList_.rewriteExprs(rewriter, analyzer_);</span><br><span class="line">  for (TableRef ref: fromClause_.getTableRefs()) ref.rewriteExprs(rewriter, analyzer_);</span><br><span class="line">  if (whereClause_ !&#x3D; null) &#123;</span><br><span class="line">    whereClause_ &#x3D; rewriter.rewrite(whereClause_, analyzer_);</span><br><span class="line">    &#x2F;&#x2F; Also rewrite exprs in the statements of subqueries.</span><br><span class="line">    List&lt;Subquery&gt; subqueryExprs &#x3D; new ArrayList&lt;&gt;();</span><br><span class="line">    whereClause_.collect(Subquery.class, subqueryExprs);</span><br><span class="line">    for (Subquery s: subqueryExprs) s.getStatement().rewriteExprs(rewriter);</span><br><span class="line">  &#125;</span><br><span class="line">  if (havingClause_ !&#x3D; null) &#123;</span><br><span class="line">    havingClause_ &#x3D; rewriteCheckOrdinalResult(rewriter, havingClause_);</span><br><span class="line">  &#125;</span><br><span class="line">  if (groupingExprs_ !&#x3D; null) &#123;</span><br><span class="line">    for (int i &#x3D; 0; i &lt; groupingExprs_.size(); ++i) &#123;</span><br><span class="line">      groupingExprs_.set(i, rewriteCheckOrdinalResult(</span><br><span class="line">          rewriter, groupingExprs_.get(i)));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  if (orderByElements_ !&#x3D; null) &#123;</span><br><span class="line">    for (OrderByElement orderByElem: orderByElements_) &#123;</span><br><span class="line">      orderByElem.setExpr(rewriteCheckOrdinalResult(rewriter, orderByElem.getExpr()));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从上述代码可以看到，这个函数分别对各个部分调用了rewriteExprs函数，传入rewrite成员r，进行重写，包括：selectList_、fromClause_、whereClause_等，这些正是我们在上篇文章中介绍到的SelectStmt的各个部分。对于selectList_，又调用了SelectList的rewriteExprs方法，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; SelectList.java</span><br><span class="line">public void rewriteExprs(ExprRewriter rewriter, Analyzer analyzer)</span><br><span class="line">    throws AnalysisException &#123;</span><br><span class="line">  for (SelectListItem item: items_) &#123;</span><br><span class="line">    if (item.isStar()) continue;</span><br><span class="line">    item.setExpr(rewriter.rewrite(item.getExpr(), analyzer));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最终，我们可以看到，通过循环处理，对每个SelectListItem中的Expr进行了重写，这个Expr就是通过SelectListItem的getExpr和setExpr进行获取和更新的，其他fromClause_、whereClause_等各个部分，也是类似的处理流程。<br>除此之外，在3.4.0版本中，Impala还提供了对解析之后的SQL进行展示，我们来看一个简单的例子，原始SQL如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select user as name,count(2) from iceberg_partitioned</span><br><span class="line">where id between 2 and 10 group by user;</span><br></pre></td></tr></table></figure>
<p>执行完成之后，就可以在Impala的web页面看到如下所示的SQL解析之后的输出：<br><img src="https://img-blog.csdnimg.cn/20201229112403805.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="sql_rewrite"><br>可以看到，解析之后的SQL经过了重写和隐式转换：</p>
<ul>
<li>count(2)被转换成了count(*)</li>
<li>between被转换成了&gt;=和&lt;=</li>
<li>常量2和10加上了CAST的操作</li>
</ul>
<p>输出的格式主要是通过如下的这个enum来控制的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">public enum ToSqlOptions &#123;</span><br><span class="line">  &#x2F;**</span><br><span class="line">   * The default way of displaying the original SQL query without rewrites.</span><br><span class="line">   *&#x2F;</span><br><span class="line">  DEFAULT(false, false),</span><br><span class="line"></span><br><span class="line">  &#x2F;**</span><br><span class="line">   * Show rewritten query if it exists</span><br><span class="line">   *&#x2F;</span><br><span class="line">  REWRITTEN(true, false),</span><br><span class="line"></span><br><span class="line">  &#x2F;**</span><br><span class="line">   * Show Implicit Casts.</span><br><span class="line">   * To see implicit casts we must also show rewrites as otherwise we see original SQL.</span><br><span class="line">   * This does have the consequence that the sql with implict casts may possibly fail</span><br><span class="line">   * to parse if resubmitted as, for example, EXISTS queries that are rewritten as</span><br><span class="line">   * semi-joins are not legal SQL.</span><br><span class="line">   *&#x2F;</span><br><span class="line">  SHOW_IMPLICIT_CASTS(true, true);</span><br><span class="line"></span><br><span class="line">  private boolean rewritten_;</span><br><span class="line"></span><br><span class="line">  private boolean implictCasts_;</span><br><span class="line">  &#x2F;&#x2F; 省略余下代码</span><br></pre></td></tr></table></figure>
<p>一共有三个选项：DEFAULT、REWRITTEN和SHOW_IMPLICIT_CASTS，上述截图中的结果，就是使用了SHOW_IMPLICIT_CASTS之后的格式化结果。输出的函数就是我们在上篇文章中提到的ParseNode中的toSql，这个函数有两个版本，不带参数的默认是使用ToSqlOptions.DEFAULT。对于我们的SQL示例，是一个SELECT语句，所以解析后的SQL格式化，最终是由SelectStmt.toSql(ToSqlOptions options)函数完成的，输入参数就是SHOW_IMPLICIT_CASTS。<br>到这里，关于Impala的SQL规则重写基本就介绍完了，后续有时间的话，会跟大家继续分享Impala的SQL解析的其他知识。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://skyyws.github.io/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8B%E9%87%8D%E5%86%99%EF%BC%88%E4%BA%8C%EF%BC%89/" data-id="cknk05owo0000hex5hx73azei" data-title="Impala 3.4 SQL查询之重写（二）" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Impala-SQL%E6%9F%A5%E8%AF%A2%E7%B3%BB%E5%88%97/" rel="tag">Impala SQL查询系列</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/olap/" rel="tag">olap</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Impala-3-4-SQL查询梳理（一）" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E6%A2%B3%E7%90%86%EF%BC%88%E4%B8%80%EF%BC%89/" class="article-date">
  <time class="dt-published" datetime="2021-04-16T06:21:01.000Z" itemprop="datePublished">2021-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E6%A2%B3%E7%90%86%EF%BC%88%E4%B8%80%EF%BC%89/">Impala 3.4 SQL查询之梳理（一）</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>当通过jdbc请求连接至Impalad节点之后，我们提交的SQL会通过BE的JNI调用FE的api进行解析，主要的调用栈如下所示：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"> query&#x2F;executeAndWait(impala-beeswax-server.cc)</span><br><span class="line">-Execute(impala-server.cc)</span><br><span class="line">--ExecuteInternal(impala-server.cc)</span><br><span class="line">---InitExecRequest(client-request-state.cc)</span><br><span class="line">----RunFrontendPlanner(query-driver.cc)</span><br><span class="line">-----GetExecRequest(frontend.cc)</span><br><span class="line">------JniFrontendcreateExecRequest()</span><br><span class="line">-------Frontend.createExecRequest()</span><br><span class="line">--------Frontend.getTExecRequest()</span><br><span class="line">---------Frontend.doCreateExecRequest()</span><br></pre></td></tr></table></figure><br> 在doCreateExecRequest方法中，会通过调用Parse.parse()来对SQL进行解析，解析完成之后，SQL就会变成对应的结构，如下所示：<br><img src="https://img-blog.csdnimg.cn/20201224113640624.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="Impala-stmt"><br>从图中我们可以看到，Impala所有的SQL最终都是继承于StatementBase，包括select、alter、create等。这里我们以简单的select查询为例，最终SQL转换之后会被解析成SelectStmt这个类，而这个类其中又包含SelectList、FromClause等部分。通过Parse.parse()的解析，我们将一条普通的SQL转成了一个Impala的类。目前，Impala在进行SQL解析的时候，采用的是一个开源的框架antlr，关于这个框架不是本文描述的重点，这里就不再展开。<br> 对于图中涉及到的一些接口和类，我们摘取了一部分代码中的注释，供大家参考。</p>
<ul>
<li> ParseNode: divide into two broad categories: statement-like nodes and expression nodes;</li>
<li>StmtNode: Base interface for statements and statement-like nodes such as clauses;</li>
<li>Expr: Root of the expr node hierarchy;</li>
<li>StatementBase: Base class for all Impala SQL statements;</li>
<li>QueryStmt: Abstract base class for any statement that returns results via a list of result expressions;</li>
</ul>
<p>在解析出了具体的StatementBase之后（上述例子中就是SelectStmt），Impala接着会构造对应的Analyer，相关的类如下所示：<br><img src="https://img-blog.csdnimg.cn/20201224113720808.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="Analyzer"><br>同样，我们截取部分代码中的注释来看一看：</p>
<ul>
<li>AnalysisContext: Wrapper class for parsing, analyzing and rewriting a SQL stmt;</li>
<li>Analyzer: Repository of analysis state for single select block;</li>
<li>GlobalState: State shared between all objects of an Analyzer tree.</li>
</ul>
<p>这里最重要的类就是Analyzer，包括了单个select查询块的所有解析之后的状态集合。我们继续以SelectStmt为例来看下生成Analyzer的接口调用流程：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> Frontend.doCreateExecRequest()</span><br><span class="line">-AnalysisContext.analyzeAndAuthorize()</span><br><span class="line">--AnalysisContext.analyze()</span><br><span class="line">---SelectStmt.analyze()</span><br><span class="line">----SelectStmt.SelectAnalyzer.analyze()</span><br></pre></td></tr></table></figure><br> 我们可以看到，主要就是调用各个StatementBase子类的analyze()，来实现对各个查询的解析。这里简单看一下SelectStmt的analyze方法，如下所示：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"> &#x2F;&#x2F; SelectStmt.analyze()</span><br><span class="line"> public void analyze(Analyzer analyzer) throws AnalysisException &#123;</span><br><span class="line">  if (isAnalyzed()) return;</span><br><span class="line">  super.analyze(analyzer);</span><br><span class="line">  new SelectAnalyzer(analyzer).analyze();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; SelectStmt.SelectAnalyzer.analyze()</span><br><span class="line">private void analyze() throws AnalysisException &#123;</span><br><span class="line">    &#x2F;&#x2F; Start out with table refs to establish aliases.</span><br><span class="line">    fromClause_.analyze(analyzer_);</span><br><span class="line"></span><br><span class="line">    analyzeSelectClause();</span><br><span class="line">    verifyResultExprs();</span><br><span class="line">    registerViewColumnPrivileges();</span><br><span class="line">    analyzeWhereClause();</span><br><span class="line">    createSortInfo(analyzer_);</span><br><span class="line">    ......</span><br></pre></td></tr></table></figure><br> 可以看到，SelectStmt的解析，主要都在其私有类SelectAnalyzer的analyze中进行处理了，这里包括了对于FromClause的处理、WhereClause的处理等操作。其他的SQL也是类似处理流程，每一个具体的SQL类都有对应的analyze方法。解析完成之后，Impala就会根据解析的结果来生成相应地执行计划：首先是生成一个单机的执行计划，接着会根据单机的执行计划来生成分布式的执行计划。关于执行计划的生成这块，我们会在后续的文章里面陆续提到，这里就不再展开描述。执行计划生成之后，Backend模块就会根据这些执行计划执行实际的扫描、聚合运算等操作，最终返回结果。<br>我们从第一幅图可以看到，ParseNode主要分为了两个部分：1）StmtNode，这个主要包括查询以及相应的clause实现；2）Expr，我们接下来就看一看这个Expr相应的各个子类都是什么样的，下面就是一个简单的关于UML的类图：<br><img src="https://img-blog.csdnimg.cn/20201224113747574.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="Impala-expr"><br>从上图可以看到，有非常多的类都继承了Expr，这里我们看几个比较常见的类：</p>
<ul>
<li>Predicate，这个类就是用来保存各种谓词条件的，包括：BetweenPredicate、BinaryPredicate等，我们在上述的SelectStmt中提到的whereClasue_最终就会转换成一个Predicate，根据不同的条件转换成相应的Predicate；</li>
<li>LiteralExpr，用来保存各种常量的值，例如布尔保存在BoolLiteral中，字符串保存在StringLiteral中等等，目前主要就包括图中的这其中；</li>
<li>FunctionCallExpr，各种函数调用，最终都会转换成这个对象，例如常见的count、sum等；</li>
<li>SlotRef，这个可以简单理解为列的描述，SQL中涉及到列都会被转换成一个SlotRef对象，保存着这个列的相关信息；</li>
<li>其他还有一些例如AnalyticExpr、CastExpr等这里就不再展开描述，感兴趣的同学可以自行查看相关的源码。</li>
</ul>
<p>下面我们就从一个具体的SQL出来，来简单看一下上面提到的各个对象是如何解析的，SQL如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select id,user,count(1) from table_name</span><br><span class="line">where id&gt;&#x3D;5 and id&lt;&#x3D;10</span><br><span class="line">group by id,user order by id desc;</span><br></pre></td></tr></table></figure>
<p>结合上面的几个类图，我们可以看看上述的SQL会被解析成什么样的：</p>
<ul>
<li>SelectList包含三个SelectListItem，分别是：id、user和count(1)，而这三个item各自包含的Expr分别是：SlotRef、SlotRef和FunctionCallExpr，而这个FunctionCallExpr本身又包含一个NumericLiteral，对应count(1)里面的1；</li>
<li>fromClause_主要包括了一个表的集合，这里只有一个成员，就是table_name；</li>
<li>whereClasue_这里转换成了一个CompoundPredicate的谓词，表示组合的谓词，操作符是AND。它本身又包含两个BinaryPredicate，表示包含两个操作数的谓词，分别对应id&gt;=5和id&lt;=10。以第一个为例，它的操作符是&gt;=，本身又包含两个child，分别是id对应的SlotRef以及10对应的NumericLiteral；</li>
<li>groupingExprs_是一系列的group by成员集合，这里主要就是包括两个SlotRef，分别对应id和user；</li>
<li>orderByElements_是从QueryStmt继承而来，成员是一个OrderByElement类，而这个OrderByElement内部也是包含了一个Expr，这里对应的仍旧是一个SlotRef，即id列；<br>到这里，我们基本对于上述示例中的SQL各个部分的解析都已经完成了。<br>本文比较浅显地讲述了Impala SQL解析中的两个部分：StmtmentBase和Expr，整个SQL解析的大部分成员对象，最终都会转换成这两个类或者其子类。关于Analyzer类，本身没有过多讲述，只是稍微提了一下，后续有机会再跟大家一起深入分享。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://skyyws.github.io/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E6%A2%B3%E7%90%86%EF%BC%88%E4%B8%80%EF%BC%89/" data-id="cknk05owu0001hex57wwlfh1e" data-title="Impala 3.4 SQL查询之梳理（一）" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Impala-SQL%E6%9F%A5%E8%AF%A2%E7%B3%BB%E5%88%97/" rel="tag">Impala SQL查询系列</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/olap/" rel="tag">olap</a></li></ul>

    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Impala-SQL%E6%9F%A5%E8%AF%A2%E7%B3%BB%E5%88%97/" rel="tag">Impala SQL查询系列</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Impala%E4%B9%8BHDFS-SCAN-NODE/" rel="tag">Impala之HDFS_SCAN_NODE</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/olap/" rel="tag">olap</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/" rel="tag">问题排查</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Impala-SQL%E6%9F%A5%E8%AF%A2%E7%B3%BB%E5%88%97/" style="font-size: 15px;">Impala SQL查询系列</a> <a href="/tags/Impala%E4%B9%8BHDFS-SCAN-NODE/" style="font-size: 10px;">Impala之HDFS_SCAN_NODE</a> <a href="/tags/impala/" style="font-size: 20px;">impala</a> <a href="/tags/olap/" style="font-size: 20px;">olap</a> <a href="/tags/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/" style="font-size: 10px;">问题排查</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/04/16/Impala-2-12-0%E4%B8%8E3-4-0%E7%89%88%E6%9C%AC%E7%9A%84compute-stats%E5%85%BC%E5%AE%B9%E9%97%AE%E9%A2%98/">Impala 2.12.0与3.4.0版本的compute stats兼容问题</a>
          </li>
        
          <li>
            <a href="/2021/04/16/Impala-cast-timestamp%E5%AF%BC%E8%87%B4%E7%9B%B8%E5%90%8CSQL%E6%9F%A5%E8%AF%A2%E4%B8%8D%E4%B8%80%E8%87%B4%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/">Impala cast timestamp导致相同SQL查询不一致问题排查</a>
          </li>
        
          <li>
            <a href="/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8BScanRange%E8%AF%A6%E8%A7%A3%EF%BC%88%E5%9B%9B%EF%BC%89/">Impala 3.4 SQL查询之ScanRange详解（四）</a>
          </li>
        
          <li>
            <a href="/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8BScanRange%E8%AF%A6%E8%A7%A3%EF%BC%88%E4%B8%89%EF%BC%89/">Impala 3.4 SQL查询之ScanRange详解（三）</a>
          </li>
        
          <li>
            <a href="/2021/04/16/Impala-HDFS-SCAN-NODE%E4%B9%8BAverageHdfsReadThreadConcurrency/">Impala HDFS_SCAN_NODE之AverageHdfsReadThreadConcurrency</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 汪胜<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>