<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>skyyws的藏宝阁</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="skyyws的藏宝阁">
<meta property="og:url" content="https://skyyws.github.io/page/2/index.html">
<meta property="og:site_name" content="skyyws的藏宝阁">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="汪胜">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="skyyws的藏宝阁" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">skyyws的藏宝阁</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://skyyws.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-Impala-HDFS-SCAN-NODE之IO-threads模型" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/16/Impala-HDFS-SCAN-NODE%E4%B9%8BIO-threads%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time class="dt-published" datetime="2021-04-16T11:10:43.000Z" itemprop="datePublished">2021-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/16/Impala-HDFS-SCAN-NODE%E4%B9%8BIO-threads%E6%A8%A1%E5%9E%8B/">Impala HDFS_SCAN_NODE之IO threads模型</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>本文主要从代码出发，跟大家一起分享下Impala HDFS_SCAN_NODE中的IO threads模型。首先，在Impala中，有几个io threads相关的配置，通过对这几个参数进行配置，我们就可以增加处理io的线程数，相关的几个配置如下所示：<br><img src="https://img-blog.csdnimg.cn/20210331144325103.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="1"><br>以我们最常见的hdfs存储引擎为例，如果impalad节点与datanode节点在一台机器上，对于impala来说，就是可以通过本地的disk直接读取数据；如果impalad节点与datanode在不同的机器上，那么就是remote的读取。在我们内部的生产环境，大部分都是这样的情况：有一个公共的HDFS集群，业务所有的离线数据都存储在上面，我们需要单独部署一个Impala集群，对于HDFS集群上的某些数据进行Ad-hoc类的多维分析，此时impala就是通过remote来读取hdfs的数据，那么将num_remote_hdfs_io_threads配置项调整的大一些，就可以适当地加快hdfs scan的速度。<br>在正式开启介绍之前，我们需要知道Impala的scan node模型分为两层：1）IO threads，这层主要就是通过IO读取远端的hdfs数据，并且返回，通过配置num_remote_hdfs_io_threads参数，就可以调整读取的线程数，值得一提的是，一些谓词可以下推到远端的hdfs，减少扫描返回的数据量；2）Scanner，当数据从远端的HDFS返回之后，会由专门的scanner线程进行处理，可能的操作包括：数据解码、cast计算等。本文我们主要讲的就是第一层IO threads，其他更多的介绍可以参考：<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/63405729/why-impala-scan-node-is-very-slow-rowbatchqueuegetwaittime">Why Impala Scan Node is very slow</a>中Tim Armstrong的回答，这篇CSDN的博客也有介绍：<a target="_blank" rel="noopener" href="https://blog.csdn.net/yu616568/article/details/74996897?spm=1001.2014.3001.5501">Impala高性能探秘之HDFS数据访问</a>。<br>下面，我们就结合代码来简单看下这个参数是如何起作用的。在Impala的BE代码中，有一个类专门用来管理IO相关的操作，用于访问本地磁盘或者远端的文件系统，即DiskIoMgr。在这个类中，有一个disk_queues_成员，这是一个集合，每个成员都代表一个disk对应的队列，或者是一种远端文件系统，例如HDFS/S3等，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; disk-io-mrg.h</span><br><span class="line">  &#x2F;&#x2F;&#x2F; Per disk queues. This is static and created once at Init() time.  One queue is</span><br><span class="line">  &#x2F;&#x2F;&#x2F; allocated for each local disk on the system and for each remote filesystem type.</span><br><span class="line">  &#x2F;&#x2F;&#x2F; It is indexed by disk id.</span><br><span class="line">  std::vector&lt;DiskQueue*&gt; disk_queues_;</span><br></pre></td></tr></table></figure>
<p>首先会在构造函数中，对这个变量进行resize操作，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; disk-io-mrg.cc</span><br><span class="line">disk_queues_.resize(num_local_disks + REMOTE_NUM_DISKS);</span><br></pre></td></tr></table></figure>
<p>这里的num_local_disks指的就是本地磁盘的个数，而REMOTE_NUM_DISKS就是一个enum变量，用来控制远端访问的偏移：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; disk-io-mrg.h</span><br><span class="line">  &#x2F;&#x2F;&#x2F; &quot;Disk&quot; queue offsets for remote accesses.  Offset 0 corresponds to</span><br><span class="line">  &#x2F;&#x2F;&#x2F; disk ID (i.e. disk_queue_ index) of num_local_disks().</span><br><span class="line">  enum &#123;</span><br><span class="line">    REMOTE_DFS_DISK_OFFSET &#x3D; 0,</span><br><span class="line">    REMOTE_S3_DISK_OFFSET,</span><br><span class="line">    REMOTE_ADLS_DISK_OFFSET,</span><br><span class="line">    REMOTE_ABFS_DISK_OFFSET,</span><br><span class="line">    REMOTE_OZONE_DISK_OFFSET,</span><br><span class="line">    REMOTE_NUM_DISKS</span><br><span class="line">  &#125;;</span><br></pre></td></tr></table></figure>
<p>所以，impala将每一种远端的文件系统访问，也当成了一个disk，按照上述的enum顺序，放到disk_queues_中，作为一个成员变量。接着在Init函数中，会循环对这个disk_queues_变量进行初始化：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; disk-io-mrg.cc</span><br><span class="line">  for (int i &#x3D; 0; i &lt; disk_queues_.size(); ++i) &#123;</span><br><span class="line">    disk_queues_[i] &#x3D; new DiskQueue(i);</span><br><span class="line">    int num_threads_per_disk;</span><br><span class="line">    string device_name;</span><br><span class="line">    if (i &#x3D;&#x3D; RemoteDfsDiskId()) &#123;</span><br><span class="line">      num_threads_per_disk &#x3D; FLAGS_num_remote_hdfs_io_threads;</span><br><span class="line">      device_name &#x3D; &quot;HDFS remote&quot;;</span><br></pre></td></tr></table></figure>
<p>在整个for循环中，会根据id来判断是需要对哪一个队列进行操作，这里以HDFS为例，id就是本地磁盘的数量+HDFS在enum中的offset：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; disk-io-mrg.cc</span><br><span class="line">  &#x2F;&#x2F;&#x2F; The disk ID (and therefore disk_queues_ index) used for DFS accesses.</span><br><span class="line">  int RemoteDfsDiskId() const &#123; return num_local_disks() + REMOTE_DFS_DISK_OFFSET; &#125;</span><br></pre></td></tr></table></figure>
<p>如果是要访问远端的HDFS，那么对应的线程数量，即num_threads_per_disk，就是我们通过配置文件指定的num_remote_hdfs_io_threads的值，默认是8。表示会启动8个线程用于处理远端的HDFS访问操作。接着，impala就会循环创建对应数量的线程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; disk-io-mrg.cc</span><br><span class="line">    for (int j &#x3D; 0; j &lt; num_threads_per_disk; ++j) &#123;</span><br><span class="line">      stringstream ss;</span><br><span class="line">      ss &lt;&lt; &quot;work-loop(Disk: &quot; &lt;&lt; device_name &lt;&lt; &quot;, Thread: &quot; &lt;&lt; j &lt;&lt; &quot;)&quot;;</span><br><span class="line">      std::unique_ptr&lt;Thread&gt; t;</span><br><span class="line">      RETURN_IF_ERROR(Thread::Create(&quot;disk-io-mgr&quot;, ss.str(), &amp;DiskQueue::DiskThreadLoop,</span><br><span class="line">          disk_queues_[i], this, &amp;t));</span><br><span class="line">      disk_thread_group_.AddThread(move(t));</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>在进行线程创建的时候，将函数DiskQueue::DiskThreadLoop绑定到了该线程上，该函数就是通过一个while循环来不断的进行处理，相关的函数调用如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DiskThreadLoop(disk-io-mrg.cc)</span><br><span class="line">-GetNextRequestRange(disk-io-mrg.cc)</span><br><span class="line">--GetNextRequestRange(request-context.cc)</span><br><span class="line">-DoRead(scan-range.cc)&#x2F;Write(disk-io-mgr.cc)</span><br></pre></td></tr></table></figure>
<p>GetNextRequestRange函数就是用来获取当前这个DiskQueue（例如远端的HDFS访问queue）的下一个RequestRange，来进行具体的io操作。RequestRange代表一个文件中的连续字节序列，主要分为：ScanRange和WriteRange。每个disk线程一次只能处理一个RequestRange。这里impala采用了一个两层的设计，在GetNextRequestRange中，首先会需要获取一个RequestContext对象，RequestContext可以理解为一个查询的某个instance下的所有IO请求集合，可以简单理解为某个表的RequestRange集合都被封装在一个RequestContext对象中。获取RequestContext的代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">*request_context &#x3D; request_contexts_.front();</span><br><span class="line">request_contexts_.pop_front();</span><br><span class="line">DCHECK(*request_context !&#x3D; nullptr);</span><br></pre></td></tr></table></figure>
<p>request_contexts_是一个RequestContext类型的list，每一个DiskQueue都包含了这样一个队列，表示该DiskQueue上的所有的待处理的RequestContext列表。这里我们可以简单的理解为每个表的扫描请求，都在这个队列中等待处理。首先会从队列的头部取出一个RequestContext，然后将该对象弹出。该DiskQueue的其他线程就可以继续处理后续的RequestContext对象，这样就不会因为当前的RequestContext对象处理时间过长，而阻塞了其他的RequestContext对象处理。<br>关于request_contexts_队列成员更新，不是本文介绍的重点，只要知道：当提交查询的时候，impalad会自动进行解析，然后进行封装，最后添加到该队列中即可。<br>在获取到RequestContext对象之后，我们就可以通过该RequestContext的GetNextRequestRange方法获取具体的RequestRange对象进行实际的扫描操作了。<br>上面的描述可能不太容易理解，我们将上述的各个成员之间的包含关系以及操作流程进行了整理成了一张图，如下所示：<br><img src="https://img-blog.csdnimg.cn/20210331144401259.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="2"><br>最终获取到了一个RequestRange之后，会进行判断，是READ还是WRITE，进行相应地处理。这里我们以READ为例，相关函数调用如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DiskThreadLoop(disk-io-mrg.cc)</span><br><span class="line">-GetNextRequestRange(disk-io-mrg.cc)</span><br><span class="line">--GetNextRequestRange(request-context.cc)</span><br><span class="line">-DoRead(scan-range.cc)</span><br><span class="line">-ReadDone(request-context.cc)</span><br></pre></td></tr></table></figure>
<p>从上面的相关代码，我们可以知道，如果我们将num_remote_hdfs_io_threads参数配置的更大一些，那么就会有更多的线程并发的通过DiskThreadLoop获取到RequestRange进行处理，从而可以在一定程度上提到SCAN的速度，进而加快整个查询进程。<br>在Impala的profile中，对于HDFS的IO theads的指标，即AverageHdfsReadThreadConcurrency，相关介绍如下所示：<br><img src="https://img-blog.csdnimg.cn/2021033114441848.png" alt="3"><br>可以简单理解为该HDFS_SCAN_NODE有多少个IO线程用于处于读写请求操作。所以说，如果线上查询的这个指标很小，那么就要考虑适当调整num_remote_hdfs_io_threads这个参数了。与这个指标很相似的是AverageScannerThreadConcurrency，这个表示scanner线程的执行数量，与我前面提到的scan node两层模型中的scanner对应，这个之后再详细介绍。除此之外，还有其他的一些指标，例如ScannerIoWaitTime，表示scanner等到IO线程的数据就绪的时间，如果这个时间很长，那么说明IO线程存在瓶颈。还有很多指标，就不再一一展开描述。我们在线上排查慢查询的时候，这些指标都是非常有用的信息。<br>上面提到了profile中的指标信息。另外，在impala服务启动之后，我们也可以通过web页面上的/threadz页面查看“disk-io-mgr”这个组下面的线程信息，就可以看到用于处理远端HDFS读取的线程：<br><img src="https://img-blog.csdnimg.cn/20210331144429621.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="3"><br>上面的User/Kernel CPU和IO-wait的时间，都是直接从机器上读取的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; os-util.h</span><br><span class="line">&#x2F;&#x2F;&#x2F; Populates ThreadStats object for a given thread by reading from</span><br><span class="line">&#x2F;&#x2F;&#x2F; &#x2F;proc&#x2F;&lt;pid&gt;&#x2F;task&#x2F;&lt;tid&gt;&#x2F;stats. Returns OK unless the file cannot be read or is in an</span><br><span class="line">&#x2F;&#x2F;&#x2F; unrecognised format, or if the kernel version is not modern enough.</span><br><span class="line">Status GetThreadStats(int64_t tid, ThreadStats* stats);</span><br></pre></td></tr></table></figure>
<p>对于每个disk queue，impala还绑定了对应的metric信息，如下所示：<br><img src="https://img-blog.csdnimg.cn/20210331144443304.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="4"><br>这些metric代表的就是读取延时和大小的统计直方图信息。<br>到这里，关于HDFS_SCAN_NODE的IO threads就介绍的差不多了，我们通过代码分析，知道了Impala对于disk以及各种远端dfs的处理，这些都是属于IO threads部分，后续有时间再跟大家一起学习scanner模块的相关知识。本文涉及到的代码分析模块，都是笔者自己根据源码分析解读出来，如有错误，欢迎指正。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://skyyws.github.io/2021/04/16/Impala-HDFS-SCAN-NODE%E4%B9%8BIO-threads%E6%A8%A1%E5%9E%8B/" data-id="cknk7opbs0000okx5gva6gjur" data-title="Impala HDFS_SCAN_NODE之IO threads模型" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Impala%E4%B9%8BHDFS-SCAN-NODE/" rel="tag">Impala之HDFS_SCAN_NODE</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/olap/" rel="tag">olap</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Impala-3-4-SQL查询之重写（二）" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8B%E9%87%8D%E5%86%99%EF%BC%88%E4%BA%8C%EF%BC%89/" class="article-date">
  <time class="dt-published" datetime="2021-04-16T06:21:01.000Z" itemprop="datePublished">2021-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8B%E9%87%8D%E5%86%99%EF%BC%88%E4%BA%8C%EF%BC%89/">Impala 3.4 SQL查询之重写（二）</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>在上一篇文章中，我们介绍了Impala基本的SQL解析流程。本文我们将跟大家一起看下Impala中的一些SQL重写规则。这里，我们首先回顾下关于Analyzer的几个类的关系图，如下所示：<br><img src="https://img-blog.csdnimg.cn/20201229112124401.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="analyzer"></p>
<p>当SQL被解析为特定的StatementBase之后，紧接着会构造一个AnalysisContext对象，这个类可以理解为整个SQL解析过程的封装，包括了：parsing, analyzing and rewriting这几个过程。在AnalysisContext的analyze方法中，我们构造了Analyzer变量，完成了对StatementBase的analyze（在上篇文章中也已经介绍过）。相关函数调用过程如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Frontend.doCreateExecRequest()</span><br><span class="line">-AnalysisContext.analyzeAndAuthorize()</span><br><span class="line">--AnalysisContext.analyze()</span><br><span class="line">---AnalysisContext.createAnalyzer()</span><br><span class="line">----Analyzer.ctor()</span><br><span class="line">-----GlobalState.ctor()</span><br><span class="line">---StatementBase.analyze()</span><br><span class="line">---StatementBase.rewriteExprs()</span><br></pre></td></tr></table></figure>
<p>最终我们在Analyzer.GlobalState的构造函数中，将各种重写规则加入到了Analyzer中，相关代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; Analyzer.java</span><br><span class="line">public GlobalState(StmtTableCache stmtTableCache, TQueryCtx queryCtx,</span><br><span class="line">    AuthorizationFactory authzFactory) &#123;</span><br><span class="line">  this.stmtTableCache &#x3D; stmtTableCache;</span><br><span class="line">  this.queryCtx &#x3D; queryCtx;</span><br><span class="line">  this.authzFactory &#x3D; authzFactory;</span><br><span class="line">  this.lineageGraph &#x3D; new ColumnLineageGraph();</span><br><span class="line">  List&lt;ExprRewriteRule&gt; rules &#x3D; new ArrayList&lt;&gt;();</span><br><span class="line">  &#x2F;&#x2F; BetweenPredicates must be rewritten to be executable. Other non-essential</span><br><span class="line">  &#x2F;&#x2F; expr rewrites can be disabled via a query option. When rewrites are enabled</span><br><span class="line">  &#x2F;&#x2F; BetweenPredicates should be rewritten first to help trigger other rules.</span><br><span class="line">  rules.add(BetweenToCompoundRule.INSTANCE);</span><br><span class="line">  &#x2F;&#x2F; Binary predicates must be rewritten to a canonical form for both Kudu predicate</span><br><span class="line">  &#x2F;&#x2F; pushdown and Parquet row group pruning based on min&#x2F;max statistics.</span><br><span class="line">  rules.add(NormalizeBinaryPredicatesRule.INSTANCE);</span><br><span class="line">  if (queryCtx.getClient_request().getQuery_options().enable_expr_rewrites) &#123;</span><br><span class="line">    rules.add(FoldConstantsRule.INSTANCE);</span><br><span class="line">    rules.add(NormalizeExprsRule.INSTANCE);</span><br><span class="line">    rules.add(ExtractCommonConjunctRule.INSTANCE);</span><br><span class="line">    &#x2F;&#x2F; Relies on FoldConstantsRule and NormalizeExprsRule.</span><br><span class="line">    rules.add(SimplifyConditionalsRule.INSTANCE);</span><br><span class="line">    rules.add(EqualityDisjunctsToInRule.INSTANCE);</span><br><span class="line">    rules.add(NormalizeCountStarRule.INSTANCE);</span><br><span class="line">    rules.add(SimplifyDistinctFromRule.INSTANCE);</span><br><span class="line">    rules.add(SimplifyCastStringToTimestamp.INSTANCE);</span><br><span class="line">  &#125;</span><br><span class="line">  exprRewriter_ &#x3D; new ExprRewriter(rules);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个构造函数里面添加了很多重写规则，这些规则最终都会被应用于SQL的重写中。Impala目前包含了很多重写规则，相关类图如下所示：<br><img src="https://img-blog.csdnimg.cn/20201229112220768.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="rewrite_rule"></p>
<p>所有的重写规则都实现了ExprRewriteRule这个接口，接口本身只包含一个方法apply，接收一个Expr和Analyzer，返回是一个修改之后的Expr。关于Expr，我们在上篇文章中也已经提到了过了，这里就不再展开描述。需要注意的是，Impala还提供了一个query option，叫ENABLE_EXPR_REWRITES，默认为true，会启用更多的重写规则，对于SQL的查询性能提升有很大的帮助。<br>通过上述代码可以看到，在构造GlobalState成员变量的时候，会将所有的重写规则放到一个数组当中，然后构造一个ExprRewriter类，这个类的作用就是：使用重写规则的数组，对指定的Expr进行重写操作。在完成对应的Analyzer构造和StatementBase的解析之后，会调用StatementBase的rewriteExprs方法，来对这个statement的所有Exprs进行重写，这里我们以SelectStmt为例（StatementBase本身是抽象类，并没有实现这个方法），来看一下是如何对Expr进行重写的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; SelectStmt.java</span><br><span class="line">public void rewriteExprs(ExprRewriter rewriter) throws AnalysisException &#123;</span><br><span class="line">  Preconditions.checkState(isAnalyzed());</span><br><span class="line">  selectList_.rewriteExprs(rewriter, analyzer_);</span><br><span class="line">  for (TableRef ref: fromClause_.getTableRefs()) ref.rewriteExprs(rewriter, analyzer_);</span><br><span class="line">  if (whereClause_ !&#x3D; null) &#123;</span><br><span class="line">    whereClause_ &#x3D; rewriter.rewrite(whereClause_, analyzer_);</span><br><span class="line">    &#x2F;&#x2F; Also rewrite exprs in the statements of subqueries.</span><br><span class="line">    List&lt;Subquery&gt; subqueryExprs &#x3D; new ArrayList&lt;&gt;();</span><br><span class="line">    whereClause_.collect(Subquery.class, subqueryExprs);</span><br><span class="line">    for (Subquery s: subqueryExprs) s.getStatement().rewriteExprs(rewriter);</span><br><span class="line">  &#125;</span><br><span class="line">  if (havingClause_ !&#x3D; null) &#123;</span><br><span class="line">    havingClause_ &#x3D; rewriteCheckOrdinalResult(rewriter, havingClause_);</span><br><span class="line">  &#125;</span><br><span class="line">  if (groupingExprs_ !&#x3D; null) &#123;</span><br><span class="line">    for (int i &#x3D; 0; i &lt; groupingExprs_.size(); ++i) &#123;</span><br><span class="line">      groupingExprs_.set(i, rewriteCheckOrdinalResult(</span><br><span class="line">          rewriter, groupingExprs_.get(i)));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  if (orderByElements_ !&#x3D; null) &#123;</span><br><span class="line">    for (OrderByElement orderByElem: orderByElements_) &#123;</span><br><span class="line">      orderByElem.setExpr(rewriteCheckOrdinalResult(rewriter, orderByElem.getExpr()));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从上述代码可以看到，这个函数分别对各个部分调用了rewriteExprs函数，传入rewrite成员r，进行重写，包括：selectList_、fromClause_、whereClause_等，这些正是我们在上篇文章中介绍到的SelectStmt的各个部分。对于selectList_，又调用了SelectList的rewriteExprs方法，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; SelectList.java</span><br><span class="line">public void rewriteExprs(ExprRewriter rewriter, Analyzer analyzer)</span><br><span class="line">    throws AnalysisException &#123;</span><br><span class="line">  for (SelectListItem item: items_) &#123;</span><br><span class="line">    if (item.isStar()) continue;</span><br><span class="line">    item.setExpr(rewriter.rewrite(item.getExpr(), analyzer));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最终，我们可以看到，通过循环处理，对每个SelectListItem中的Expr进行了重写，这个Expr就是通过SelectListItem的getExpr和setExpr进行获取和更新的，其他fromClause_、whereClause_等各个部分，也是类似的处理流程。<br>除此之外，在3.4.0版本中，Impala还提供了对解析之后的SQL进行展示，我们来看一个简单的例子，原始SQL如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select user as name,count(2) from iceberg_partitioned</span><br><span class="line">where id between 2 and 10 group by user;</span><br></pre></td></tr></table></figure>
<p>执行完成之后，就可以在Impala的web页面看到如下所示的SQL解析之后的输出：<br><img src="https://img-blog.csdnimg.cn/20201229112403805.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="sql_rewrite"><br>可以看到，解析之后的SQL经过了重写和隐式转换：</p>
<ul>
<li>count(2)被转换成了count(*)</li>
<li>between被转换成了&gt;=和&lt;=</li>
<li>常量2和10加上了CAST的操作</li>
</ul>
<p>输出的格式主要是通过如下的这个enum来控制的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">public enum ToSqlOptions &#123;</span><br><span class="line">  &#x2F;**</span><br><span class="line">   * The default way of displaying the original SQL query without rewrites.</span><br><span class="line">   *&#x2F;</span><br><span class="line">  DEFAULT(false, false),</span><br><span class="line"></span><br><span class="line">  &#x2F;**</span><br><span class="line">   * Show rewritten query if it exists</span><br><span class="line">   *&#x2F;</span><br><span class="line">  REWRITTEN(true, false),</span><br><span class="line"></span><br><span class="line">  &#x2F;**</span><br><span class="line">   * Show Implicit Casts.</span><br><span class="line">   * To see implicit casts we must also show rewrites as otherwise we see original SQL.</span><br><span class="line">   * This does have the consequence that the sql with implict casts may possibly fail</span><br><span class="line">   * to parse if resubmitted as, for example, EXISTS queries that are rewritten as</span><br><span class="line">   * semi-joins are not legal SQL.</span><br><span class="line">   *&#x2F;</span><br><span class="line">  SHOW_IMPLICIT_CASTS(true, true);</span><br><span class="line"></span><br><span class="line">  private boolean rewritten_;</span><br><span class="line"></span><br><span class="line">  private boolean implictCasts_;</span><br><span class="line">  &#x2F;&#x2F; 省略余下代码</span><br></pre></td></tr></table></figure>
<p>一共有三个选项：DEFAULT、REWRITTEN和SHOW_IMPLICIT_CASTS，上述截图中的结果，就是使用了SHOW_IMPLICIT_CASTS之后的格式化结果。输出的函数就是我们在上篇文章中提到的ParseNode中的toSql，这个函数有两个版本，不带参数的默认是使用ToSqlOptions.DEFAULT。对于我们的SQL示例，是一个SELECT语句，所以解析后的SQL格式化，最终是由SelectStmt.toSql(ToSqlOptions options)函数完成的，输入参数就是SHOW_IMPLICIT_CASTS。<br>到这里，关于Impala的SQL规则重写基本就介绍完了，后续有时间的话，会跟大家继续分享Impala的SQL解析的其他知识。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://skyyws.github.io/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E4%B9%8B%E9%87%8D%E5%86%99%EF%BC%88%E4%BA%8C%EF%BC%89/" data-id="cknk05owo0000hex5hx73azei" data-title="Impala 3.4 SQL查询之重写（二）" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Impala-SQL%E6%9F%A5%E8%AF%A2%E7%B3%BB%E5%88%97/" rel="tag">Impala SQL查询系列</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/olap/" rel="tag">olap</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Impala-3-4-SQL查询梳理（一）" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E6%A2%B3%E7%90%86%EF%BC%88%E4%B8%80%EF%BC%89/" class="article-date">
  <time class="dt-published" datetime="2021-04-16T06:21:01.000Z" itemprop="datePublished">2021-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E6%A2%B3%E7%90%86%EF%BC%88%E4%B8%80%EF%BC%89/">Impala 3.4 SQL查询之梳理（一）</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>当通过jdbc请求连接至Impalad节点之后，我们提交的SQL会通过BE的JNI调用FE的api进行解析，主要的调用栈如下所示：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"> query&#x2F;executeAndWait(impala-beeswax-server.cc)</span><br><span class="line">-Execute(impala-server.cc)</span><br><span class="line">--ExecuteInternal(impala-server.cc)</span><br><span class="line">---InitExecRequest(client-request-state.cc)</span><br><span class="line">----RunFrontendPlanner(query-driver.cc)</span><br><span class="line">-----GetExecRequest(frontend.cc)</span><br><span class="line">------JniFrontendcreateExecRequest()</span><br><span class="line">-------Frontend.createExecRequest()</span><br><span class="line">--------Frontend.getTExecRequest()</span><br><span class="line">---------Frontend.doCreateExecRequest()</span><br></pre></td></tr></table></figure><br> 在doCreateExecRequest方法中，会通过调用Parse.parse()来对SQL进行解析，解析完成之后，SQL就会变成对应的结构，如下所示：<br><img src="https://img-blog.csdnimg.cn/20201224113640624.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="Impala-stmt"><br>从图中我们可以看到，Impala所有的SQL最终都是继承于StatementBase，包括select、alter、create等。这里我们以简单的select查询为例，最终SQL转换之后会被解析成SelectStmt这个类，而这个类其中又包含SelectList、FromClause等部分。通过Parse.parse()的解析，我们将一条普通的SQL转成了一个Impala的类。目前，Impala在进行SQL解析的时候，采用的是一个开源的框架antlr，关于这个框架不是本文描述的重点，这里就不再展开。<br> 对于图中涉及到的一些接口和类，我们摘取了一部分代码中的注释，供大家参考。</p>
<ul>
<li> ParseNode: divide into two broad categories: statement-like nodes and expression nodes;</li>
<li>StmtNode: Base interface for statements and statement-like nodes such as clauses;</li>
<li>Expr: Root of the expr node hierarchy;</li>
<li>StatementBase: Base class for all Impala SQL statements;</li>
<li>QueryStmt: Abstract base class for any statement that returns results via a list of result expressions;</li>
</ul>
<p>在解析出了具体的StatementBase之后（上述例子中就是SelectStmt），Impala接着会构造对应的Analyer，相关的类如下所示：<br><img src="https://img-blog.csdnimg.cn/20201224113720808.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="Analyzer"><br>同样，我们截取部分代码中的注释来看一看：</p>
<ul>
<li>AnalysisContext: Wrapper class for parsing, analyzing and rewriting a SQL stmt;</li>
<li>Analyzer: Repository of analysis state for single select block;</li>
<li>GlobalState: State shared between all objects of an Analyzer tree.</li>
</ul>
<p>这里最重要的类就是Analyzer，包括了单个select查询块的所有解析之后的状态集合。我们继续以SelectStmt为例来看下生成Analyzer的接口调用流程：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> Frontend.doCreateExecRequest()</span><br><span class="line">-AnalysisContext.analyzeAndAuthorize()</span><br><span class="line">--AnalysisContext.analyze()</span><br><span class="line">---SelectStmt.analyze()</span><br><span class="line">----SelectStmt.SelectAnalyzer.analyze()</span><br></pre></td></tr></table></figure><br> 我们可以看到，主要就是调用各个StatementBase子类的analyze()，来实现对各个查询的解析。这里简单看一下SelectStmt的analyze方法，如下所示：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"> &#x2F;&#x2F; SelectStmt.analyze()</span><br><span class="line"> public void analyze(Analyzer analyzer) throws AnalysisException &#123;</span><br><span class="line">  if (isAnalyzed()) return;</span><br><span class="line">  super.analyze(analyzer);</span><br><span class="line">  new SelectAnalyzer(analyzer).analyze();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; SelectStmt.SelectAnalyzer.analyze()</span><br><span class="line">private void analyze() throws AnalysisException &#123;</span><br><span class="line">    &#x2F;&#x2F; Start out with table refs to establish aliases.</span><br><span class="line">    fromClause_.analyze(analyzer_);</span><br><span class="line"></span><br><span class="line">    analyzeSelectClause();</span><br><span class="line">    verifyResultExprs();</span><br><span class="line">    registerViewColumnPrivileges();</span><br><span class="line">    analyzeWhereClause();</span><br><span class="line">    createSortInfo(analyzer_);</span><br><span class="line">    ......</span><br></pre></td></tr></table></figure><br> 可以看到，SelectStmt的解析，主要都在其私有类SelectAnalyzer的analyze中进行处理了，这里包括了对于FromClause的处理、WhereClause的处理等操作。其他的SQL也是类似处理流程，每一个具体的SQL类都有对应的analyze方法。解析完成之后，Impala就会根据解析的结果来生成相应地执行计划：首先是生成一个单机的执行计划，接着会根据单机的执行计划来生成分布式的执行计划。关于执行计划的生成这块，我们会在后续的文章里面陆续提到，这里就不再展开描述。执行计划生成之后，Backend模块就会根据这些执行计划执行实际的扫描、聚合运算等操作，最终返回结果。<br>我们从第一幅图可以看到，ParseNode主要分为了两个部分：1）StmtNode，这个主要包括查询以及相应的clause实现；2）Expr，我们接下来就看一看这个Expr相应的各个子类都是什么样的，下面就是一个简单的关于UML的类图：<br><img src="https://img-blog.csdnimg.cn/20201224113747574.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NreXl3cw==,size_16,color_FFFFFF,t_70" alt="Impala-expr"><br>从上图可以看到，有非常多的类都继承了Expr，这里我们看几个比较常见的类：</p>
<ul>
<li>Predicate，这个类就是用来保存各种谓词条件的，包括：BetweenPredicate、BinaryPredicate等，我们在上述的SelectStmt中提到的whereClasue_最终就会转换成一个Predicate，根据不同的条件转换成相应的Predicate；</li>
<li>LiteralExpr，用来保存各种常量的值，例如布尔保存在BoolLiteral中，字符串保存在StringLiteral中等等，目前主要就包括图中的这其中；</li>
<li>FunctionCallExpr，各种函数调用，最终都会转换成这个对象，例如常见的count、sum等；</li>
<li>SlotRef，这个可以简单理解为列的描述，SQL中涉及到列都会被转换成一个SlotRef对象，保存着这个列的相关信息；</li>
<li>其他还有一些例如AnalyticExpr、CastExpr等这里就不再展开描述，感兴趣的同学可以自行查看相关的源码。</li>
</ul>
<p>下面我们就从一个具体的SQL出来，来简单看一下上面提到的各个对象是如何解析的，SQL如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select id,user,count(1) from table_name</span><br><span class="line">where id&gt;&#x3D;5 and id&lt;&#x3D;10</span><br><span class="line">group by id,user order by id desc;</span><br></pre></td></tr></table></figure>
<p>结合上面的几个类图，我们可以看看上述的SQL会被解析成什么样的：</p>
<ul>
<li>SelectList包含三个SelectListItem，分别是：id、user和count(1)，而这三个item各自包含的Expr分别是：SlotRef、SlotRef和FunctionCallExpr，而这个FunctionCallExpr本身又包含一个NumericLiteral，对应count(1)里面的1；</li>
<li>fromClause_主要包括了一个表的集合，这里只有一个成员，就是table_name；</li>
<li>whereClasue_这里转换成了一个CompoundPredicate的谓词，表示组合的谓词，操作符是AND。它本身又包含两个BinaryPredicate，表示包含两个操作数的谓词，分别对应id&gt;=5和id&lt;=10。以第一个为例，它的操作符是&gt;=，本身又包含两个child，分别是id对应的SlotRef以及10对应的NumericLiteral；</li>
<li>groupingExprs_是一系列的group by成员集合，这里主要就是包括两个SlotRef，分别对应id和user；</li>
<li>orderByElements_是从QueryStmt继承而来，成员是一个OrderByElement类，而这个OrderByElement内部也是包含了一个Expr，这里对应的仍旧是一个SlotRef，即id列；<br>到这里，我们基本对于上述示例中的SQL各个部分的解析都已经完成了。<br>本文比较浅显地讲述了Impala SQL解析中的两个部分：StmtmentBase和Expr，整个SQL解析的大部分成员对象，最终都会转换成这两个类或者其子类。关于Analyzer类，本身没有过多讲述，只是稍微提了一下，后续有机会再跟大家一起深入分享。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://skyyws.github.io/2021/04/16/Impala-3-4-SQL%E6%9F%A5%E8%AF%A2%E6%A2%B3%E7%90%86%EF%BC%88%E4%B8%80%EF%BC%89/" data-id="cknk05owu0001hex57wwlfh1e" data-title="Impala 3.4 SQL查询之梳理（一）" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Impala-SQL%E6%9F%A5%E8%AF%A2%E7%B3%BB%E5%88%97/" rel="tag">Impala SQL查询系列</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/olap/" rel="tag">olap</a></li></ul>

    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Impala-SQL%E6%9F%A5%E8%AF%A2%E7%B3%BB%E5%88%97/" rel="tag">Impala SQL查询系列</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Impala%E4%B9%8BHDFS-SCAN-NODE/" rel="tag">Impala之HDFS_SCAN_NODE</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/impala/" rel="tag">impala</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kylin/" rel="tag">kylin</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/olap/" rel="tag">olap</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/" rel="tag">经验总结</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/" rel="tag">问题排查</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Impala-SQL%E6%9F%A5%E8%AF%A2%E7%B3%BB%E5%88%97/" style="font-size: 12.5px;">Impala SQL查询系列</a> <a href="/tags/Impala%E4%B9%8BHDFS-SCAN-NODE/" style="font-size: 10px;">Impala之HDFS_SCAN_NODE</a> <a href="/tags/impala/" style="font-size: 17.5px;">impala</a> <a href="/tags/kylin/" style="font-size: 10px;">kylin</a> <a href="/tags/olap/" style="font-size: 20px;">olap</a> <a href="/tags/%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/" style="font-size: 10px;">经验总结</a> <a href="/tags/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/" style="font-size: 15px;">问题排查</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/04/16/%E5%85%B3%E4%BA%8EImpala%E7%9A%84use-local-tz-for-unix-timestamp-conversions%E5%8F%82%E6%95%B0%E6%8E%A2%E7%A9%B6/">关于Impala的use_local_tz_for_unix_timestamp_conversions参数探究</a>
          </li>
        
          <li>
            <a href="/2021/04/16/Impala%E9%85%8D%E7%BD%AERanger%E6%9C%8D%E5%8A%A1%E8%BF%9B%E8%A1%8C%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6/">Impala配置Ranger服务进行权限控制</a>
          </li>
        
          <li>
            <a href="/2021/04/16/%E8%AE%B0%E4%B8%80%E6%AC%A1Apache-Kylin%E7%9A%84%E6%85%A2%E6%9F%A5%E8%AF%A2%E6%8E%92%E6%9F%A5%E5%8F%8A%E4%BC%98%E5%8C%96/">记一次Apache Kylin的慢查询排查及优化</a>
          </li>
        
          <li>
            <a href="/2021/04/16/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Kylin%E5%BC%80%E5%90%AFG1%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%97%E6%B3%95%E5%AF%BC%E8%87%B4%E8%BF%9B%E7%A8%8B%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8/">问题排查--Kylin开启G1垃圾回收算法导致进程无法启动</a>
          </li>
        
          <li>
            <a href="/2021/04/16/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Impala%E6%9F%A5%E8%AF%A2Decimal%E6%95%B0%E6%8D%AE%E4%B8%BANULL%EF%BC%8CHive%E6%9F%A5%E8%AF%A2%E6%AD%A3%E5%B8%B8/">问题排查--Impala查询Decimal数据为NULL，Hive查询正常</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 汪胜<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>